[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Dynamic Effects of Weather Shocks on Agricultural Production",
    "section": "",
    "text": "Introduction\nThis ebook is the online supplementary materials for the article titled “The Dynamic Effects of Weather Shocks on Agricultural Production”.\nThe document is divided in five parts.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#replication-codes",
    "href": "index.html#replication-codes",
    "title": "The Dynamic Effects of Weather Shocks on Agricultural Production",
    "section": "Replication Codes",
    "text": "Replication Codes\nThe codes presented in this ebook are available in the R folder. The functions that are sourced within those R scripts are defined the weatherperu/R folder (the weatherperu is our R package that helps us create and document the functions used to perform the analysis). This package does not need to be installed.\n\nFull version: Replication codes + ebook + data (408Mo):\nFull zip archive (412Mo)\nLight version: Replication codes only (no data) (31.5Mo):\n codes (184ko)\nLight version of the datasets: Zip archive with light datasets (1.2Mo)\n\nThe following tree architecture is adopted:\nSupplementary-materials\n├ ── README.txt\n├ ── Replication_book\n├ ── R\n│   └── data-weather.R\n│   └── data-agriculture-calendar.R\n│   └── data-agriculture.R\n│   └── data-macro.R\n│   └── data-other.R\n│   └── data-merge.R\n│   └── data-desc-stats.R\n│   └── local_proj_linear.R\n│   └── local_proj_quadratic.R\n│   └── local_proj_seasonal.R\n│   └── aggregate_fluctuations.R\n│   └── robustness-data-merge-quarter.R\n│   └── robustness-data-merge-annual.R\n│   └── robustness-local_proj_linear_quarter.R\n│   └── robustness-local_proj_linear_annual.R\n│   └── robustness-local_proj_comparison.R\n│   └── robustness-local_proj_linear_chirps.R\n│   └── robustness-aggregate_fluctuations_chirps.R\n│   └── robustness-local_proj_linear_surprise.R\n│   └── robustness-local_proj_until_2014.R\n├ ── data\n│   └── output\n|   |   └── dataset_2001_2015.rda\n│   └── raw\n├ ── weatherperu\n│   └── R\n\nReplication_book: folder that contains the codes to produce this ebook.\nR: folder that contains the R scripts that allow to create the datasets and estimate the models.\ndata: folder that contains data (raw data in raw and processed data on output).\nweatherperu: useful functions used in the R codes, made available as an R package (the functions are defined in the R subfolder).\n\nNote that if you run the codes, the estimation results will be saved in the R/output/ folder.\n\n\n\n\n\n\nRaw Data\n\n\n\nTo be able to reproduce the results from the article, we provide all the R codes. The user is kindly invited to download the raw data themselves.\n\n\nThe codes are divided in three parts. The first part contains the code used to obtain the dataset used in the estimations. The second part contains the codes that allow to estimate the local projections and the results obtained with the vector autoregressive model. The third part explores how are the results from the local projections impacted when using quarterly data instead of monthly data.\n\nPreparing the data\n\ndata-weather.R: Weather data (Chapter 1  Weather Data)\ndata-agriculture-calendar.R: Agricultural calendars (Section 2.3.2.1 Calendar)\ndata-agriculture.R: Agricultural production (Chapter 2  Agricultural Data)\ndata-macro.R: Macroeconomic Data (Chapter 3  Macroeconomic Data)\ndata-other.R: Natural regions, ENSO (Chapter 4  Other Data)\ndata-merge.R: Merging the datasets to produce the one used in the local projections estimations (Chapter 5  Merging the files)\ndata-desc_stats.R: Descriptive statistics (Chapter 6  Descriptive Statistics).\n\nReplication of the estimations\n\nlocal_proj_linear.R: The Dynamic Effects of Weather Shocks (Chapter 7  The Dynamic Effects of Weather Shocks)\nlocal_proj_quadratic.R: Quadratic Terms (Chapter 8  Quadratic Terms)\nlocal_proj_seasonal.R: Time-varying exposure to weather shocks (Chapter 9  Time-varying exposure to weather shocks)\naggregate_fluctuations.R: From Regional to Aggregate Fluctuations (Chapter 10  From Regional to Aggregate Fluctuations).\n\nRobustness checks: Data Frequency\n\nrobustness-data-merge-quarter.R: Merging the datasets to produce the one used in the local projections estimations using quarterly data (Chapter 11  Merging: quarterly data)\nrobustness-data-merge-annual.R: Merging the datasets to produce the one used in the local projections estimations using annual data (Chapter 12  Merging: annual data)\nrobustness-local_proj_linear_quarter.R: Agricultural production response to a weather shock (using Local Projections) with quarterly data (Chapter 13  Quarterly Agricultural Production (LP))\nrobustness-local_proj_linear_annual.R: Agricultural production response to a weather shock (using Local Projections) with annual data (Chapter 13  Quarterly Agricultural Production (LP))\nrobustness-local_projections-comparisons.R: Comparison of agricultural response to a weather shock (using Local Projections) according to the data frequency of the agricultural production.\n\nRobustness checks: CHIRPS Data\n\nrobustness-local_proj_linear-chirps.R: Agricultural production response to a weather shock (using Local Projections) with CHIRPS data (Chapter 16  Agricultural Production (LP))\nrobustness-aggregate_fluctuations-chirps.R: Aggregate fluctuations using CHIRPS data (Chapter 17  Aggregate Fluctuations).\n\nRobustness Check: Other\n\nrobustness-local_proj_linear_surprise.R: Agricultural production response to a surprise weather shock (using Local Projections) with monthly data (Chapter 18  Agricultural Production: Positive vs. Negative Surprise Shocks (LP)).\nrobustness-local_proj_until_2014.R: Agricultural production response to a weather shock (using Local Projections) excluding the last year of data (Chapter 19  Agricultural Production (LP): without 2015 data).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "data-weather.html",
    "href": "data-weather.html",
    "title": "1  Weather Data",
    "section": "",
    "text": "1.1 Maps\nTwo shapefiles were downloaded from GEO GPS PERÙ:\nLet us first load the departemental boundaries, using st_read() from {sf}.\nmap_peru &lt;- sf::st_read(\"../data/raw/shapefile_peru/departamentos/\", quiet = T)\nThis map is really heavy. Let us simplify the polygons.\nmap_peru &lt;-\n  rmapshaper::ms_simplify(input = as(map_peru, 'Spatial')) |&gt;\n  st_as_sf()\nThe map can easily be plotted:\nggplot(data = map_peru) +\n  geom_sf()\n\n\n\n\n\n\n\nFigure 1.1: Boundaries of regions in Peru\nNow, let us load the grid covering Peru, once again using st_read():\nmap_peru_grid &lt;- sf::st_read(\n  str_c(\n    \"../data/raw/shapefile_peru/grid/\",\n    \"cuadro de empalme oficial 100k ign peru geogpsperu/\"\n    ),\n  quiet = TRUE\n)\nHere is what it looks like:\nggplot(data = map_peru_grid) + \n  geom_sf(fill = \"#00B0CA\", alpha = .2, colour = \"white\")\n\n\n\n\n\n\n\nFigure 1.2: Grid for Peru\nIf we stack both the departmental boundaries and this grid:\nggplot(data = map_peru_grid) + \n  geom_sf(fill = \"#00B0CA\", alpha = .1, colour = \"white\") +\n  geom_sf(data = map_peru, fill = NA)\n\n\n\n\n\n\n\nFigure 1.3: Regional boundaries in Peru with the grid",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Weather Data</span>"
    ]
  },
  {
    "objectID": "data-weather.html#sec-peru-map",
    "href": "data-weather.html#sec-peru-map",
    "title": "1  Weather Data",
    "section": "",
    "text": "the departmental boundaries\na grid map of the national map.",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Weather Data</span>"
    ]
  },
  {
    "objectID": "data-weather.html#sec-weather-land-use",
    "href": "data-weather.html#sec-weather-land-use",
    "title": "1  Weather Data",
    "section": "1.2 Land Use",
    "text": "1.2 Land Use\nLet us turn to the land cover data. We downloaded from Copernicus the three land cover tiles intersecting Peru for 2015.\nThe resolution is 100m. The raster is only made of 1 layer.\nThe files are located in ../data/land_use/global_land_cover/.\n\nbase_location &lt;- \"../data/raw/land_use/global_land_cover/\"\nend_location &lt;- str_c(\n  \"_PROBAV_LC100_global_v3.0.1_2015-\",\n  \"base_Crops-CoverFraction-layer_EPSG-4326.tif\"\n)\n\nfile &lt;- str_c(base_location, \"W080N00\", end_location)\nfile_2 &lt;- str_c(base_location, \"W100N00\", end_location)\nfile_3 &lt;- str_c(base_location, \"W080N20\", end_location)\n\nUsing raster() from {raster}, these TIF files can be loaded into R.\n\nraster_land_1 &lt;- raster(file)\nraster_land_2 &lt;- raster(file_2)\nraster_land_3 &lt;- raster(file_3)\n\nThese three tiles can be merged into a single one:\n\nraster_land &lt;- raster::merge(raster_land_1, raster_land_2, raster_land_3)\n\nWe may only focus on the bounding box of Peru:\n\nraster_land_crop &lt;- crop(raster_land, map_peru)\n\nThe resulting raster can be saved for later use.\n\nraster::writeRaster(\n  raster_land_crop,\n  filename = str_c(\n    \"../data/output/land/global_land_cover/\",\n    \"raster_land_2015_cropped.tif\"\n  )\n)\n\nAs this file is pretty large, we will only focus iteratively on fractions of it. To do so, it is easier to load the TIF file with read_stars() from {stars}.\n\nland &lt;- stars::read_stars(\n  \"../data/output/land/global_land_cover/raster_land_2015_cropped.tif\"\n)\n\nThe numerical values associated with each tile of the TIF are the estimated fraction of the 100\\(m^2\\) covered with agricultural land.\n\nnames(land)[1] &lt;- \"percent_cropland\"\n\nLet us create a function that will consider the ith cell of Peru’s grid (as obtained previously in the Map section). We extract the tiles from the land use raster intersecting with the cell, only keeping tiles within the boundaries. Then, we compute the fraction of cropland. The function returns a table with the value of i, the fraction of cropland, and the area of the cell within the boundaries (in squared metres).\n\n#' Get the cover fractions of cropland for the ith cell of Peru's grid\n#' \n#' @param i index of the cell in `map_peru_grid`\nget_percent_cropland_cell &lt;- function(i) {\n  # Cell from the grid of Peru\n  poly_cell &lt;- map_peru_grid$geometry[i]\n  # Part of that cell within the boundaries\n  poly_cell_in_map &lt;- st_intersection(poly_cell, map_peru)\n  poly_cell_in_map_area &lt;- st_area(poly_cell_in_map) |&gt; sum()\n  # Cropping land cover data\n  land_crop_try &lt;- try(land_crop &lt;- st_crop(land, poly_cell) |&gt; st_as_sf())\n  if (inherits(land_crop_try, \"try-error\")) {\n    new_bbox_border &lt;- function(bbox){\n      if (bbox[[\"xmin\"]] &lt; st_bbox(land)[[\"xmin\"]]) {\n        bbox[[\"xmin\"]] &lt;- st_bbox(land)[[\"xmin\"]]\n      }\n      \n      if (bbox[[\"xmax\"]] &gt; st_bbox(land)[[\"xmax\"]]) {\n        bbox[[\"xmax\"]] &lt;- st_bbox(land)[[\"xmax\"]]\n      }\n      \n      bbox\n    }\n    new_bbox_poly_cell &lt;- new_bbox_border(st_bbox(poly_cell))\n    land_crop &lt;- \n      st_crop(land, new_bbox_poly_cell) |&gt;\n      st_as_sf()\n  }\n  # land cover data within the part of the grid cell within the boundaries\n  land_within_cell &lt;- st_intersection(land_crop, poly_cell_in_map)\n  \n  tibble(\n    i = i, \n    percent_cropland = mean(land_within_cell$percent_cropland),\n    area_cell = poly_cell_in_map_area\n  )\n}\n\nThis function just needs to be applied to each cell of the grid. It takes a few hours on a standard 2021 computer.\n\npercent_cropland_cells &lt;- vector(mode = \"list\", length = nrow(map_peru_grid))\npb &lt;- txtProgressBar(min = 0, max = nrow(map_peru_grid), style = 3)\nfor (i in 1:nrow(map_peru_grid)) {\n  percent_cropland_cells[[i]] &lt;- get_percent_cropland_cell(i)\n  setTxtProgressBar(pb, i)\n}\npercent_cropland_cells &lt;- bind_rows(percent_cropland_cells)\n\nLet us save this object.\n\nsave(\n  percent_cropland_cells, \n  file = \"../data/output/land/percent_cropland_cells.rda\"\n)\n\n\npercent_cropland_cells\n\n# A tibble: 501 × 3\n       i percent_cropland   area_cell\n   &lt;int&gt;            &lt;dbl&gt;       [m^2]\n 1     1          0         92458048.\n 2     2          0.00965 2086260528.\n 3     3          0.00555 1274189377.\n 4     4          0         11536878.\n 5     5          0.00169 1568751829.\n 6     6          0.00147 3077021588.\n 7     7          0.00686 1044813210.\n 8     8          0         41984846.\n 9     9          0.00243 2873299529.\n10    10          0.0106  3076565442.\n# ℹ 491 more rows\n\n\nLet us associate these values to each cell of the grid:\n\nmap_peru_grid_agri &lt;- map_peru_grid\nmap_peru_grid_agri$i &lt;- 1:nrow(map_peru_grid_agri)\n\nmap_peru_grid_agri &lt;- \n  map_peru_grid_agri |&gt; \n  left_join(percent_cropland_cells)\n\nJoining with `by = join_by(i)`\n\n\nThe area of the cell is expressed in squared meters. Let us convert those values to squared kilometres. Then, for each cell, let us compute the cropland area.\n\nmap_peru_grid_agri &lt;- \n  map_peru_grid_agri |&gt; \n  mutate(area_cell_sqkm = units::drop_units(area_cell)*10^-6) |&gt; \n  mutate(cropland = (percent_cropland/100) * area_cell_sqkm)\nsave(map_peru_grid_agri, file = \"../data/output/land/map_peru_grid_agri.rda\")\n\nPeru’s area is 1,285,216 km2. The grid (some cells of which are partially outside the boundaries) area is:\n\nscales::number(sum(map_peru_grid_agri$area_cell_sqkm), big.mark = \",\")\n\n[1] \"1,286,400\"\n\n\nWhile this value makes sense, we get an odd value for the agricultural land. According to the World Bank, agricultural lands correspond to roughly 18% of total land in Peru. We only get 1.7%.\n\n100 * sum(map_peru_grid_agri$cropland) / \n  sum(map_peru_grid_agri$area_cell_sqkm) # far from it...\n\n[1] 1.667849\n\n\nHowever, what we are interested in here is the relative cropland area. Let us have a look at that.\n\np &lt;- \n  ggplot() +\n  geom_sf(\n    data = map_peru_grid_agri,\n    mapping = aes(fill = cropland), colour = \"white\"\n  ) +\n  geom_sf(data = map_peru, fill = NA) +\n  scale_fill_gradient2(\n    \"Cropland (squared meters)\",\n    low = \"white\", high = \"#61C250\"\n  )\np\n\n\n\n\n\n\n\nFigure 1.4: Agricultural land use in Peru\n\n\n\n\n\nEven if the values do not add up to 18% of total land, the relative values seem to be close to what can be observed on the Google Earth Engine:\nHere is the code used to reproduce that map on Google Earth Engine:\n\nvar dataset = ee.Image(\"COPERNICUS/Landcover/100m/Proba-V-C3/Global/2019\")\n.select('discrete_classification');\n\nMap.setCenter(-88.6, 26.4, 1);\n\nMap.addLayer(dataset, {}, \"Land Cover\");\n\nvar worldcountries = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017');\nvar filterCountry = ee.Filter.eq('country_na', 'Peru');\nvar country = worldcountries.filter(filterCountry);\nvar styling = {color: 'red', fillColor: '00000000'};\nMap.addLayer(country.style(styling));\nMap.centerObject(country, 5);\n\n\n\"figs/land_use_google_earth_Engine.png\" |&gt; \n  knitr::include_graphics()\n\n\n\n\n\n\n\nFigure 1.5: Land Use in Peru as displayed on Google Earth Engine",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Weather Data</span>"
    ]
  },
  {
    "objectID": "data-weather.html#weather-data",
    "href": "data-weather.html#weather-data",
    "title": "1  Weather Data",
    "section": "1.3 Weather Data",
    "text": "1.3 Weather Data\n\n1.3.1 Rainfall (Piscop)\nThe rainfall data are obtained from the gridded daily precipitation dataset PISCOp (Aybar et al. (2020)).\nNote: this data source was added during the reviewing process to provide a robustness check kindly suggested by a reviewer.\nFirst, we load the raster data corresponding to the daily precipitation:\n\nprcp_pisco_sf_raster &lt;- raster::raster(\n  \"data/weather/Piscop//PISCOpd.nc\"\n)\n\nThen, we extract the start date of the data and create the vector of dates for the observations:\n\nstart_date &lt;- ymd(\"1981-01-01\")\nnbands &lt;- prcp_pisco_sf_raster@file@nbands\ndates &lt;- seq(start_date, by=\"day\", length.out = nbands)\ndates_tbl &lt;- tibble(date = dates) |&gt; \n  mutate(time = row_number())\n\nWe load the data:\n\nlibrary(tidync)\nprcp &lt;- tidync::tidync(\"./data/weather/Piscop//PISCOpd.nc\")\n\nWe will loop over the cells of the map of Peru. For each cell, we will load chunks of data iteratively (because the size of the data is too large to be loaded in memory). Let us consider 500 chunks.\n\nchunk_size &lt;- 500\na_parcourir &lt;- tibble(\n  start = c(1, seq(1, length(dates))[seq(1, length(dates)) %% chunk_size == 0])\n  ) |&gt; \n  mutate(end = lead(start)-1)\n\n\nif (last(a_parcourir$start) &lt; length(dates)) {\n  a_parcourir &lt;- a_parcourir |&gt; \n    mutate(end = ifelse(is.na(end), yes = length(dates), no = end))\n}\n\nOnce we have created the a_parcourir object that identifies the dates of the elements in each of the 500 chunks, we can loop over the grid cells.\n\nprcp_grille_k &lt;- vector(mode = \"list\", length = nrow(map_peru_grid))\npb &lt;- txtProgressBar(min = 0, max = nrow(map_peru_grid), style = 3)\n# Loop over the cells  of the grid\nfor (k in 1:nrow(map_peru_grid)) {\n  # Identifying the kth cell\n  cell &lt;- map_peru_grid[k,]\n  \n  \n  grid_prcp &lt;- \n    prcp |&gt; \n    tidync::hyper_filter(z = z == 0) |&gt; \n    tidync::hyper_tibble() |&gt; \n    sf::st_as_sf(\n      coords = c(\"longitude\", \"latitude\"),\n      crs = \"+init=EPSG:4326\",\n      remove = FALSE\n    )\n  # Keeping only data in the kth cell: recovering coordinates\n  ind_cell &lt;- st_contains(cell, grid_prcp)[[1]]\n  \n  coords_keep &lt;- \n    grid_prcp |&gt; \n    slice(ind_cell) |&gt; \n    as_tibble() |&gt; \n    dplyr::select(longitude, latitude)\n  \n  prcp_grid_j &lt;- vector(mode=\"list\", length = nrow(a_parcourir))\n  # Loop over chunks of dates\n  for(j in 1:nrow(a_parcourir)){\n    start &lt;- a_parcourir$start[j]\n    end &lt;- a_parcourir$end[j]\n    \n    prcp_grid_j[[j]] &lt;- \n      prcp |&gt; \n      tidync::hyper_filter(z = dplyr::between(z, start, end)) |&gt;\n      tidync::hyper_tibble() |&gt; \n      semi_join(coords_keep, by = c(\"longitude\", \"latitude\")) |&gt; \n      rename(time = z) |&gt; \n      dplyr::left_join(dates_tbl, by = \"time\") |&gt; \n      dplyr::filter(!is.na(date)) |&gt; \n      dplyr::select(variable, date) |&gt; \n      group_by(date) |&gt; \n      summarise(variable = mean(variable, na.rm=TRUE)) |&gt; \n      dplyr::mutate(grid_id = k)\n  }\n  \n  prcp_grid_j &lt;- bind_rows(prcp_grid_j)\n  \n  prcp_grille_k[[k]] &lt;- prcp_grid_j\n  setTxtProgressBar(pb, k)\n}\n\nprcp_grille &lt;- bind_rows(prcp_grille_k)\n\nsave(\n  prcp_grille,\n  file = \"../data/output/weather/PISCO_precip/prcp_grille_piscop.rda\"\n)\n\n\nprcp_grille\n\n# A tibble: 6,587,148 × 3\n   date       variable grid_id\n   &lt;date&gt;        &lt;dbl&gt;   &lt;int&gt;\n 1 1981-01-01    2.42        1\n 2 1981-01-02    2.25        1\n 3 1981-01-03    1.75        1\n 4 1981-01-04    0.264       1\n 5 1981-01-05   10.6         1\n 6 1981-01-06   23.0         1\n 7 1981-01-07   36.4         1\n 8 1981-01-08   10.4         1\n 9 1981-01-09    3.05        1\n10 1981-01-10    6.06        1\n# ℹ 6,587,138 more rows\n\n\n\n\n1.3.2 Rainfall (Chirps)\nThe rainfall data are obtained from the CHIRPS v2.0 database, made available by the Climate Hazards Center of the UC Santa Barbara. Covering the quasi totality of the globe, the data set provides daily information on rainfall on a 0.05° resolution satellite imagery, from 1981 to present. The complete presentation of the data can be found on the Climate Hazards Center’s website Funk et al. (2015) and the data set is freely available online.\n\nN &lt;- list.files(\n  \"data/raw/weather/CHIRPS-2_0_global_daily_p25\", \n  pattern = \"\\\\.nc$\", full.names = TRUE\n)\n\nLet us loop over the files (one per year). At each iteration, let us save the intermediate results.\n\nfor (ii in 1:length(N)) {\n  file &lt;- N[ii]\n  precip_sf_raster &lt;- raster::raster(file)\n  proj4string(precip_sf_raster) &lt;- CRS(\"+init=EPSG:4326\")\n  \n  precip_sf &lt;- \n    stars::st_as_stars(precip_sf_raster) |&gt; \n    sf::st_as_sf()\n  \n  sf::sf_use_s2(FALSE)\n  precip_sf$geometry &lt;-\n    precip_sf$geometry |&gt;\n    s2::s2_rebuild() |&gt;\n    sf::st_as_sfc()\n  \n  \n  precip_grille_i &lt;- vector(mode = \"list\", length = nrow(map_peru_grid))\n  pb &lt;- txtProgressBar(min=0, max=nrow(map_peru_grid), style=3)\n  for (k in 1:nrow(map_peru_grid)) {\n    \n    # Average precipitations in the k-th cell\n    tmp &lt;- suppressMessages(\n      st_interpolate_aw(precip_sf, map_peru_grid[k,], extensive = FALSE)\n    )\n    # Each column of `tmp` gives the average for a given date\n    tmp &lt;- tibble(tmp)\n    \n    # Let us transform `tmp` so that each row gives the average precipitations \n    # in the k-th cell, for a specific date\n    \n    tmp_2 &lt;- \n      tmp |&gt; \n      tidyr::pivot_longer(cols = -c(geometry), names_to = \"date_v\") |&gt; \n      dplyr::mutate(date = str_remove(date_v, \"^X\") |&gt; lubridate::ymd()) |&gt; \n      dplyr::select(-date_v, -geometry) |&gt; \n      dplyr::mutate(grid_id = k)\n    # Add the ID of the k-th cell\n    precip_grille_i[[k]] &lt;- tmp_2\n    setTxtProgressBar(pb, k)\n  }\n  \n  precip_grille &lt;- \n    precip_grille_i |&gt; bind_rows()\n  \n  file_name &lt;- str_replace(file, \"\\\\.nc$\", \".rda\")\n  file_name &lt;- str_replace(file_name, \"data/raw/\", \"data/output/\")\n  \n  save(precip_grille, file = file_name)\n}\n\nThe intermediate results can be loaded again, and merged to a single tibble:\n\nN &lt;- list.files(\n  \"../data/output/weather/CHIRPS-2_0_global_daily_p25\", \n  pattern = \"days_p25\\\\.rda$\", \n  full.names = TRUE\n)\nload_precip &lt;- function(x) {load(x) ; precip_grille}\nprecip_grille &lt;- map(N, load_precip) |&gt; list_rbind()\n\nThe resulting tibble can be saved:\n\nsave(\n  precip_grille,\n  file = \"../data/output/weather/CHIRPS-2_0_global_daily_p25/precip_grille.rda\"\n)\n\n\nprecip_grille\n\n# A tibble: 7,441,353 × 3\n    value date       grid_id\n   [mm/d] &lt;date&gt;       &lt;int&gt;\n 1  1.76  1981-01-01       1\n 2  0.702 1981-01-02       1\n 3  0     1981-01-03       1\n 4  0     1981-01-04       1\n 5  0     1981-01-05       1\n 6  5.09  1981-01-06       1\n 7 10.3   1981-01-07       1\n 8 37.6   1981-01-08       1\n 9 23.8   1981-01-09       1\n10  0     1981-01-10       1\n# ℹ 7,441,343 more rows\n\n\n\n\n1.3.3 Temperatures\nWe use the PISCO temperature dataset version 1.1c (PISCOt). The PISCOt dataset represents a gridded product of maximum (tx) temperature and minimum (tn) temperature at a spatial resolution of 0.1° for Peru. It is available at daily (d), monthly (m), and annual (a) scales. In this analysis, we utilize the daily values of the PISCOt dataset. More details can be found on Github.\n\n1.3.3.1 Minimum Temperatures\nFirst, we load the raster data corresponding to the daily minimum temperatures:\n\ntmin_sf_raster &lt;- raster::raster(\n  \"../data/raw/weather/PISCO_temperature/PISCOdtn_v1.1.nc\"\n)\n\nThen, we extract the start date of the data and create the vector of dates for the observations:\n\nstart_date &lt;- tmin_sf_raster@z[[1]] |&gt; ymd()\nnbands &lt;- tmin_sf_raster@file@nbands\ndates &lt;- seq(start_date, by=\"day\", length.out = nbands)\ndates_tbl &lt;- tibble(date = dates) |&gt; \n  mutate(time = row_number())\n\nWe load the data:\n\nlibrary(tidync)\ntmin &lt;- tidync::tidync(\"./data/raw/weather/PISCO_temperature/PISCOdtn_v1.1.nc\")\n\nWe will loop over the cells of the map of Peru. For each cell, we will load chunks of data iteratively (because the size of the data is too large to be loaded in memory). Let us consider 500 chunks.\n\nchunk_size &lt;- 500\na_parcourir &lt;- tibble(\n  start = c(1, seq(1, length(dates))[seq(1, length(dates)) %% chunk_size == 0])\n  ) |&gt; \n  mutate(end = lead(start)-1)\n\n\nif (last(a_parcourir$start) &lt; length(dates)) {\n  a_parcourir &lt;- a_parcourir |&gt; \n    mutate(end = ifelse(is.na(end), yes = length(dates), no = end))\n}\n\nOnce we have created the a_parcourir object that identifies the dates of the elements in each of the 500 chunks, we can loop over the grid cells.\n\ntmin_grille_k &lt;- vector(mode = \"list\", length = nrow(map_peru_grid))\npb &lt;- txtProgressBar(min=0, max=nrow(map_peru_grid), style=3)\n# Loop over the cells  of the grid\nfor (k in 1:nrow(map_peru_grid)) {\n  # Identifying the kth cell\n  cell &lt;- map_peru_grid[k,]\n  \n  grid_tmin &lt;- \n    tmin |&gt; \n    tidync::hyper_filter(time = time ==0) |&gt; \n    tidync::hyper_tibble() |&gt; \n    sf::st_as_sf(\n      coords = c(\"longitude\", \"latitude\"),\n      crs=\"+init=EPSG:4326\",\n      remove=FALSE\n    )\n  # Keeping only data in the kth cell: recovering coordinates\n  ind_cell &lt;- st_contains(cell, grid_tmin)[[1]]\n  \n  coords_keep &lt;- \n    grid_tmin |&gt; \n    slice(ind_cell) |&gt; \n    as_tibble() |&gt; \n    dplyr::select(longitude, latitude)\n  \n  tmin_grid_j &lt;- vector(mode=\"list\", length = nrow(a_parcourir))\n  # Loop over chunks of dates\n  for(j in 1:nrow(a_parcourir)){\n    start &lt;- a_parcourir$start[j]\n    end &lt;- a_parcourir$end[j]\n    \n    tmin_grid_j[[j]] &lt;- \n      tmin |&gt; \n      tidync::hyper_filter(time = dplyr::between(time, start, end)) |&gt;\n      tidync::hyper_tibble() |&gt; \n      semi_join(coords_keep, by = c(\"longitude\", \"latitude\")) |&gt; \n      dplyr::left_join(dates_tbl, by = \"time\") |&gt; \n      dplyr::filter(!is.na(date)) |&gt; \n      dplyr::select(tn, date) |&gt; \n      group_by(date) |&gt; \n      summarise(tn = mean(tn, na.rm=TRUE)) |&gt; \n      dplyr::mutate(grid_id = k)\n  }\n  \n  tmin_grid_j &lt;- bind_rows(tmin_grid_j)\n  \n  tmin_grille_k[[k]] &lt;- tmin_grid_j\n  setTxtProgressBar(pb, k)\n}\n\ntmin_grille &lt;- bind_rows(tmin_grille_k)\n\nsave(\n  tmin_grille,\n  file = \"data/output/weather/PISCO_temperature/tmin_grille.rda\"\n)\n\n\n\n1.3.3.2 Maximum Temperatures\nWe follow the same procedure for the maximum temperatures.\n\ntmax_sf_raster &lt;- raster::raster(\n  \"data/raw/weather/PISCO_temperature/PISCOdtx_v1.1.nc\"\n)\n\nstart_date &lt;- tmax_sf_raster@z[[1]] |&gt; ymd()\nnbands &lt;- tmax_sf_raster@file@nbands\ndates &lt;- seq(start_date, by=\"day\", length.out = nbands)\ndates_tbl &lt;- tibble(date = dates) |&gt; \n  mutate(time = row_number())\ndates_tbl\n\nlibrary(tidync)\ntmax &lt;- tidync::tidync(\n  \"./data/raw/weather/PISCO_temperature/PISCOdtx_v1.1.nc\"\n)\n\nchunk_size &lt;- 500\na_parcourir &lt;- tibble(\n  start = c(1, seq(1, length(dates))[seq(1, length(dates)) %% chunk_size == 0])\n) |&gt; \n  mutate(end = lead(start) - 1)\n\nif (last(a_parcourir$start) &lt; length(dates)) {\n  a_parcourir &lt;- a_parcourir |&gt; \n    mutate(end = ifelse(is.na(end), yes = length(dates), no = end))\n}\n\n\ntmax_grille_k &lt;- vector(mode = \"list\", length = nrow(map_peru_grid))\npb &lt;- txtProgressBar(min=0, max=nrow(map_peru_grid), style = 3)\n# Loop over the cells  of the grid\nfor(k in 1:nrow(map_peru_grid)){\n  # Identifying the kth cell\n  cell &lt;- map_peru_grid[k,]\n  \n  grid_tmax &lt;- \n    tmax |&gt; \n    tidync::hyper_filter(time = time ==0) |&gt; \n    tidync::hyper_tibble() |&gt; \n    sf::st_as_sf(\n      coords = c(\"longitude\", \"latitude\"),\n      crs=\"+init=EPSG:4326\",\n      remove=FALSE\n    )\n  # Keeping only data in the kth cell: recovering coordinates\n  ind_cell &lt;- st_contains(cell, grid_tmax)[[1]]\n  \n  coords_keep &lt;- \n    grid_tmax |&gt; \n    slice(ind_cell) |&gt; \n    as_tibble() |&gt; \n    dplyr::select(longitude, latitude)\n  \n  tmax_grid_j &lt;- vector(mode=\"list\", length = nrow(a_parcourir))\n  # Loop over chunks of dates\n  for (j in 1:nrow(a_parcourir)) {\n    start &lt;- a_parcourir$start[j]\n    end &lt;- a_parcourir$end[j]\n    \n    tmax_grid_j[[j]] &lt;- \n      tmax |&gt; \n      tidync::hyper_filter(time = dplyr::between(time, start, end)) |&gt;\n      tidync::hyper_tibble() |&gt; \n      semi_join(coords_keep, by = c(\"longitude\", \"latitude\")) |&gt; \n      dplyr::left_join(dates_tbl, by = \"time\") |&gt; \n      dplyr::filter(!is.na(date)) |&gt; \n      dplyr::select(tx, date) |&gt; \n      group_by(date) |&gt; \n      summarise(tx = mean(tx, na.rm=TRUE)) |&gt; \n      dplyr::mutate(grid_id = k)\n  }\n  \n  tmax_grid_j &lt;- bind_rows(tmax_grid_j)\n  \n  tmax_grille_k[[k]] &lt;- tmax_grid_j\n  setTxtProgressBar(pb, k)\n}\n\ntmax_grille &lt;- bind_rows(tmax_grille_k)\n\nsave(\n  tmax_grille, \n  file = \"data/output/weather/PISCO_temperature/tmax_grille.rda\"\n)\n\n\n\n1.3.3.3 Merging Temperatures\nLastly, we merge the daily temperature data at the grid level together in a single tibble.\n\ntemperatures_grid &lt;- \n  tmax_grille |&gt; \n  left_join(tmin_grille)\n\nAnd we define daily mean temperatures as the average between min and max temperatures:\n\ntemperatures_grid &lt;- \n  temperatures_grid |&gt; \n  rename(temp_max = tx, temp_min = tn) |&gt; \n  mutate(temp_mean = (temp_min + temp_max) / 2)\n\nLet us save the observations:\n\nsave(\n  temperatures_grid, \n  file = \"../data/output/weather/PISCO_temperature/temperatures_grid.rda\"\n)\n\nLet us visualize the data at the grid level. We only look at the example of 2010. We compute the monthly average of daily mean, min, and maximum temperatures.\n\nmap_peru_grid_temp &lt;- \n  map_peru_grid |&gt; \n  mutate(grid_id = row_number()) |&gt; \n  left_join(\n    temperatures_grid |&gt; \n      filter(year(date) == 2010) |&gt; \n      mutate(\n        month = month(date, abbr = FALSE, label = TRUE, locale = \"en_US\")\n      ) |&gt; \n      # Monthly average\n      group_by(grid_id, month) |&gt; \n      summarise(\n        mean_temp_mean = mean(temp_mean),\n        mean_temp_min = mean(temp_min),\n        mean_temp_max = mean(temp_max),\n        .groups = \"drop\"\n      )\n  )\n\nJoining with `by = join_by(grid_id)`\n\n\nTo have a similar color scale for each variable, let us extract the range of values:\n\nrange_temperatures &lt;- c(\n  min(map_peru_grid_temp$mean_temp_min, na.rm = TRUE),\n  max(map_peru_grid_temp$mean_temp_max, na.rm = TRUE)\n)\n\n\nMean temp.Min temp.Max temp.\n\n\n\n\nCode\np_tmp_mean &lt;- \n  ggplot() +\n  geom_sf(\n    data = map_peru_grid_temp |&gt; filter(!is.na(mean_temp_mean)),\n    mapping = aes(fill = mean_temp_mean)\n  ) +\n  geom_sf(data = map_peru, fill = NA) +\n  scale_fill_gradient2(\n    low = \"#005A8B\", mid = \"white\", high = \"#AA2F2F\", \n    midpoint = 0, limits = range_temperatures\n  ) +\n  facet_wrap(~month)\np_tmp_mean\n\n\n\n\n\n\n\n\nFigure 1.6: Monthly Mean of Daily Average Temperature 2010 in Peru\n\n\n\n\n\n\n\n\n\nCode\np_tmp_min &lt;-\n  ggplot() +\n  geom_sf(\n    data = map_peru_grid_temp |&gt; filter(!is.na(mean_temp_min)),\n    mapping = aes(fill = mean_temp_min)\n  ) +\n  geom_sf(data = map_peru, fill = NA) +\n  scale_fill_gradient2(\n    low = \"#005A8B\", mid = \"white\", high = \"#AA2F2F\",\n    midpoint = 0, limits = range_temperatures\n  ) +\n  facet_wrap(~month)\np_tmp_min\n\n\n\n\n\n\n\n\nFigure 1.7: Monthly Mean of Daily Minimum Temperature 2010 in Peru\n\n\n\n\n\n\n\n\n\nCode\np_tmp_max &lt;- \n  ggplot() +\n  geom_sf(\n    data = map_peru_grid_temp |&gt; filter(!is.na(mean_temp_max)),\n    mapping = aes(fill = mean_temp_max)\n  ) +\n  geom_sf(data = map_peru, fill = NA) +\n  scale_fill_gradient2(\n    low = \"#005A8B\", mid = \"white\", high = \"#AA2F2F\", \n    midpoint = 0, limits = range_temperatures,\n  ) +\n  facet_wrap(~month)\np_tmp_max\n\n\n\n\n\n\n\n\nFigure 1.8: Monthly Mean of Daily Maximym Temperature 2010 in Peru",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Weather Data</span>"
    ]
  },
  {
    "objectID": "data-weather.html#creation-of-variables-on-the-grid",
    "href": "data-weather.html#creation-of-variables-on-the-grid",
    "title": "1  Weather Data",
    "section": "1.4 Creation of Variables on the Grid",
    "text": "1.4 Creation of Variables on the Grid\nLet us merge the temperature and precipitation data in a single tibble:\n\nweather_peru_daily_grid &lt;- \n  precip_grille |&gt; \n  rename(precip = value) |&gt; \n  left_join(temperatures_grid, by = c(\"date\", \"grid_id\")) |&gt; \n  left_join(\n    prcp_grille |&gt; rename(precip_piscop = variable),\n    by = c(\"date\", \"grid_id\")\n  )\n\nThere are some issues with the 501th cell Let us remove that cell.\n\nweather_peru_daily_grid &lt;- \n  weather_peru_daily_grid |&gt; \n  filter(grid_id != 501)\n\n\n1.4.1 Degree Days\nFollowing Aragón, Oteiza, and Rud (2021), we compute degree days, with crop-specific threshold values. \\[\n\\text{DD} = \\frac{1}{n} \\sum_{d=1}^{n}\\left(\\min{(h_d, \\tau)} - \\tau_{\\ell}\\right) \\mathbf{1}(h_d\\geq 8),\n\\] where \\(h_d\\) is the average observed temperature observed in day \\(d\\), \\(n\\) is the number of days in the period of interest (month, quarter, or year), \\(\\tau\\) is a crop-specific threshold set to 29 for rice and maize and set to 30 for potato and cassava. The parameter \\(\\tau_{\\ell}\\) is a lower bound set to 8 for all crops.\nDegree days capture the cumulative exposure to temperatures comprised between the lower bound \\(\\tau_{\\ell}\\) and the upper bound \\(\\tau\\).\n\n# Degree Days\nweather_peru_daily_grid &lt;- \n  weather_peru_daily_grid |&gt; \n  mutate(\n    gdd_daily_rice = temp_mean - 8,\n    gdd_daily_rice = ifelse(temp_mean &gt; 29, yes = 21, no = gdd_daily_rice),\n    gdd_daily_rice = ifelse(temp_mean &lt; 8, yes = 0, no = gdd_daily_rice),\n    #\n    gdd_daily_maize = temp_mean - 8,\n    gdd_daily_maize = ifelse(temp_mean &gt; 29, yes = 21, no = gdd_daily_maize),\n    gdd_daily_maize = ifelse(temp_mean &lt; 8, yes = 0, no = gdd_daily_maize),\n    #\n    gdd_daily_potato = temp_mean - 10,\n    gdd_daily_potato = ifelse(temp_mean &gt; 30, yes = 20, no = gdd_daily_potato),\n    gdd_daily_potato = ifelse(temp_mean &lt; 10, yes = 0, no = gdd_daily_potato),\n    #\n    gdd_daily_cassava = temp_mean - 10,\n    gdd_daily_cassava = ifelse(temp_mean &gt; 30, yes = 20, no = gdd_daily_cassava),\n    gdd_daily_cassava = ifelse(temp_mean &lt; 10, yes = 0, no = gdd_daily_cassava)\n  )\n\n\n\n1.4.2 Harmful Degree Days\nWe also define harmful degree days (Aragón, Oteiza, and Rud (2021)): \\[\n\\text{HDD} = \\frac{1}{n}\\sum_{d=1}^{n} = \\left(h_d - \\tau_{\\text{high}}\\right) \\mathbf{1}(h_d &gt; \\tau_\\ell),\n\\] where \\(\\tau_{\\text{high}}\\) is a crop-specific threshold set to 29 for rice and maize and set to 30 for potato and cassava.\n\nweather_peru_daily_grid &lt;- \n  weather_peru_daily_grid |&gt; \n  mutate(\n    hdd_daily_maize  = ifelse(temp_mean &gt; 29, yes = temp_mean - 29, no = 0), \n    hdd_daily_rice  = ifelse(temp_mean &gt; 29, yes = temp_mean - 29, no = 0), \n    hdd_daily_potato  = ifelse(temp_mean &gt; 30, yes = temp_mean - 30, no = 0), \n    hdd_daily_cassava  = ifelse(temp_mean &gt; 30, yes = temp_mean - 30, no = 0)\n  )\n\n\n\n1.4.3 Cold / Hot / Dry / Wet Surprise Days\nWe follow Natoli (2024) and define cold and hot surprise weather shocks.\nThe idea is to compare the realized temperatures that occurred during a month \\(m\\) of a year \\(y\\) in a location \\(c\\) with the expected realizations calculated from the past observed temperatures during the same month \\(m\\) in previous years at the same location \\(c\\). The difference between these two values is defined as a surprise shock.\nThe shock in cell \\(c\\) is defined as follows for days hotter than expected during month \\(m\\) of year \\(y\\): \\[\n\\mathcal{W}\\text{hot}_{c,y,m} = \\underbrace{\\sum_{d=1}^{n_{m}} \\mathrm{1}\\left(\\mathcal{T}_{c,y,m,d} &gt; \\text{ut}_{c,y,m} \\right)}_{\\text{observed realizations}} - \\underbrace{\\frac{1}{5}\\sum_{k=1}^{5}\\sum_{d=1}^{n_m} \\mathrm{1}\\left(\\mathcal{T}_{c,y-k,m,d} &gt; \\text{ut}_{c,y,m}\\right)}_{\\text{expected realizations}},\n\\tag{1.1}\\]\nwhere \\(\\mathcal{T}_{c,y,m,d}\\) is the average temperature observed on day \\(d\\) of month \\(m\\) in year \\(y\\) in cell \\(c\\), \\(n_m\\) is the number of days in month \\(m\\), and \\(\\text{ut}_{c,y,m}\\) is a threshold above which the daily average temperature is considered hot.\nSimilarly, surprise cold shocks are defined as follows: \\[\n\\mathcal{W}\\text{cold}_{c,y,m} = \\underbrace{\\sum_{d=1}^{n_{m}} \\mathrm{1}\\left(\\mathcal{T}_{c,y,m,d} &lt; \\text{lt}_{c,y,m} \\right)}_{\\text{observed realizations}} - \\underbrace{\\frac{1}{5}\\sum_{k=1}^{5}\\sum_{d=1}^{n_m} \\mathrm{1}\\left(\\mathcal{T}_{c,y-k,m,d} &lt; \\text{lt}_{c,y,m}\\right)}_{\\text{expected realizations}},\n\\tag{1.2}\\]\nwhere \\(\\text{lt}_{c,y,m}\\) is a threshold used to define cold days.\nThe thresholds \\(\\text{ut}_{c,y,m}\\) and \\(\\text{lt}_{c,y,m}\\) are defined using the average temperature observations during month \\(m\\) of the 5 years preceding year \\(y\\), denoted as \\(\\boldsymbol{T}_{c,y,d} = \\left\\{\\{\\mathcal{T}_{c,y-1,m,d}\\}_{d=1}^{n_m}, \\{\\mathcal{T}_{c,y-2,m,d}\\}_{d=1}^{n_m}, \\ldots, \\{\\mathcal{T}_{c,y-5,m,d}\\}_{d=1}^{n_m}\\}\\right\\}\\). Based on the observations \\(\\boldsymbol{T}_{c,y,d}\\), the empirical 10th and 90th percentiles are calculated, \\(P_{10}(\\boldsymbol{T}_{c,y,d})\\) and \\(P_{90}(\\boldsymbol{T}_{c,y,d})\\).\nIn Natoli (2024), the thresholds are constructed as follows: \\[\n\\begin{align}\n\\text{ut}_{c,y,m} &= \\max\\left\\{P_{90}(\\boldsymbol{T}_{c,y,d}), \\tau_{\\text{upper}}\\right\\}\\\\\n\\text{lt}_{c,y,m} &= \\min\\left\\{P_{10}(\\boldsymbol{T}_{c,y,d}), \\tau_{\\text{lower}}\\right\\}\n\\end{align}\n\\] where \\(\\tau_{\\text{upper}}\\) and \\(\\tau_{\\text{lower}}\\) are arbitrarily fixed values. Here, we rely on agronomic literature to choose the values: \\(\\tau_{\\text{upper}} = 29^\\circ\\)C for rice and maize, and \\(\\tau_{\\text{upper}} = 30^\\circ\\)C for potatoes and cassava. For \\(\\tau_{\\text{lower}}\\), we set the value at \\(8^\\circ\\)C for rice and maize, and \\(10^\\circ\\)C for potatoes and cassava.\nSimilarly, we define surprise precipitation shocks for dry days as follows: \\[\n\\mathcal{W}\\text{dry}_{c,y,m} = \\underbrace{\\sum_{d=1}^{n_{m}} \\mathrm{1}\\left(\\mathcal{P}_{c,y,m,d} &lt; \\text{lt}_{c,y,m} \\right)}_{\\text{observed realizations}} - \\underbrace{\\frac{1}{5}\\sum_{k=1}^{5}\\sum_{d=1}^{n_m} \\mathrm{1}\\left(\\mathcal{P}_{c,y-k,m,d} &lt; \\text{lt}_{c,y,m}\\right)}_{\\text{expected realizations}},\n\\tag{1.3}\\]\nwhere \\(\\mathcal{P}_{c,y,m,d}\\) represents total precipitation. And for wet days:\n\\[\n\\mathcal{W}\\text{wet}_{c,y,m} = \\underbrace{\\sum_{d=1}^{n_{m}} \\mathrm{1}\\left(\\mathcal{P}_{c,y,m,d} &gt; \\text{ut}_{c,y,m} \\right)}_{\\text{observed realizations}} - \\underbrace{\\frac{1}{5}\\sum_{k=1}^{5}\\sum_{d=1}^{n_m} \\mathrm{1}\\left(\\mathcal{P}_{c,y-k,m,d} &gt; \\text{ut}_{c,y,m}\\right)}_{\\text{expected realizations}},\n\\tag{1.4}\\]\nIn the case of dry/wet days, the thresholds are simply defined using percentiles: \\[\n\\begin{cases}\n\\text{ut}_{c,y,m} & = P_{90}(\\boldsymbol{P}_{c,y,d})\\\\\n\\text{lt}_{c,y,m} & = P_{10}(\\boldsymbol{P}_{c,y,d})\n\\end{cases}\n\\] with \\(\\boldsymbol{P}_{c,y,d} = \\left\\{\\{\\mathcal{P}_{c,y-1,m,d}\\}_{d=1}^{n_m}, \\{\\mathcal{P}_{c,y-2,m,d}\\}_{d=1}^{n_m}, \\ldots, \\{\\mathcal{P}_{c,y-5,m,d}\\}_{d=1}^{n_m}\\}\\right\\}\\) representing the total daily precipitation observed during month \\(m\\) in the 5 years preceding year \\(y\\) in cell \\(c\\).\nLet us implement this in R. First, let us extract the year and month of each observation from the grid.\n\nweather_peru_daily_grid &lt;- \n  weather_peru_daily_grid |&gt; \n  mutate(year = year(date), month = month(date))\n\nThen, we define function get_surprise_w_shocks_monthly() that computes, for a specific month \\(m\\) in year \\(y\\) the surprise shocks for each cell.\n\n#' Computes the cell-level cold surprise and hot surprise with respect to the \n#' daily temperatures in the past years\n#' \n# Natoli, Filippo, The Macroeconomic Effects of Unexpected Temperature Shocks \n# (April 11, 2024). \n# Available at SSRN: https://ssrn.com/abstract=4160944\n#' \n#' @paam year year to consider\n#' @param month month to consider\n#' @param window number of years for the window\n#' @param upper_threshold upper threshold above which a day is considered hot\n#' @param lower_threshold upper threshold above which a day is considered cold\nget_surprise_w_shocks_monthly &lt;- function(year,\n                                          month, \n                                          window, \n                                          upper_threshold, \n                                          lower_threshold) {\n  \n  # Focus on values from the previous `window` years for the same month\n  years_window &lt;- seq(year-window, year-1)\n  weather_window &lt;- weather_peru_daily_grid |&gt; \n    filter(year %in% !!years_window, month == !!month)\n  # Compute 10th and 90th percentiles of daily temperatures for each cell over\n  # the past years\n  # Then compute the expected number of cold/hot days in each cell\n  # (as the average of the number of cold/hot days observed over the window, \n  # in each cell)\n  weather_expected &lt;- \n    weather_window |&gt; \n    nest(.by = grid_id) |&gt; \n    mutate(\n      p10_temp = map(data, ~quantile(.x$temp_mean, probs = .1, na.rm = TRUE)),\n      p90_temp = map(data, ~quantile(.x$temp_mean, probs = .9, na.rm = TRUE)),\n      p10_precip_piscop = map(\n        data, ~quantile(.x$precip_piscop, probs = .1, na.rm = TRUE)\n      ),\n      p90_precip_piscop = map(\n        data, ~quantile(.x$precip_piscop, probs = .9, na.rm = TRUE)\n      )\n    ) |&gt; \n    unnest(cols = c(\n      data, p10_temp, p90_temp, p10_precip_piscop, p90_precip_piscop)\n    ) |&gt; \n    dplyr::select(\n      grid_id, date, year, month, \n      p10_temp, p90_temp, p10_precip_piscop, p90_precip_piscop, \n      temp_mean, precip_piscop\n    ) |&gt; \n    mutate(\n      lt_temp = pmin(p10_temp, lower_threshold, na.rm = TRUE),\n      ut_temp = pmax(p90_temp, upper_threshold, na.rm = TRUE),\n      lt_precip_piscop = p10_precip_piscop,\n      ut_precip_piscop = p90_precip_piscop\n    ) |&gt; \n    mutate(\n      is_cold = temp_mean &lt; lt_temp,\n      is_hot = temp_mean &gt; ut_temp,\n      is_dry = precip_piscop &lt; lt_precip_piscop,\n      is_wet = precip_piscop &gt; ut_precip_piscop\n    ) |&gt; \n    # Number of cold/hot days in the month of each cell, for each year\n    group_by(\n      grid_id, year, month, lt_temp, ut_temp, lt_precip_piscop, ut_precip_piscop\n    ) |&gt; \n    summarise(\n      nb_cold_expected = sum(is_cold, na.rm = TRUE),\n      nb_hot_expected = sum(is_hot, na.rm = TRUE),\n      nb_dry_expected = sum(is_dry, na.rm = TRUE),\n      nb_wet_expected = sum(is_wet, na.rm = TRUE),\n      .groups = \"drop\"\n    ) |&gt; \n    # Average number of cold/hot days in each cell, over the window\n    group_by(\n      grid_id, month, lt_temp, ut_temp, lt_precip_piscop, ut_precip_piscop\n    ) |&gt; \n    summarise(\n      nb_cold_expected = mean(nb_cold_expected, na.rm = TRUE),\n      nb_hot_expected = mean(nb_hot_expected, na.rm = TRUE),\n      nb_dry_expected = mean(nb_dry_expected, na.rm = TRUE),\n      nb_wet_expected = mean(nb_wet_expected, na.rm = TRUE),\n      .groups = \"drop\"\n    )\n  \n  # The weather data for the current (year, month), for all cells\n  weather_current &lt;- \n    weather_peru_daily_grid |&gt; \n    filter(year == !!year, month == !!month)\n  \n  weather_current |&gt; \n    dplyr::select(date, grid_id, temp_mean, precip_piscop, year, month) |&gt; \n    left_join(weather_expected, by = c(\"grid_id\", \"month\")) |&gt; \n    mutate(\n      is_cold = temp_mean &lt; lt_temp,\n      is_hot = temp_mean &gt; ut_temp,\n      is_dry = temp_mean &lt; lt_precip_piscop,\n      is_wet = temp_mean &gt; ut_precip_piscop\n    ) |&gt; \n    group_by(\n      year, month, grid_id, \n      nb_cold_expected, nb_hot_expected, nb_dry_expected, nb_wet_expected\n    ) |&gt; \n    summarise(\n      nb_cold_obs = sum(is_cold, na.rm = TRUE),\n      nb_hot_obs = sum(is_hot, na.rm = TRUE),\n      nb_dry_obs = sum(is_dry, na.rm = TRUE),\n      nb_wet_obs = sum(is_wet, na.rm = TRUE),\n      .groups = \"drop\"\n    ) |&gt; \n    mutate(\n      cold_surprise = nb_cold_obs - nb_cold_expected,\n      hot_surprise = nb_hot_obs - nb_hot_expected,\n      dry_surprise = nb_dry_obs - nb_dry_expected,\n      wet_surprise = nb_wet_obs - nb_wet_expected\n    ) |&gt; \n    dplyr::select(\n      year, month, grid_id, \n      cold_surprise, hot_surprise, dry_surprise, wet_surprise\n    )\n}\n\nSince the get_surprise_w_shocks_monthly() function returns the surprise shocks for a single month of a year (but for all cells of the grid), we need to loop over the different months/years of the sample.\nThe sliding window size is set to 5 (years).\n\n# Five years window\nwindow &lt;- 5\n\nWe prepare a table with the values of the different months/years to consider.\n\ngrid_years_months &lt;- expand_grid(\n  year = seq(min(weather_peru_daily_grid$year) + window, \n             max(weather_peru_daily_grid$year)),\n  month = 1:12\n)\n\nFor rice and maize, we set the upper threshold \\(\\text{ut}_{c,y,m}=29\\) and the lower threshold \\(\\text{lt}_{c,y,m}=8\\)\n\nupper_threshold &lt;- 29\nlower_threshold &lt;- 8\n\nTo run the codes a bit faster, we loop over the grid_years_months using parallel computation.\n\nlibrary(pbapply)\nlibrary(parallel)\nncl &lt;- detectCores()-1\n(cl &lt;- makeCluster(ncl))\n\nclusterEvalQ(cl, {\n  library(tidyverse)\n}) |&gt;\n  invisible()\n\nWe send the required objects and functions to the workers:\n\nclusterExport(\n  cl, c(\n    \"weather_peru_daily_grid\", \"get_surprise_w_shocks_monthly\",\n    \"grid_years_months\", \"window\", \"upper_threshold\", \"lower_threshold\"\n  )\n)\n\nThen, we can loop over the rows of grid_years_months.\n\nsurprise_w_shocks_monthly_rice &lt;- pblapply(\n  1:nrow(grid_years_months),\n  function(i) get_surprise_w_shocks_monthly(\n      year = grid_years_months$year[i], \n      month = grid_years_months$month[i],\n      window = window, \n      upper_threshold = upper_threshold,\n      lower_threshold = lower_threshold\n    ), \n  cl = cl\n) |&gt; \n  list_rbind() |&gt; \n  rename(\n    cold_surprise_rice = cold_surprise,\n    hot_surprise_rice = hot_surprise,\n    dry_surprise_rice = dry_surprise,\n    wet_surprise_rice = wet_surprise\n  )\n\nThe clusters need to be closed at the end.\n\nstopCluster(cl)\n\nThe maize values are identical to that of rice:\n\nsurprise_w_shocks_monthly_maize &lt;- \n  surprise_w_shocks_monthly_rice |&gt; \n  rename(\n    cold_surprise_maize = cold_surprise_rice,\n    hot_surprise_maize = hot_surprise_rice,\n    dry_surprise_maize = dry_surprise_rice,\n    wet_surprise_maize = wet_surprise_rice\n  )\n\nNow, for potato and cassava, the threshold change and are set as follows:\n\nupper_threshold &lt;- 30\nlower_threshold &lt;- 8\n\nAgain, we need to loop over the months/years:\n\ngrid_years_months &lt;- expand_grid(\n  year = seq(min(weather_peru_daily_grid$year) + window, \n             max(weather_peru_daily_grid$year)),\n  month = 1:12\n)\nncl &lt;- detectCores()-1\n(cl &lt;- makeCluster(ncl))\n\nclusterEvalQ(cl, {\n  library(tidyverse)\n}) |&gt;\n  invisible()\n\nclusterExport(\n  cl, c(\n    \"weather_peru_daily_grid\", \"get_surprise_w_shocks_monthly\",\n    \"grid_years_months\", \"window\", \"upper_threshold\", \"lower_threshold\"\n  )\n)\n\nsurprise_w_shocks_monthly_potato &lt;- pblapply(\n  1:nrow(grid_years_months),\n  function(i) get_surprise_w_shocks_monthly(\n    year = grid_years_months$year[i], \n    month = grid_years_months$month[i],\n    window = window, \n    upper_threshold = upper_threshold,\n    lower_threshold = lower_threshold\n  ), \n  cl = cl\n) |&gt; \n  list_rbind() |&gt; \n  rename(\n    cold_surprise_potato = cold_surprise,\n    hot_surprise_potato = hot_surprise,\n    dry_surprise_potato = dry_surprise,\n    wet_surprise_potato = wet_surprise\n  )\n\nstopCluster(cl)\n\nFor cassava, the values are identical to that computed for potatos.\n\nsurprise_w_shocks_monthly_cassava &lt;- \n  surprise_w_shocks_monthly_potato |&gt; \n  rename(\n    cold_surprise_cassava = cold_surprise_potato,\n    hot_surprise_cassava = hot_surprise_potato,\n    dry_surprise_cassava = dry_surprise_potato,\n    wet_surprise_cassava = wet_surprise_potato\n  )\n\nLastly, the surprise weather shocks can be merged in a single tibble.\n\nsurprise_w_shocks_monthly &lt;- \n  surprise_w_shocks_monthly_rice |&gt; \n  left_join(surprise_w_shocks_monthly_maize) |&gt; \n  left_join(surprise_w_shocks_monthly_potato) |&gt; \n  left_join(surprise_w_shocks_monthly_cassava)\n\n\n\n1.4.4 Monthly Aggregation\nTo convert the daily grid data into monthly data, we aggregate the values on a monthly basis while preserving the spatial scale of the grid cells. This allows us to analyze the data at a monthly resolution and maintain consistency with the original grid structure.\nNote that we only keep data from 1986 to 2015.\n\nmonthly_weather_data &lt;- \n  weather_peru_daily_grid |&gt; \n  mutate(\n    # year       = lubridate::year(date),\n    # month      = lubridate::month(date),\n    month_name = lubridate::month(\n      date, abbr = FALSE, label = TRUE, locale = \"en_US\"\n    )\n  ) |&gt; \n  # Keeping 30 years of data\n  filter(year &gt;= 1986, year &lt;= 2015) |&gt; \n  # Monthly aggregates, at the grid cell level\n  group_by(year, month, grid_id) |&gt; \n  summarise(\n    # Average min temperature\n    temp_min = mean(temp_min, na.rm = TRUE),\n    # Average max temperature\n    temp_max = mean(temp_max, na.rm = TRUE),\n    # Average mean temperature\n    temp_mean = mean(temp_mean, na.rm = TRUE),\n    # Total rainfall\n    precip_sum = sum(precip, na.rm = TRUE),\n    # Total rainfall piscop\n    precip_piscop_sum = sum(precip_piscop, na.rm = TRUE),\n    # Monthly DD\n    gdd_rice = sum(gdd_daily_rice, na.rm = TRUE),\n    gdd_maize = sum(gdd_daily_maize, na.rm = TRUE),\n    gdd_potato = sum(gdd_daily_potato, na.rm = TRUE),\n    gdd_cassava = sum(gdd_daily_cassava, na.rm = TRUE),\n    # Monthly HDD\n    hdd_maize = sum(hdd_daily_maize, na.rm = TRUE),\n    hdd_rice = sum(hdd_daily_rice, na.rm = TRUE),\n    hdd_potato = sum(hdd_daily_potato, na.rm = TRUE),\n    hdd_cassava = sum(hdd_daily_cassava, na.rm = TRUE)\n  )\n\nWe do not need to keep track of the unit for the weather data. Let us drop that information.\n\nmonthly_weather_data &lt;- units::drop_units(monthly_weather_data)\n\nWe add surprise weather shocks:\n\nmonthly_weather_data &lt;- \n  monthly_weather_data |&gt; \n  left_join(surprise_w_shocks_monthly)\n\n\n\n1.4.5 Quarterly Aggregation\nTo convert the daily grid data into quarterly data, we aggregate the values on a quarter basis while preserving the spatial scale of the grid cells. Note: we only keep data from 1986 to 2015.\n\nquarterly_weather_data &lt;- \n  weather_peru_daily_grid |&gt; \n  mutate(\n    # year       = lubridate::year(date),\n    quarter = lubridate::quarter(date)\n  ) |&gt; \n  # Keeping 30 years of data\n  filter(year &gt;= 1986, year &lt;= 2015) |&gt; \n  # Quartlery aggregates, at the grid cell level\n  group_by(year, quarter, grid_id) |&gt; \n  summarise(\n    # Average min temperature\n    temp_min = mean(temp_min, na.rm = TRUE),\n    # Average max temperature\n    temp_max = mean(temp_max, na.rm = TRUE),\n    # Average mean temperature\n    temp_mean = mean(temp_mean, na.rm = TRUE),\n    # Total rainfall\n    precip_sum = sum(precip, na.rm = TRUE),\n    # Total rainfall piscop\n    precip_piscop_sum = sum(precip_piscop, na.rm = TRUE),\n    # Quarterly GDD\n    gdd_rice = sum(gdd_daily_rice, na.rm = TRUE),\n    gdd_maize = sum(gdd_daily_maize, na.rm = TRUE),\n    gdd_potato = sum(gdd_daily_potato, na.rm = TRUE),\n    gdd_cassava = sum(gdd_daily_cassava, na.rm = TRUE),\n    # Quarterly HDD\n    hdd_maize = sum(hdd_daily_maize, na.rm = TRUE),\n    hdd_rice = sum(hdd_daily_rice, na.rm = TRUE),\n    hdd_potato = sum(hdd_daily_potato, na.rm = TRUE),\n    hdd_cassava = sum(hdd_daily_cassava, na.rm = TRUE)\n  )\n\nWe do not need to keep track of the unit for the weather data. Let us drop that information.\n\nquarterly_weather_data &lt;- units::drop_units(quarterly_weather_data)\n\n\n\n1.4.6 Annual Aggregation\nTo convert the daily grid data into annual data, we aggregate the values on an annual basis while preserving the spatial scale of the grid cells. Note: we only keep data from 1986 to 2015.\n\nannual_weather_data &lt;- \n  weather_peru_daily_grid |&gt; \n  mutate(\n    year = lubridate::year(date)\n  ) |&gt; \n  # Keeping 30 years of data\n  filter(year &gt;= 1986, year &lt;= 2015) |&gt; \n  # Annual aggregates, at the grid cell level\n  group_by(year, grid_id) |&gt; \n  summarise(\n    # Average min temperature\n    temp_min = mean(temp_min, na.rm = TRUE),\n    # Average max temperature\n    temp_max = mean(temp_max, na.rm = TRUE),\n    # Average mean temperature\n    temp_mean = mean(temp_mean, na.rm = TRUE),\n    # Total rainfall\n    precip_sum = sum(precip, na.rm = TRUE),\n    # Total rainfall piscop\n    precip_piscop_sum = sum(precip_piscop, na.rm = TRUE),\n    # Annual DD\n    gdd_rice = sum(gdd_daily_rice, na.rm = TRUE),\n    gdd_maize = sum(gdd_daily_maize, na.rm = TRUE),\n    gdd_potato = sum(gdd_daily_potato, na.rm = TRUE),\n    gdd_cassava = sum(gdd_daily_cassava, na.rm = TRUE),\n    # Annual HDD\n    hdd_maize = sum(hdd_daily_maize, na.rm = TRUE),\n    hdd_rice = sum(hdd_daily_rice, na.rm = TRUE),\n    hdd_potato = sum(hdd_daily_potato, na.rm = TRUE),\n    hdd_cassava = sum(hdd_daily_cassava, na.rm = TRUE)\n  )\n\nWe do not need to keep track of the unit for the weather data. Let us drop that information.\n\nannual_weather_data &lt;- units::drop_units(annual_weather_data)\n\n\n\n1.4.7 Climate Normals\nTo calculate the climate normals, which are the long-term averages, we compute the means of the weather variables over a period of 30 years.\n\nmonthly_weather_data &lt;- \n  monthly_weather_data |&gt; \n  group_by(month, grid_id) |&gt; \n  mutate(\n    temp_min_monthly_lt   = mean(temp_min),\n    temp_max_monthly_lt   = mean(temp_max),\n    temp_mean_monthly_lt  = mean(temp_mean),\n    precip_sum_monthly_lt = mean(precip_sum),\n    precip_piscop_sum_monthly_lt = mean(precip_piscop_sum),\n    #\n    gdd_rice_monthly_lt = mean(gdd_rice),\n    gdd_maize_monthly_lt = mean(gdd_maize),\n    gdd_potato_monthly_lt = mean(gdd_potato),\n    gdd_cassava_monthly_lt = mean(gdd_cassava),\n    #\n    hdd_maize_monthly_lt = mean(hdd_maize),\n    hdd_rice_monthly_lt = mean(hdd_rice),\n    hdd_potato_monthly_lt = mean(hdd_potato),\n    hdd_cassava_monthly_lt = mean(hdd_cassava)\n  ) |&gt; \n  ungroup()\n\nFor quarterly data:\n\nquarterly_weather_data &lt;- \n  quarterly_weather_data |&gt; \n  group_by(quarter, grid_id) |&gt; \n  mutate(\n    temp_min_quarterly_lt   = mean(temp_min),\n    temp_max_quarterly_lt   = mean(temp_max),\n    temp_mean_quarterly_lt  = mean(temp_mean),\n    precip_sum_quarterly_lt = mean(precip_sum),\n    precip_piscop_sum_quarterly_lt = mean(precip_piscop_sum),\n    #\n    gdd_rice_quarterly_lt = mean(gdd_rice),\n    gdd_maize_quarterly_lt = mean(gdd_maize),\n    gdd_potato_quarterly_lt = mean(gdd_potato),\n    gdd_cassava_quarterly_lt = mean(gdd_cassava),\n    #\n    hdd_maize_quarterly_lt = mean(hdd_maize),\n    hdd_rice_quarterly_lt = mean(hdd_rice),\n    hdd_potato_quarterly_lt = mean(hdd_potato),\n    hdd_cassava_quarterly_lt = mean(hdd_cassava)\n  ) |&gt; \n  ungroup()\n\nFor annual data:\n\nannual_weather_data &lt;- \n  annual_weather_data |&gt; \n  group_by(grid_id) |&gt; \n  mutate(\n    temp_min_annual_lt   = mean(temp_min),\n    temp_max_annual_lt   = mean(temp_max),\n    temp_mean_annual_lt  = mean(temp_mean),\n    precip_sum_annual_lt = mean(precip_sum),\n    precip_piscop_sum_annual_lt = mean(precip_piscop_sum),\n    #\n    gdd_rice_annual_lt = mean(gdd_rice),\n    gdd_maize_annual_lt = mean(gdd_maize),\n    gdd_potato_annual_lt = mean(gdd_potato),\n    gdd_cassava_annual_lt = mean(gdd_cassava),\n    #\n    hdd_maize_annual_lt = mean(hdd_maize),\n    hdd_rice_annual_lt = mean(hdd_rice),\n    hdd_potato_annual_lt = mean(hdd_potato),\n    hdd_cassava_annual_lt = mean(hdd_cassava)\n  ) |&gt; \n  ungroup()\n\n\n\n1.4.8 Rainfall Shock\nBurke, Gong, and Jones (2014) defines precipitation shocks as using a Gamma distribution. The historical data of monthly rainfall at each grid point are fitted to a grid-specific gamma distribution, and each year at the grid point is assigned to its corresponding percentile in that distribution. Note that Burke, Gong, and Jones (2014) uses crop-year data, whereas in our analysis, we use grid-month data.\nWe define a function, percentile_gamma_dist(), to estimate the parameters of a Gamma distribution using some input values. Then, for each input value, the corresponding percentile in the estimated distribution can be returned.\n\n#' Estimate the shape and scale parameters of a Gamma distribution on the\n#' monthly sum of precipitation for a given cell, then returns the\n#' corresponding percentiles in the estimated distribution\n#' \n#' @param x data frame with `precip_sum`, for a cell in a given calendar month\npercentile_gamma_dist &lt;- function(x, source = c(\"chirps\", \"piscop\")) {\n  # Estimate the shape and scale parameters of a Gamma distribution\n  if (source == \"chirps\") {\n    estim_gamma_precip &lt;- EnvStats::egamma(x$precip_sum)\n  } else {\n    estim_gamma_precip &lt;- EnvStats::egamma(x$precip_piscop_sum)\n  }\n  estim_g_shape &lt;- estim_gamma_precip$parameters[[\"shape\"]]\n  estim_g_scale &lt;- estim_gamma_precip$parameters[[\"scale\"]]\n  # Corresponding percentile in the estimated distribution\n  pgamma(q = x$precip_sum, shape = estim_g_shape, scale = estim_g_scale)\n}\n\nWe aggregate the observations by grid cell and month. For each series of observations, we apply the percentile_gamma_dist() function.\n\nmonthly_weather_data &lt;- \n  monthly_weather_data |&gt; \n  ungroup() |&gt; \n  nest(.by = c(grid_id, month)) |&gt; \n  mutate(\n    perc_gamma_precip = map(\n      data, ~percentile_gamma_dist(.x, source = \"chirps\")\n    ),\n    perc_gamma_precip_piscop = map(\n      data, ~percentile_gamma_dist(.x, source = \"piscop\")\n    )\n  ) |&gt; \n  unnest(c(data, perc_gamma_precip, perc_gamma_precip_piscop))\n\nLet us also do it for the quarterly data:\n\nquarterly_weather_data &lt;- \n  quarterly_weather_data |&gt; \n  ungroup() |&gt; \n  nest(.by = c(grid_id, quarter)) |&gt; \n  mutate(\n    perc_gamma_precip = map(\n      data, ~percentile_gamma_dist(.x, source = \"chirps\")\n    ),\n    perc_gamma_precip_piscop = map(\n      data, ~percentile_gamma_dist(.x, source = \"piscop\")\n    )\n  ) |&gt; \n  unnest(c(data, perc_gamma_precip, perc_gamma_precip_piscop))\n\nAnd the annual data:\n\nannual_weather_data &lt;- \n  annual_weather_data |&gt; \n  ungroup() |&gt; \n  nest(.by = c(grid_id)) |&gt; \n  mutate(\n    perc_gamma_precip = map(\n      data, ~percentile_gamma_dist(.x, source = \"chirps\")\n    ),\n    perc_gamma_precip_piscop = map(\n      data, ~percentile_gamma_dist(.x, source = \"piscop\")\n    )\n  ) |&gt; \n  unnest(c(data, perc_gamma_precip, perc_gamma_precip_piscop))\n\n\n\n1.4.9 Deviations from Normals\nIn Section Section 1.4.7, we established climate normals. We will now calculate the deviation from those normals for each grid cell and each month.\n\nmonthly_weather_data &lt;- \n  monthly_weather_data |&gt; \n  mutate(\n    temp_min_dev   = temp_min - temp_min_monthly_lt,\n    temp_max_dev   = temp_max - temp_max_monthly_lt,\n    temp_mean_dev  = temp_mean - temp_mean_monthly_lt,\n    precip_sum_dev = precip_sum - precip_sum_monthly_lt,\n    precip_piscop_sum_dev = precip_piscop_sum - precip_piscop_sum_monthly_lt,\n    #\n    gdd_rice_dev = gdd_maize - gdd_maize_monthly_lt,\n    gdd_maize_dev = gdd_rice - gdd_rice_monthly_lt,\n    gdd_potato_dev = gdd_potato - gdd_potato_monthly_lt,\n    gdd_cassava_dev = gdd_cassava - gdd_cassava_monthly_lt,\n    #\n    hdd_rice_dev = hdd_maize - hdd_maize_monthly_lt,\n    hdd_maize_dev = hdd_rice - hdd_rice_monthly_lt,\n    hdd_potato_dev = hdd_potato - hdd_potato_monthly_lt,\n    hdd_cassava_dev = hdd_cassava - hdd_cassava_monthly_lt\n  )\n\nAnd also for each grid cell and each quarter:\n\nquarterly_weather_data &lt;- \n  quarterly_weather_data |&gt; \n  mutate(\n    temp_min_dev   = temp_min - temp_min_quarterly_lt,\n    temp_max_dev   = temp_max - temp_max_quarterly_lt,\n    temp_mean_dev  = temp_mean - temp_mean_quarterly_lt,\n    precip_sum_dev = precip_sum - precip_sum_quarterly_lt,\n    precip_piscop_sum_dev = precip_piscop_sum - precip_piscop_sum_quarterly_lt,\n    #\n    gdd_rice_dev = gdd_maize - gdd_maize_quarterly_lt,\n    gdd_maize_dev = gdd_rice - gdd_rice_quarterly_lt,\n    gdd_potato_dev = gdd_potato - gdd_potato_quarterly_lt,\n    gdd_cassava_dev = gdd_cassava - gdd_cassava_quarterly_lt,\n    #\n    hdd_rice_dev = hdd_maize - hdd_maize_quarterly_lt,\n    hdd_maize_dev = hdd_rice - hdd_rice_quarterly_lt,\n    hdd_potato_dev = hdd_potato - hdd_potato_quarterly_lt,\n    hdd_cassava_dev = hdd_cassava - hdd_cassava_quarterly_lt\n  )\n\nAnd also for the annual data:\n\nannual_weather_data &lt;- \n  annual_weather_data |&gt; \n  mutate(\n    temp_min_dev   = temp_min - temp_min_annual_lt,\n    temp_max_dev   = temp_max - temp_max_annual_lt,\n    temp_mean_dev  = temp_mean - temp_mean_annual_lt,\n    precip_sum_dev = precip_sum - precip_sum_annual_lt,\n    precip_piscop_sum_dev = precip_piscop_sum - precip_piscop_sum_annual_lt,\n    #\n    gdd_rice_dev = gdd_maize - gdd_maize_annual_lt,\n    gdd_maize_dev = gdd_rice - gdd_rice_annual_lt,\n    gdd_potato_dev = gdd_potato - gdd_potato_annual_lt,\n    gdd_cassava_dev = gdd_cassava - gdd_cassava_annual_lt,\n    #\n    hdd_rice_dev = hdd_maize - hdd_maize_annual_lt,\n    hdd_maize_dev = hdd_rice - hdd_rice_annual_lt,\n    hdd_potato_dev = hdd_potato - hdd_potato_annual_lt,\n    hdd_cassava_dev = hdd_cassava - hdd_cassava_annual_lt\n  )\n\n\n\n1.4.10 SPEI\nThe Standardized Precipitation-Evapotranspiration Index (SPEI) is a versatile drought index that utilizes climatic data to assess the onset, duration, and severity of drought conditions compared to normal conditions.\nTo calculate evapotranspiration, we need to provide the latitude parameter to the thornthwaite() function from the {SPIE} package. For each grid cell, we use the latitude of its barycenter as the input.\n\ncentroids &lt;- st_centroid(map_peru_grid$geometry)\ncentroids_grid &lt;- as_tibble(st_coordinates(centroids)) |&gt; \n  mutate(grid_id = row_number()) |&gt; \n  rename(longitude = X, latitude = Y)\nsave(\n  centroids_grid,\n  file = \"../data/output/shapefile_peru/centroids_grid.rda\"\n)\n\nThe centroids of the cells are added to the monthly_weather_data tibble:\n\nmonthly_weather_data &lt;- \n  monthly_weather_data |&gt; \n  left_join(centroids_grid)\n\nand to the quarterly_weather_data tibble:\n\nquarterly_weather_data &lt;- \n  quarterly_weather_data |&gt; \n  left_join(centroids_grid)\n\nand to the annual_weather_data tibble:\n\nannual_weather_data &lt;- \n  annual_weather_data |&gt; \n  left_join(centroids_grid)\n\nWe define a function, compute_spei_cell(), that computes the Potential evapotranspiration (PET), the Standardized Precipitation Index (SPI), and the Standardized Precipitation-Evapotranspiration Index (SPEI), at the grid level. We can specify, by giving a vector of integers to the scale argument, the time scale at which the SPI and the SPEI are computed. This scale argument controls for the magnitude of the memory. In the help file of {SPEI}, one can read:\n\nThe magnitude of this memory is controlled by parameter scale. For example, a value of six would imply that data from the current month and of the past five months will be used for computing the SPEI or SPI value for a given month.\n\n\n#' Returns the SPI and SPEI on a monthly basis for a specific grid cell at\n#' different scales\n#'\n#' @param grid_id id of the grid cell\n#' @param scale vector of scales for the spi and spei functions\n#' @returns a tibble at the grid x monthly scale with the spi and the spei at\n#'   different scales\ncompute_spei_cell &lt;- function(grid_id, \n                              scale = c(1,3,6,12)) {\n  lat &lt;- centroids_grid |&gt; \n    filter(grid_id == !!grid_id) |&gt; \n    pull(latitude)  |&gt; \n    unique() |&gt; \n    as.numeric()\n  \n  tmp_grid &lt;- \n    monthly_weather_data |&gt; \n    # Focus on grid cell\n    filter(grid_id == !!grid_id) |&gt; \n    ungroup() |&gt; \n    arrange(year, month) |&gt; \n    dplyr::select(\n      year, month, grid_id, precip_sum, precip_piscop_sum, temp_mean, latitude\n    ) |&gt; \n    mutate(\n      PET = SPEI::thornthwaite(temp_mean, lat, verbose = FALSE), \n      BAL = precip_sum - PET,\n      BAL_piscop = precip_piscop_sum - PET\n    )\n  \n  df_spi &lt;- \n    map(\n      .x = scale,\n      .f = ~tibble(\n        scale = .x,\n        spi = SPEI::spi(\n          tmp_grid$precip_sum, scale = .x, verbose = FALSE)$fitted,\n        spei = SPEI::spei(\n          tmp_grid$precip_sum, scale = .x, verbose = FALSE)$fitted,\n        spi_piscop = SPEI::spi(\n          tmp_grid$precip_piscop_sum, scale = .x, verbose = FALSE)$fitted,\n        spei_piscop = SPEI::spei(\n          tmp_grid$precip_piscop_sum, scale = .x, verbose = FALSE)$fitted\n      ) |&gt; \n        mutate(t = row_number()) |&gt; \n        mutate(\n          spi = ifelse(is.infinite(spi), yes = NA, no = spi),\n          spei = ifelse(is.infinite(spei), yes = NA, no = spei),\n          spi_piscop = ifelse(is.infinite(spi_piscop), yes = NA, no = spi_piscop),\n          spei_piscop = ifelse(is.infinite(spei_piscop), yes = NA, no = spei_piscop)\n        )\n    ) |&gt; \n    list_rbind() |&gt; \n    pivot_wider(names_from = scale, values_from = c(spi, spei, spi_piscop, spei_piscop)) |&gt; \n    dplyr::select(-t)\n  \n  bind_cols(tmp_grid, df_spi) |&gt; \n    dplyr::select(-precip_sum, -precip_piscop_sum, -temp_mean, -latitude)\n  \n}\n\nWe can then apply the compute_spei_cell() function to all the grid cells\n\nresul_spei &lt;- \n  map(\n  .x = unique(monthly_weather_data$grid_id),\n  .f = compute_spei_cell,\n  .progress = TRUE\n  )\n\nAnd we bind the results in a single tibble:\n\nresul_spei &lt;- \n  resul_spei |&gt; \n  list_rbind(names_to = \"grid_id\")\n\nThe SPI and SPEI can be added to the weather tibble:\n\nmonthly_weather_data &lt;- \n  monthly_weather_data |&gt; \n  ungroup() |&gt; \n  left_join(resul_spei)",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Weather Data</span>"
    ]
  },
  {
    "objectID": "data-weather.html#aggregation-at-the-monthly-regional-level",
    "href": "data-weather.html#aggregation-at-the-monthly-regional-level",
    "title": "1  Weather Data",
    "section": "1.5 Aggregation at the (monthly) regional level",
    "text": "1.5 Aggregation at the (monthly) regional level\nWe now proceed to aggregate the monthly values of each grid cell to the scale of administrative regions. To accomplish this, for each region, we simply calculate the average of the values from each cell, weighting each term according to two measures. Firstly, the proportion that the cell represents in the total surface area of the region. Secondly, the proportion that the cell represents in the agricultural production of the region.\nWe focus on the following weather variables:\n\nweather_variables &lt;- c(\n  \"temp_min\", \"temp_max\", \"temp_mean\", \"precip_sum\", \"precip_piscop_sum\",\n  #\n  \"perc_gamma_precip\", \"perc_gamma_precip_piscop\",\n  #\n  \"gdd_rice\", \"gdd_maize\", \"gdd_potato\", \"gdd_cassava\",\n  \"hdd_rice\", \"hdd_maize\", \"hdd_potato\", \"hdd_cassava\",\n  #\n  \"temp_min_dev\", \"temp_max_dev\", \"temp_mean_dev\",\n  \"precip_sum_dev\", \"precip_piscop_sum_dev\",\n  #\n  \"gdd_rice_dev\", \"gdd_maize_dev\", \"gdd_potato_dev\", \"gdd_cassava_dev\",\n  \"hdd_rice_dev\", \"hdd_maize_dev\",\"hdd_potato_dev\", \"hdd_cassava_dev\",\n  #\n  \"cold_surprise_maize\", \"cold_surprise_rice\", \n  \"cold_surprise_potato\", \"cold_surprise_cassava\",\n  \"hot_surprise_maize\", \"hot_surprise_rice\", \n  \"hot_surprise_potato\", \"hot_surprise_cassava\",\n  #\n  \"dry_surprise_maize\", \"dry_surprise_rice\", \n  \"dry_surprise_potato\", \"dry_surprise_cassava\",\n  \"wet_surprise_maize\", \"wet_surprise_rice\", \n  \"wet_surprise_potato\", \"wet_surprise_cassava\"\n)\nweather_spi_variables &lt;- colnames(resul_spei |&gt; dplyr::select(matches(\"^spe?i\")))\nweather_variables &lt;- c(weather_variables, weather_spi_variables)\nweather_variables\n\nWe define a function, aggregate_region_i(), to perform the aggregation for a specific region.\n\n#' Aggregates the weather data at the region level\n#' @description For a specific region, the grid cells within that region are\n#'   first identified. Then, the area of the intersection between each grid cell\n#'   and the region are computed. The share of agricultural lands in each cell\n#'   is also computed. Combined together, the area of the intersection and the\n#'   agricultural share are used as weights to aggregate the data at the\n#'   regional level.\n#'\n#' @param i row index of the region in `map_peru`\n#' @param weather_variables vector of name of the weather variables to aggregate\naggregate_region_i &lt;- function(i, weather_variables) {\n  map_region_i &lt;- map_peru[i,]\n  tmp &lt;- \n    sf::st_intersection(map_peru_grid_agri, map_region_i) |&gt; \n    # Get the area of the intersection between the polygon of the current\n    # region and each grid cell that intersects it\n    dplyr::mutate(area_cell_intersect = sf::st_area(geometry)) |&gt; \n    dplyr::rename(grid_id = i) |&gt; \n    # Get the weather data for the corresponding grid cells\n    dplyr::left_join(\n      monthly_weather_data,\n      by = \"grid_id\"\n    ) |&gt; \n    dplyr::filter(!is.na(year)) |&gt; \n    # Remove the unit in the area (squared metres)\n    dplyr::mutate(\n      area_cell_intersect = units::drop_units(area_cell_intersect)\n    ) |&gt; \n    dplyr::group_by(month, year) |&gt; \n    # Compute the weights to attribute to each grid cell\n    dplyr::mutate(\n      w_cropland = cropland / sum(cropland),\n      w_area     = area_cell_intersect / sum(area_cell_intersect),\n      w          = w_cropland * w_area,\n      w          = w / sum(w)) |&gt; \n    # Weighted weather variables\n    dplyr::mutate(\n      dplyr::across(\n        .cols = !!weather_variables,\n        .fns = ~ .x * w\n      )\n    ) |&gt; \n    dplyr::select(-geometry) |&gt; \n    tibble::as_tibble() |&gt; \n    dplyr::group_by(year, month) |&gt; \n    dplyr::select(-geometry) |&gt; \n    tibble::as_tibble() |&gt; \n    dplyr::group_by(year, month) |&gt; \n    dplyr::summarise(\n      dplyr::across(\n        .cols = !!weather_variables,\n        .fns = ~sum(.x, na.rm = TRUE)\n      ),\n      .groups = \"drop\"\n    ) |&gt; \n    dplyr::mutate(IDDPTO = map_region_i$IDDPTO)\n}\n\nThen, we apply the aggregate_region_i() function to each region:\n\nweather_regions_df &lt;- \n  map(\n  .x = seq_len(nrow(map_peru)),\n  .f = ~aggregate_region_i(.x, weather_variables), \n  .progress = TRUE\n)\n# Sanity check\nweather_regions_df |&gt; \n  list_rbind() |&gt; \n  filter(is.na(year)) |&gt; \n  dplyr::select(IDDPTO)\n\nWe add the department name:\n\nweather_regions_df &lt;- \n  weather_regions_df |&gt; \n  list_rbind() |&gt; \n  left_join(\n    map_peru |&gt; dplyr::select(IDDPTO, DEPARTAMEN) |&gt;\n      as_tibble() |&gt; dplyr::select(-geometry),\n    by = \"IDDPTO\"\n  )\n\nWe add labels to the columns:\n\nweather_regions_df &lt;- \n  weather_regions_df |&gt; \n  labelled::set_variable_labels(\n    year = \"Year\",\n    month = \"Month\",\n    temp_min = \"Min. Temperature\",\n    temp_max = \"Max. Temperature\",\n    temp_mean = \"Mean Temperature\",\n    precip_sum = \"Total Rainfall (Chirps)\",\n    precip_piscop_sum = \"Total Rainfall (Piscop)\",\n    perc_gamma_precip = \"Precipitation Shock (Percentile from Gamma Dist., Chirps)\",\n    perc_gamma_precip_piscop = \"Precipitation Shock (Percentile from Gamma Dist., Piscop)\",\n    temp_min_dev = \"Deviation of Min. Temperature from Normals\",\n    temp_max_dev = \"Deviation of Max. Temperature from Normals\",\n    temp_mean_dev = \"Deviation of Mean Temperature from Normals\",\n    precip_sum_dev = \"Deviation of Total Rainfall from Normals (Chirps)\",\n    precip_piscop_sum_dev = \"Deviation of Total Rainfall from Normals (Piscop)\",\n    gdd_rice = \"Degree Days\",\n    gdd_maize = \"Degree Days\",\n    gdd_potato = \"Degree Days\",\n    gdd_cassava = \"Degree Days\",\n    hdd_rice = \"Harmful Degree Days\",\n    hdd_maize = \"Harmful Degree Days\",\n    hdd_potato = \"Harmful Degree Days\",\n    hdd_cassava = \"Harmful Degree Days\",\n    gdd_rice_dev = \"Deviation of Degree Days from Normals\",\n    gdd_maize_dev = \"Deviation of Degree Days from Normals\",\n    gdd_potato_dev = \"Deviation of Degree Days from Normals\",\n    gdd_cassava_dev = \"Deviation of Degree Days from Normals\",\n    hdd_rice_dev = \"Deviation of Harmful Degree Days from Normals\",\n    hdd_maize_dev = \"Deviation of Harmful Degree Days from Normals\",\n    hdd_potato_dev = \"Deviation of Harmful Degree Days from Normals\",\n    hdd_cassava_dev = \"Deviation of Harmful Degree Days from Normals\",\n    spi_1 = \"SPI Index (scale = 1)\",\n    spi_3 = \"SPI Index (scale = 3)\",\n    spi_6 = \"SPI Index (scale = 6)\",\n    spi_12 = \"SPI Index (scale = 12)\",\n    spei_1 = \"SPEI Index (scale = 1)\",\n    spei_3 = \"SPEI Index (scale = 3)\",\n    spei_6 = \"SPEI Index (scale = 6)\",\n    spei_12 = \"SPEI Index (scale = 12)\",\n    spi_piscop_1 = \"SPI Index (scale = 1)\",\n    spi_piscop_3 = \"SPI Index (scale = 3)\",\n    spi_piscop_6 = \"SPI Index (scale = 6)\",\n    spi_piscop_12 = \"SPI Index (scale = 12)\",\n    spei_piscop_1 = \"SPEI Index (scale = 1)\",\n    spei_piscop_3 = \"SPEI Index (scale = 3)\",\n    spei_piscop_6 = \"SPEI Index (scale = 6)\",\n    spei_piscop_12 = \"SPEI Index (scale = 12)\",\n    cold_surprise_maize = \"Cold Surprise Weather\", \n    cold_surprise_rice = \"Cold Surprise Weather\", \n    cold_surprise_potato = \"Cold Surprise Weather\", \n    cold_surprise_cassava = \"Cold Surprise Weather\",\n    hot_surprise_maize = \"Hot Surprise Weather\",\n    hot_surprise_rice = \"Hot Surprise Weather\",\n    hot_surprise_potato = \"Hot Surprise Weather\",\n    hot_surprise_cassava = \"Hot Surprise Weather\",\n    dry_surprise_maize = \"Dry Surprise Weather\", \n    dry_surprise_rice = \"Dry Surprise Weather\", \n    dry_surprise_potato = \"Dry Surprise Weather\", \n    dry_surprise_cassava = \"Dry Surprise Weather\",\n    wet_surprise_maize = \"Wet Surprise Weather\",\n    wet_surprise_rice = \"Wet Surprise Weather\",\n    wet_surprise_potato = \"Wet Surprise Weather\",\n    wet_surprise_cassava = \"Wet Surprise Weather\",\n    IDDPTO = \"Region ID\",\n    DEPARTAMEN = \"Region Name\"\n  )\n\nAnd the results are saved:\n\nsave(\n  weather_regions_df,\n  file = \"../data/output/weather/weather_regions_df.rda\"\n)\n\nSome monthly statistics over the period can be computed.\n\nmonthly_temp_peru_monthly_avg &lt;- \n  weather_regions_df |&gt; \n  dplyr::group_by(month) |&gt; \n  dplyr::summarise(\n    across(\n      .cols = c(\n        \"temp_min\", \"temp_max\", \"temp_mean\", \"precip_sum\", \"precip_piscop_sum\",\n        \"perc_gamma_precip\", \"perc_gamma_precip_piscop\",\n        \"temp_min_dev\", \"temp_max_dev\", \"temp_mean_dev\",\n        \"precip_sum_dev\", \"precip_piscop_sum_dev\",\n        \"gdd_rice\", \"gdd_maize\", \"gdd_potato\", \"gdd_cassava\",\n        \"hdd_rice\", \"hdd_maize\",\"hdd_potato\", \"hdd_cassava\"\n      ),\n      .fns = list(mean = mean, sd = sd)\n    )\n  )\n\nLet us have a look at average values:\n\nMean temp.Min temp.Max temp.Precipitation (Chirps)Precipitation (Piscop)Degree DaysHarmful Degree Days\n\n\n\n\nCode\nggplot(data = monthly_temp_peru_monthly_avg |&gt; \n         mutate(\n           month = month.abb[month],\n           month = factor(month, levels = month.abb)\n         )\n) +\n  geom_bar(\n    mapping = aes(x = month, y = temp_mean_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = month,\n      ymin = temp_mean_mean - temp_mean_sd,\n      ymax = temp_mean_mean + temp_mean_sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Temperatures (°C)\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 30, by = 2))\n\n\n\n\n\n\n\n\nFigure 1.9: Average monthly temperatures in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = monthly_temp_peru_monthly_avg |&gt; \n         mutate(\n           month = month.abb[month],\n           month = factor(month, levels = month.abb)\n         )\n) +\n  geom_bar(\n    mapping = aes(x = month, y = temp_min_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = month,\n      ymin = temp_min_mean - temp_min_sd,\n      ymax = temp_min_mean + temp_min_sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Temperatures (°C)\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 30, by = 2))\n\n\n\n\n\n\n\n\nFigure 1.10: Average monthly minimum temperatures in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = monthly_temp_peru_monthly_avg |&gt; \n         mutate(\n           month = month.abb[month],\n           month = factor(month, levels = month.abb)\n         )\n) +\n  geom_bar(\n    mapping = aes(x = month, y = temp_max_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = month,\n      ymin = temp_max_mean - temp_max_sd,\n      ymax = temp_max_mean + temp_max_sd\n    ),\n    width=0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Temperatures (°C)\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 30, by = 2))\n\n\n\n\n\n\n\n\nFigure 1.11: Average monthly maximum temperatures in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = monthly_temp_peru_monthly_avg |&gt; \n         mutate(\n           month = month.abb[month],\n           month = factor(month, levels = month.abb)\n         )\n) +\n  geom_bar(\n    mapping = aes(x = month, y = precip_sum_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = month,\n      ymin = precip_sum_mean - precip_sum_sd,\n      ymax = precip_sum_mean + precip_sum_sd\n    ),\n    width=0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Precipiation (mm)\"\n  )\n\n\n\n\n\n\n\n\nFigure 1.12: Average monthly precipitation in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = monthly_temp_peru_monthly_avg |&gt; \n         mutate(\n           month = month.abb[month],\n           month = factor(month, levels = month.abb)\n         )\n) +\n  geom_bar(\n    mapping = aes(x = month, y = precip_piscop_sum_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = month,\n      ymin = precip_piscop_sum_mean - precip_piscop_sum_sd,\n      ymax = precip_piscop_sum_mean + precip_piscop_sum_sd\n    ),\n    width=0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Precipiation (mm)\"\n  )\n\n\n\n\n\n\n\n\nFigure 1.13: Average monthly precipitation in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n  data = monthly_temp_peru_monthly_avg |&gt; \n    mutate(\n      month = month.abb[month],\n      month = factor(month, levels = month.abb)\n    ) |&gt; \n    dplyr::select(\n      month,\n      gdd_rice_mean, gdd_rice_sd, \n      gdd_maize_mean, gdd_maize_sd,\n      gdd_potato_mean, gdd_potato_sd,\n      gdd_cassava_mean, gdd_cassava_sd\n    ) |&gt; \n    pivot_longer(cols = c(-month)) |&gt; \n    mutate(\n      culture = str_extract(name, \"gdd_(.*)_\") |&gt; \n        str_remove(\"^gdd_\") |&gt; \n        str_remove(\"_$\"),\n      culture = factor(\n        culture, \n        levels = c(\"rice\", \"maize\", \"potato\", \"cassava\"),\n        labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\")\n      )\n    ) |&gt; \n    mutate(name = ifelse(str_detect(name, \"_mean$\"), \"mean\", \"sd\")) |&gt; \n    pivot_wider(names_from = name, values_from = value)\n) +\n  geom_bar(\n    mapping = aes(x = month, y = mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = month,\n      ymin = mean - sd,\n      ymax = mean + sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(x = NULL, y = \"Degree Days\") +\n  facet_wrap(~culture)\n\n\n\n\n\n\n\n\nFigure 1.14: Average monthly Degree Days in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n  data = monthly_temp_peru_monthly_avg |&gt; \n    mutate(\n      month = month.abb[month],\n      month = factor(month, levels = month.abb)\n    ) |&gt; \n    dplyr::select(\n      month,\n      hdd_rice_mean, hdd_rice_sd, \n      hdd_maize_mean, hdd_maize_sd,\n      hdd_potato_mean, hdd_potato_sd,\n      hdd_cassava_mean, hdd_cassava_sd\n    ) |&gt; \n    pivot_longer(cols = c(-month)) |&gt; \n    mutate(\n      culture = str_extract(name, \"hdd_(.*)_\") |&gt; \n        str_remove(\"^hdd_\") |&gt; \n        str_remove(\"_$\"),\n      culture = factor(\n        culture, \n        levels = c(\"rice\", \"maize\", \"potato\", \"cassava\"),\n        labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\")\n      )\n    ) |&gt; \n    mutate(name = ifelse(str_detect(name, \"_mean$\"), \"mean\", \"sd\")) |&gt; \n    pivot_wider(names_from = name, values_from = value)\n) +\n  geom_bar(\n    mapping = aes(x = month, y = mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = month,\n      ymin = mean - sd,\n      ymax = mean + sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(x = NULL, y = \"Harmful Degree Days\") +\n  facet_wrap(~culture)\n\n\n\n\n\n\n\n\nFigure 1.15: Average monthly Harmful Degree Days in Peru (1988-2015)\n\n\n\n\n\n\n\n\nNow, let us make some choropleth maps with the weather values. First, we define a tibble with the values for 2010 only.\n\nmap_peru_regional_weather &lt;- \n  map_peru |&gt; \n  left_join(\n    weather_regions_df |&gt; \n      dplyr::mutate(\n        month = factor(month.abb[month], levels = month.abb)) |&gt; \n      filter(year == 2010)\n  )\n\nJoining with `by = join_by(DEPARTAMEN, IDDPTO)`\n\n\n\nMean temp.Min temp.Max temp.Precipitation (Chirps)Precipitation (Piscop)Degree DaysHamrful Degree Days\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$temp_mean,\n    probs = seq(0,1, by = .1),\n    na.rm=TRUE\n  )\n)\ncols &lt;- viridis::magma(6) |&gt; rev()\ncols &lt;- cols[-length(cols)]\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather |&gt; filter(!is.na(month)),\n    mapping = aes(fill = temp_mean),\n    colour = \"white\"\n  ) +\n  scale_fill_gradientn(name = \"Temperature (°C)\", colours = cols) +\n  facet_wrap(~month, ncol = 4)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np + guides(fill = guide_colorbar(barheight = panel_height))\n\n\n\n\n\n\n\n\nFigure 1.16: Regional mean temperatures in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$temp_min,\n    probs = seq(0,1, by = .1),\n    na.rm=TRUE\n  )\n)\ncols &lt;- viridis::magma(6) |&gt; rev()\ncols &lt;- cols[-length(cols)]\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather |&gt; filter(!is.na(month)),\n    mapping = aes(fill = temp_min),\n    colour = \"white\"\n  ) +\n  scale_fill_gradientn(name = \"Temperature (°C)\", colours = cols) +\n  facet_wrap(~month, ncol = 4)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np + guides(fill = guide_colorbar(barheight = panel_height))\n\n\n\n\n\n\n\n\nFigure 1.17: Regional minimum temperatures in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$temp_max,\n    probs = seq(0,1, by = .1),\n    na.rm=TRUE\n  )\n)\ncols &lt;- viridis::magma(6) |&gt; rev()\ncols &lt;- cols[-length(cols)]\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather |&gt; filter(!is.na(month)),\n    mapping = aes(fill = temp_max),\n    colour = \"white\"\n  ) +\n  scale_fill_gradientn(name = \"Temperature (°C)\", colours = cols) +\n  facet_wrap(~month, ncol = 4)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np + guides(fill = guide_colorbar(barheight = panel_height))\n\n\n\n\n\n\n\n\nFigure 1.18: Regional maximum temperatures in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$precip_sum,\n    probs = seq(0,1, by = .1)\n  )\n)\n\ncols &lt;- rainbow(6)\ncols &lt;- cols[-length(cols)]\n\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather,\n    mapping = aes(fill = precip_sum),\n    colour = \"white\"\n  ) +\n  scale_fill_continuous(\n    \"Rainfall (mm/month)\", \n    low = \"white\", high = \"#005A8B\", na.value = \"grey50\",\n    breaks = round(b), labels = round(b)\n  ) +\n  facet_wrap(~month, ncol = 4)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np_2 &lt;- p + guides(fill = guide_colorbar(barheight = panel_height))\np_2\n\n\n\n\n\n\n\n\nFigure 1.19: Regional precipitation in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$precip_piscop_sum,\n    probs = seq(0,1, by = .1)\n  )\n)\n\ncols &lt;- rainbow(6)\ncols &lt;- cols[-length(cols)]\n\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather,\n    mapping = aes(fill = precip_piscop_sum),\n    colour = \"white\"\n  ) +\n  scale_fill_continuous(\n    \"Rainfall (mm/month)\", \n    low = \"white\", high = \"#005A8B\", na.value = \"grey50\",\n    breaks = round(b), labels = round(b)\n  ) +\n  facet_wrap(~month, ncol = 4)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np_2 &lt;- p + guides(fill = guide_colorbar(barheight = panel_height))\np_2\n\n\n\n\n\n\n\n\nFigure 1.20: Regional precipitation in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nmap_peru_regional_weather_gdd &lt;- \n  map_peru_regional_weather |&gt; \n  dplyr::select(\n    month,\n    gdd_rice,\n    gdd_maize,\n    gdd_potato,\n    gdd_cassava\n  ) |&gt; \n  pivot_longer(cols = c(-month, -geometry)) |&gt; \n  mutate(\n    culture = str_remove(name, \"^gdd_\") |&gt; str_remove(\"_$\"),\n    culture = factor(\n      culture, \n      levels = c(\"rice\", \"maize\", \"potato\", \"cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\")\n    )\n  )\n\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather_gdd$value,\n    probs = seq(0,1, by = .1)\n  )\n)\n\n\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather_gdd,\n    mapping = aes(fill = value),\n    colour = \"white\"\n  ) +\n  scale_fill_continuous(\n    \"Degree Days\", \n    low = \"white\", high = \"#882255\", na.value = \"grey50\",\n    breaks = round(b), labels = round(b)\n  ) +\n  facet_grid(culture~month)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np_2 &lt;- p + guides(fill = guide_colorbar(barheight = panel_height))\np_2\n\n\n\n\n\n\n\n\nFigure 1.21: Regional Degree Days in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nmap_peru_regional_weather_hdd &lt;- \n  map_peru_regional_weather |&gt; \n  dplyr::select(\n    month,\n    hdd_rice,\n    hdd_maize,\n    hdd_potato,\n    hdd_cassava\n  ) |&gt; \n  pivot_longer(cols = c(-month, -geometry)) |&gt; \n  mutate(\n    culture = str_remove(name, \"^hdd_\") |&gt; str_remove(\"_$\"),\n    culture = factor(\n      culture, \n      levels = c(\"rice\", \"maize\", \"potato\", \"cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\")\n    )\n  )\n\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather_hdd$value,\n    probs = seq(0,1, by = .1)\n  )\n)\n\n\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather_hdd,\n    mapping = aes(fill = value),\n    colour = \"white\"\n  ) +\n  scale_fill_continuous(\n    \"Harmful Degree Days\", \n    low = \"white\", high = \"#882255\", na.value = \"grey50\",\n    breaks = round(b), labels = round(b)\n  ) +\n  facet_grid(culture~month)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np_2 &lt;- p + guides(fill = guide_colorbar(barheight = panel_height))\np_2\n\n\n\n\n\n\n\n\nFigure 1.22: Regional Harmful Degree Days in Peru - 2010",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Weather Data</span>"
    ]
  },
  {
    "objectID": "data-weather.html#aggregation-at-the-quarterly-regional-level",
    "href": "data-weather.html#aggregation-at-the-quarterly-regional-level",
    "title": "1  Weather Data",
    "section": "1.6 Aggregation at the (quarterly) regional level",
    "text": "1.6 Aggregation at the (quarterly) regional level\nWe now proceed to aggregate the quarterly values of each grid cell to the scale of administrative regions.\nWe focus on the following weather variables:\n\nweather_variables &lt;- c(\n  \"temp_min\", \"temp_max\", \"temp_mean\", \"precip_sum\", \"precip_piscop_sum\",\n  \"perc_gamma_precip\", \"perc_gamma_precip_piscop\",\n  \"gdd_rice\", \"gdd_maize\", \"gdd_potato\", \"gdd_cassava\",\n  \"hdd_rice\", \"hdd_maize\", \"hdd_potato\", \"hdd_cassava\",\n  \"temp_min_dev\", \"temp_max_dev\", \"temp_mean_dev\",\n  \"precip_sum_dev\", \"precip_piscop_sum_dev\",\n  \"gdd_rice_dev\", \"gdd_maize_dev\", \"gdd_potato_dev\", \"gdd_cassava_dev\",\n  \"hdd_rice_dev\", \"hdd_maize_dev\",\"hdd_potato_dev\", \"hdd_cassava_dev\"\n)\n\nWe define a function, aggregate_quarter_region_i(), to perform the aggregation for a specific region.\n\n#' Aggregates the weather data at the quarterly region level\n#' @description For a specific region, the grid cells within that region are\n#'   first identified. Then, the area of the intersection between each grid cell\n#'   and the region are computed. The share of agricultural lands in each cell\n#'   is also computed. Combined together, the area of the intersection and the\n#'   agricultural share are used as weights to aggregate the data at the\n#'   regional level.\n#'\n#' @param i row index of the region in `map_peru`\n#' @param weather_variables vector of name of the weather variables to aggregate\naggregate_quarter_region_i &lt;- function(i, weather_variables) {\n  map_region_i &lt;- map_peru[i,]\n  tmp &lt;- \n    sf::st_intersection(map_peru_grid_agri, map_region_i) |&gt; \n    # Get the area of the intersection between the polygon of the current\n    # region and each grid cell that intersects it\n    dplyr::mutate(area_cell_intersect = sf::st_area(geometry)) |&gt; \n    dplyr::rename(grid_id = i) |&gt; \n    # Get the weather data for the corresponding grid cells\n    dplyr::left_join(\n      quarterly_weather_data,\n      by = \"grid_id\"\n    ) |&gt; \n    dplyr::filter(!is.na(year)) |&gt; \n    # Remove the unit in the area (squared metres)\n    dplyr::mutate(\n      area_cell_intersect = units::drop_units(area_cell_intersect)\n    ) |&gt; \n    dplyr::group_by(quarter, year) |&gt; \n    # Compute the weights to attribute to each grid cell\n    dplyr::mutate(\n      w_cropland = cropland / sum(cropland),\n      w_area     = area_cell_intersect / sum(area_cell_intersect),\n      w          = w_cropland * w_area,\n      w          = w / sum(w)) |&gt; \n    # Weighted weather variables\n    dplyr::mutate(\n      dplyr::across(\n        .cols = !!weather_variables,\n        .fns = ~ .x * w\n      )\n    ) |&gt; \n    dplyr::select(-geometry) |&gt; \n    tibble::as_tibble() |&gt; \n    dplyr::group_by(year, quarter) |&gt; \n    dplyr::select(-geometry) |&gt; \n    tibble::as_tibble() |&gt; \n    dplyr::group_by(year, quarter) |&gt; \n    dplyr::summarise(\n      dplyr::across(\n        .cols = !!weather_variables,\n        .fns = ~sum(.x, na.rm = TRUE)\n      ),\n      .groups = \"drop\"\n    ) |&gt; \n    dplyr::mutate(IDDPTO = map_region_i$IDDPTO)\n}\n\nThen, we apply the aggregate_quarter_region_i() function to each region:\n\nweather_quarter_regions_df &lt;- \n  map(\n    .x = seq_len(nrow(map_peru)),\n    .f = ~aggregate_quarter_region_i(.x, weather_variables), \n    .progress = TRUE\n  )\n\nWe add the department name:\n\nweather_quarter_regions_df &lt;- \n  weather_quarter_regions_df |&gt; \n  list_rbind() |&gt; \n  left_join(\n    map_peru |&gt; dplyr::select(IDDPTO, DEPARTAMEN) |&gt;\n      as_tibble() |&gt; dplyr::select(-geometry),\n    by = \"IDDPTO\"\n  )\n\nWe add labels to the columns:\n\nweather_quarter_regions_df &lt;- \n  weather_quarter_regions_df |&gt; \n  labelled::set_variable_labels(\n    year = \"Year\",\n    quarter = \"Quarter\",\n    temp_min = \"Min. Temperature\",\n    temp_max = \"Max. Temperature\",\n    temp_mean = \"Mean Temperature\",\n    precip_sum = \"Total Rainfall (Chirps)\",\n    precip_piscop_sum = \"Total Rainfall (Piscop)\",\n    perc_gamma_precip = \"Precipitation Shock (Percentile from Gamma Dist., Chirps)\",\n    perc_gamma_precip_piscop = \"Precipitation Shock (Percentile from Gamma Dist., Piscop)\",\n    temp_min_dev = \"Deviation of Min. Temperature from Normals\",\n    temp_max_dev = \"Deviation of Max. Temperature from Normals\",\n    temp_mean_dev = \"Deviation of Mean Temperature from Normals\",\n    precip_sum_dev = \"Deviation of Total Rainfall from Normals (Chirps)\",\n    precip_piscop_sum_dev = \"Deviation of Total Rainfall from Normals (Piscop)\",\n    gdd_rice = \"Degree Days\",\n    gdd_maize = \"Degree Days\",\n    gdd_potato = \"Degree Days\",\n    gdd_cassava = \"Degree Days\",\n    hdd_rice = \"Harmful Degree Days\",\n    hdd_maize = \"Harmful Degree Days\",\n    hdd_potato = \"Harmful Degree Days\",\n    hdd_cassava = \"Harmful Degree Days\",\n    gdd_rice_dev = \"Deviation of Degree Days from Normals\",\n    gdd_maize_dev = \"Deviation of Degree Days from Normals\",\n    gdd_potato_dev = \"Deviation of Degree Days from Normals\",\n    gdd_cassava_dev = \"Deviation of Degree Days from Normals\",\n    hdd_rice_dev = \"Deviation of Harmful Degree Days from Normals\",\n    hdd_maize_dev = \"Deviation of Harmful Degree Days from Normals\",\n    hdd_potato_dev = \"Deviation of Harmful Degree Days from Normals\",\n    hdd_cassava_dev = \"Deviation of Harmful Degree Days from Normals\",\n    IDDPTO = \"Region ID\",\n    DEPARTAMEN = \"Region Name\"\n  )\n\nAnd the results are saved:\n\nsave(\n  weather_quarter_regions_df,\n  file = \"../data/output/weather/weather_quarter_regions_df.rda\"\n)\n\nSome monthly statistics over the period can be computed.\n\nquarterly_temp_peru_quarterly_avg &lt;- \n  weather_quarter_regions_df |&gt; \n  dplyr::group_by(quarter) |&gt; \n  dplyr::summarise(\n    across(\n      .cols = c(\n        \"temp_min\", \"temp_max\", \"temp_mean\", \"precip_sum\", \"precip_piscop_sum\",\n        \"perc_gamma_precip\", \"perc_gamma_precip_piscop\",\n        \"temp_min_dev\", \"temp_max_dev\", \"temp_mean_dev\",\n        \"precip_sum_dev\", \"precip_piscop_sum_dev\",\n        \"gdd_rice\", \"gdd_maize\", \"gdd_potato\", \"gdd_cassava\",\n        \"hdd_rice\", \"hdd_maize\",\"hdd_potato\", \"hdd_cassava\"\n      ),\n      .fns = list(mean = mean, sd = sd)\n    )\n  )\n\nLet us have a look at average values:\n\nMean temp.Min temp.Max temp.Precipitation (Chirps)Precipitation (Piscop)Degree DaysHarmful Degree Days\n\n\n\n\nCode\nggplot(data = quarterly_temp_peru_quarterly_avg |&gt; \n         mutate(\n           quarter = factor(quarter, levels = 1:4, labels = str_c(\"Q\", 1:4))\n         )\n) +\n  geom_bar(\n    mapping = aes(x = quarter, y = temp_mean_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = quarter,\n      ymin = temp_mean_mean - temp_mean_sd,\n      ymax = temp_mean_mean + temp_mean_sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Temperatures (°C)\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 30, by = 2))\n\n\n\n\n\n\n\n\nFigure 1.23: Average quarterly temperatures in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = quarterly_temp_peru_quarterly_avg |&gt; \n         mutate(\n           quarter = factor(quarter, levels = 1:4, labels = str_c(\"Q\", 1:4))\n         )\n) +\n  geom_bar(\n    mapping = aes(x = quarter, y = temp_min_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = quarter,\n      ymin = temp_min_mean - temp_min_sd,\n      ymax = temp_min_mean + temp_min_sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Temperatures (°C)\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 30, by = 2))\n\n\n\n\n\n\n\n\nFigure 1.24: Average quarterly minimum temperatures in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = quarterly_temp_peru_quarterly_avg |&gt; \n         mutate(\n           quarter = factor(quarter, levels = 1:4, labels = str_c(\"Q\", 1:4))\n         )\n) +\n  geom_bar(\n    mapping = aes(x = quarter, y = temp_max_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = quarter,\n      ymin = temp_max_mean - temp_max_sd,\n      ymax = temp_max_mean + temp_max_sd\n    ),\n    width=0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Temperatures (°C)\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 30, by = 2))\n\n\n\n\n\n\n\n\nFigure 1.25: Average quarterly maximum temperatures in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = quarterly_temp_peru_quarterly_avg |&gt; \n         mutate(\n           quarter = factor(quarter, levels = 1:4, labels = str_c(\"Q\", 1:4))\n         )\n) +\n  geom_bar(\n    mapping = aes(x = quarter, y = precip_sum_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = quarter,\n      ymin = precip_sum_mean - precip_sum_sd,\n      ymax = precip_sum_mean + precip_sum_sd\n    ),\n    width=0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Precipiation (mm)\"\n  )\n\n\n\n\n\n\n\n\nFigure 1.26: Average quarterly precipitation in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = quarterly_temp_peru_quarterly_avg |&gt; \n         mutate(\n           quarter = factor(quarter, levels = 1:4, labels = str_c(\"Q\", 1:4))\n         )\n) +\n  geom_bar(\n    mapping = aes(x = quarter, y = precip_piscop_sum_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = quarter,\n      ymin = precip_piscop_sum_mean - precip_piscop_sum_sd,\n      ymax = precip_piscop_sum_mean + precip_piscop_sum_sd\n    ),\n    width=0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Precipiation (mm)\"\n  )\n\n\n\n\n\n\n\n\nFigure 1.27: Average quarterly precipitation in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n  data = quarterly_temp_peru_quarterly_avg |&gt; \n    mutate(\n      quarter = factor(quarter, levels = 1:4, labels = str_c(\"Q\", 1:4))\n    ) |&gt; \n    dplyr::select(\n      quarter,\n      gdd_rice_mean, gdd_rice_sd, \n      gdd_maize_mean, gdd_maize_sd,\n      gdd_potato_mean, gdd_potato_sd,\n      gdd_cassava_mean, gdd_cassava_sd\n    ) |&gt; \n    pivot_longer(cols = c(-quarter)) |&gt; \n    mutate(\n      culture = str_extract(name, \"gdd_(.*)_\") |&gt; \n        str_remove(\"^gdd_\") |&gt; \n        str_remove(\"_$\"),\n      culture = factor(\n        culture, \n        levels = c(\"rice\", \"maize\", \"potato\", \"cassava\"),\n        labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\")\n      )\n    ) |&gt; \n    mutate(name = ifelse(str_detect(name, \"_mean$\"), \"mean\", \"sd\")) |&gt; \n    pivot_wider(names_from = name, values_from = value)\n) +\n  geom_bar(\n    mapping = aes(x = quarter, y = mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = quarter,\n      ymin = mean - sd,\n      ymax = mean + sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(x = NULL, y = \"Degree Days\") +\n  facet_wrap(~culture)\n\n\n\n\n\n\n\n\nFigure 1.28: Average quarterly Degree Days in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n  data = quarterly_temp_peru_quarterly_avg |&gt; \n    mutate(\n      quarter = factor(quarter, levels = 1:4, labels = str_c(\"Q\", 1:4))\n    ) |&gt; \n    dplyr::select(\n      quarter,\n      hdd_rice_mean, hdd_rice_sd, \n      hdd_maize_mean, hdd_maize_sd,\n      hdd_potato_mean, hdd_potato_sd,\n      hdd_cassava_mean, hdd_cassava_sd\n    ) |&gt; \n    pivot_longer(cols = c(-quarter)) |&gt; \n    mutate(\n      culture = str_extract(name, \"hdd_(.*)_\") |&gt; \n        str_remove(\"^hdd_\") |&gt; \n        str_remove(\"_$\"),\n      culture = factor(\n        culture, \n        levels = c(\"rice\", \"maize\", \"potato\", \"cassava\"),\n        labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\")\n      )\n    ) |&gt; \n    mutate(name = ifelse(str_detect(name, \"_mean$\"), \"mean\", \"sd\")) |&gt; \n    pivot_wider(names_from = name, values_from = value)\n) +\n  geom_bar(\n    mapping = aes(x = quarter, y = mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = quarter,\n      ymin = mean - sd,\n      ymax = mean + sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(x = NULL, y = \"Harmful Degree Days\") +\n  facet_wrap(~culture)\n\n\n\n\n\n\n\n\nFigure 1.29: Average quarterly Harmful Degree Days in Peru (1988-2015)\n\n\n\n\n\n\n\n\nNow, let us make some choropleth maps with the weather values. First, we define a tibble with the values for 2010 only.\n\nmap_peru_regional_weather &lt;- \n  map_peru |&gt; \n  left_join(\n    weather_quarter_regions_df |&gt; \n      dplyr::mutate(\n        quarter = factor(quarter, levels = 1:4, labels = str_c(\"Q\", 1:4))\n      ) |&gt; \n      filter(year == 2010)\n  )\n\nJoining with `by = join_by(DEPARTAMEN, IDDPTO)`\n\n\n\nMean temp.Min temp.Max temp.PrecipitationPrecipitation (piscop)Degree DaysHarmful Degree Days\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$temp_mean,\n    probs = seq(0,1, by = .1),\n    na.rm=TRUE\n  )\n)\ncols &lt;- viridis::magma(6) |&gt; rev()\ncols &lt;- cols[-length(cols)]\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather |&gt; filter(!is.na(quarter)),\n    mapping = aes(fill = temp_mean),\n    colour = \"white\"\n  ) +\n  scale_fill_gradientn(name = \"Temperature (°C)\", colours = cols) +\n  facet_wrap(~quarter, ncol = 4)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np + guides(fill = guide_colorbar(barheight = panel_height))\n\n\n\n\n\n\n\n\nFigure 1.30: Regional mean temperatures in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$temp_min,\n    probs = seq(0,1, by = .1),\n    na.rm = TRUE\n  )\n)\ncols &lt;- viridis::magma(6) |&gt; rev()\ncols &lt;- cols[-length(cols)]\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather |&gt; filter(!is.na(quarter)),\n    mapping = aes(fill = temp_min),\n    colour = \"white\"\n  ) +\n  scale_fill_gradientn(name = \"Temperature (°C)\", colours = cols) +\n  facet_wrap(~quarter, ncol = 4)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np + guides(fill = guide_colorbar(barheight = panel_height))\n\n\n\n\n\n\n\n\nFigure 1.31: Regional minimum temperatures in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$temp_max,\n    probs = seq(0,1, by = .1),\n    na.rm=TRUE\n  )\n)\ncols &lt;- viridis::magma(6) |&gt; rev()\ncols &lt;- cols[-length(cols)]\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather |&gt; filter(!is.na(quarter)),\n    mapping = aes(fill = temp_max),\n    colour = \"white\"\n  ) +\n  scale_fill_gradientn(name = \"Temperature (°C)\", colours = cols) +\n  facet_wrap(~quarter, ncol = 4)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np + guides(fill = guide_colorbar(barheight = panel_height))\n\n\n\n\n\n\n\n\nFigure 1.32: Regional maximum temperatures in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$precip_sum,\n    probs = seq(0,1, by = .1)\n  )\n)\n\ncols &lt;- rainbow(6)\ncols &lt;- cols[-length(cols)]\n\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather,\n    mapping = aes(fill = precip_sum),\n    colour = \"white\"\n  ) +\n  scale_fill_continuous(\n    \"Rainfall (mm/quarter)\", \n    low = \"white\", high = \"#005A8B\", na.value = \"grey50\",\n    breaks = round(b), labels = round(b)\n  ) +\n  facet_wrap(~quarter, ncol = 4)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np_2 &lt;- p + guides(fill = guide_colorbar(barheight = panel_height))\np_2\n\n\n\n\n\n\n\n\nFigure 1.33: Regional precipitation in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$precip_piscop_sum,\n    probs = seq(0,1, by = .1)\n  )\n)\n\ncols &lt;- rainbow(6)\ncols &lt;- cols[-length(cols)]\n\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather,\n    mapping = aes(fill = precip_piscop_sum),\n    colour = \"white\"\n  ) +\n  scale_fill_continuous(\n    \"Rainfall (mm/quarter)\", \n    low = \"white\", high = \"#005A8B\", na.value = \"grey50\",\n    breaks = round(b), labels = round(b)\n  ) +\n  facet_wrap(~quarter, ncol = 4)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np_2 &lt;- p + guides(fill = guide_colorbar(barheight = panel_height))\np_2\n\n\n\n\n\n\n\n\nFigure 1.34: Regional precipitation in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nmap_peru_regional_weather_gdd &lt;- \n  map_peru_regional_weather |&gt; \n  dplyr::select(\n    quarter,\n    gdd_rice,\n    gdd_maize,\n    gdd_potato,\n    gdd_cassava\n  ) |&gt; \n  pivot_longer(cols = c(-quarter, -geometry)) |&gt; \n  mutate(\n    culture = str_remove(name, \"^gdd_\") |&gt; str_remove(\"_$\"),\n    culture = factor(\n      culture, \n      levels = c(\"rice\", \"maize\", \"potato\", \"cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\")\n    )\n  )\n\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather_gdd$value,\n    probs = seq(0,1, by = .1)\n  )\n)\n\n\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather_gdd,\n    mapping = aes(fill = value),\n    colour = \"white\"\n  ) +\n  scale_fill_continuous(\n    \"Degree Days\", \n    low = \"white\", high = \"#882255\", na.value = \"grey50\",\n    breaks = round(b), labels = round(b)\n  ) +\n  facet_grid(culture~quarter)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np_2 &lt;- p + guides(fill = guide_colorbar(barheight = panel_height))\np_2\n\n\n\n\n\n\n\n\nFigure 1.35: Regional Degree Days in Peru - 2010\n\n\n\n\n\n\n\n\n\nCode\nmap_peru_regional_weather_hdd &lt;- \n  map_peru_regional_weather |&gt; \n  dplyr::select(\n    quarter,\n    hdd_rice,\n    hdd_maize,\n    hdd_potato,\n    hdd_cassava\n  ) |&gt; \n  pivot_longer(cols = c(-quarter, -geometry)) |&gt; \n  mutate(\n    culture = str_remove(name, \"^hdd_\") |&gt; str_remove(\"_$\"),\n    culture = factor(\n      culture, \n      levels = c(\"rice\", \"maize\", \"potato\", \"cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\")\n    )\n  )\n\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather_hdd$value,\n    probs = seq(0,1, by = .1)\n  )\n)\n\n\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather_hdd,\n    mapping = aes(fill = value),\n    colour = \"white\"\n  ) +\n  scale_fill_continuous(\n    \"Harmful Degree Days\", \n    low = \"white\", high = \"#882255\", na.value = \"grey50\",\n    breaks = round(b), labels = round(b)\n  ) +\n  facet_grid(culture~quarter)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np_2 &lt;- p + guides(fill = guide_colorbar(barheight = panel_height))\np_2\n\n\n\n\n\n\n\n\nFigure 1.36: Regional Harmful Degree Days in Peru - 2010",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Weather Data</span>"
    ]
  },
  {
    "objectID": "data-weather.html#aggregation-at-the-annual-regional-level",
    "href": "data-weather.html#aggregation-at-the-annual-regional-level",
    "title": "1  Weather Data",
    "section": "1.7 Aggregation at the (annual) regional level",
    "text": "1.7 Aggregation at the (annual) regional level\nWe now proceed to aggregate the annual values of each grid cell to the scale of administrative regions.\nWe focus on the following weather variables:\n\nweather_variables &lt;- c(\n  \"temp_min\", \"temp_max\", \"temp_mean\", \"precip_sum\", \"precip_piscop_sum\",\n  \"perc_gamma_precip\", \"perc_gamma_precip_piscop\",\n  \"gdd_rice\", \"gdd_maize\", \"gdd_potato\", \"gdd_cassava\",\n  \"hdd_rice\", \"hdd_maize\", \"hdd_potato\", \"hdd_cassava\",\n  \"temp_min_dev\", \"temp_max_dev\", \"temp_mean_dev\",\n  \"precip_sum_dev\", \"precip_piscop_sum_dev\",\n  \"gdd_rice_dev\", \"gdd_maize_dev\", \"gdd_potato_dev\", \"gdd_cassava_dev\",\n  \"hdd_rice_dev\", \"hdd_maize_dev\",\"hdd_potato_dev\", \"hdd_cassava_dev\"\n)\n\nWe define a function, aggregate_annual_region_i(), to perform the aggregation for a specific region.\n\n#' Aggregates the weather data at the annual region level\n#' @description For a specific region, the grid cells within that region are\n#'   first identified. Then, the area of the intersection between each grid cell\n#'   and the region are computed. The share of agricultural lands in each cell\n#'   is also computed. Combined together, the area of the intersection and the\n#'   agricultural share are used as weights to aggregate the data at the\n#'   regional level.\n#'\n#' @param i row index of the region in `map_peru`\n#' @param weather_variables vector of name of the weather variables to aggregate\naggregate_annual_region_i &lt;- function(i, weather_variables) {\n  map_region_i &lt;- map_peru[i,]\n  tmp &lt;- \n    sf::st_intersection(map_peru_grid_agri, map_region_i) |&gt; \n    # Get the area of the intersection between the polygon of the current\n    # region and each grid cell that intersects it\n    dplyr::mutate(area_cell_intersect = sf::st_area(geometry)) |&gt; \n    dplyr::rename(grid_id = i) |&gt; \n    # Get the weather data for the corresponding grid cells\n    dplyr::left_join(\n      annual_weather_data,\n      by = \"grid_id\"\n    ) |&gt; \n    dplyr::filter(!is.na(year)) |&gt; \n    # Remove the unit in the area (squared metres)\n    dplyr::mutate(\n      area_cell_intersect = units::drop_units(area_cell_intersect)\n    ) |&gt; \n    dplyr::group_by(year) |&gt; \n    # Compute the weights to attribute to each grid cell\n    dplyr::mutate(\n      w_cropland = cropland / sum(cropland),\n      w_area     = area_cell_intersect / sum(area_cell_intersect),\n      w          = w_cropland * w_area,\n      w          = w / sum(w)) |&gt; \n    # Weighted weather variables\n    dplyr::mutate(\n      dplyr::across(\n        .cols = !!weather_variables,\n        .fns = ~ .x * w\n      )\n    ) |&gt; \n    dplyr::select(-geometry) |&gt; \n    tibble::as_tibble() |&gt; \n    dplyr::group_by(year) |&gt; \n    dplyr::select(-geometry) |&gt; \n    tibble::as_tibble() |&gt; \n    dplyr::group_by(year) |&gt; \n    dplyr::summarise(\n      dplyr::across(\n        .cols = !!weather_variables,\n        .fns = ~sum(.x, na.rm = TRUE)\n      ),\n      .groups = \"drop\"\n    ) |&gt; \n    dplyr::mutate(IDDPTO = map_region_i$IDDPTO)\n}\n\nThen, we apply the aggregate_quarter_region_i() function to each region:\n\nweather_annual_regions_df &lt;- \n  map(\n    .x = seq_len(nrow(map_peru)),\n    .f = ~aggregate_annual_region_i(.x, weather_variables), \n    .progress = TRUE\n  )\n\nWe add the department name:\n\nweather_annual_regions_df &lt;- \n  weather_annual_regions_df |&gt; \n  list_rbind() |&gt; \n  left_join(\n    map_peru |&gt; dplyr::select(IDDPTO, DEPARTAMEN) |&gt;\n      as_tibble() |&gt; dplyr::select(-geometry),\n    by = \"IDDPTO\"\n  )\n\nWe add labels to the columns:\n\nweather_annual_regions_df &lt;- \n  weather_annual_regions_df |&gt; \n  labelled::set_variable_labels(\n    year = \"Year\",\n    temp_min = \"Min. Temperature\",\n    temp_max = \"Max. Temperature\",\n    temp_mean = \"Mean Temperature\",\n    precip_sum = \"Total Rainfall (Chirps)\",\n    precip_piscop_sum = \"Total Rainfall (Piscop)\",\n    perc_gamma_precip = \"Precipitation Shock (Percentile from Gamma Dist., Chirps)\",\n    perc_gamma_precip_piscop = \"Precipitation Shock (Percentile from Gamma Dist., Piscop)\",\n    temp_min_dev = \"Deviation of Min. Temperature from Normals\",\n    temp_max_dev = \"Deviation of Max. Temperature from Normals\",\n    temp_mean_dev = \"Deviation of Mean Temperature from Normals\",\n    precip_sum_dev = \"Deviation of Total Rainfall from Normals (Chirps)\",\n    precip_piscop_sum_dev = \"Deviation of Total Rainfall from Normals (Piscop)\",\n    gdd_rice = \"Growing Degree Days\",\n    gdd_maize = \"Growing Degree Days\",\n    gdd_potato = \"Growing Degree Days\",\n    gdd_cassava = \"Growing Degree Days\",\n    hdd_rice = \"Harmful Degree Days\",\n    hdd_maize = \"Harmful Degree Days\",\n    hdd_potato = \"Harmful Degree Days\",\n    hdd_cassava = \"Harmful Degree Days\",\n    gdd_rice_dev = \"Deviation of Growing Degree Days from Normals\",\n    gdd_maize_dev = \"Deviation of Growing Degree Days from Normals\",\n    gdd_potato_dev = \"Deviation of Growing Degree Days from Normals\",\n    gdd_cassava_dev = \"Deviation of Growing Degree Days from Normals\",\n    hdd_rice_dev = \"Deviation of Harmful Degree Days from Normals\",\n    hdd_maize_dev = \"Deviation of Harmful Degree Days from Normals\",\n    hdd_potato_dev = \"Deviation of Harmful Degree Days from Normals\",\n    hdd_cassava_dev = \"Deviation of Harmful Degree Days from Normals\",\n    IDDPTO = \"Region ID\",\n    DEPARTAMEN = \"Region Name\"\n  )\n\nAnd the results are saved:\n\nsave(\n  weather_annual_regions_df,\n  file = \"../data/output/weather/weather_annual_regions_df.rda\"\n)\n\nSome monthly statistics over the period can be computed.\n\nannual_temp_peru_annual_avg &lt;- \n  weather_annual_regions_df |&gt; \n  dplyr::group_by(year) |&gt; \n  dplyr::summarise(\n    across(\n      .cols = c(\n        \"temp_min\", \"temp_max\", \"temp_mean\", \"precip_sum\", \"precip_piscop_sum\",\n        \"perc_gamma_precip\", \"perc_gamma_precip_piscop\",\n        \"temp_min_dev\", \"temp_max_dev\", \"temp_mean_dev\",\n        \"precip_sum_dev\", \"precip_piscop_sum_dev\",\n        \"gdd_rice\", \"gdd_maize\", \"gdd_potato\", \"gdd_cassava\",\n        \"hdd_rice\", \"hdd_maize\",\"hdd_potato\", \"hdd_cassava\"\n      ),\n      .fns = list(mean = mean, sd = sd)\n    )\n  )\n\nLet us have a look at average values:\n\nMean temp.Min temp.Max temp.Precipitation (Chirps)Precipitation (Piscop)Degree DaysHarmful Degree Days\n\n\n\n\nCode\nggplot(data = annual_temp_peru_annual_avg) +\n  geom_bar(\n    mapping = aes(x = year, y = temp_mean_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = year,\n      ymin = temp_mean_mean - temp_mean_sd,\n      ymax = temp_mean_mean + temp_mean_sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Temperatures (°C)\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 30, by = 2))\n\n\n\n\n\n\n\n\nFigure 1.37: Average annual temperatures in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = annual_temp_peru_annual_avg) +\n  geom_bar(\n    mapping = aes(x = year, y = temp_min_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = year,\n      ymin = temp_min_mean - temp_min_sd,\n      ymax = temp_min_mean + temp_min_sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Temperatures (°C)\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 30, by = 2))\n\n\n\n\n\n\n\n\nFigure 1.38: Average annual minimum temperatures in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = annual_temp_peru_annual_avg) +\n  geom_bar(\n    mapping = aes(x = year, y = temp_max_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = year,\n      ymin = temp_max_mean - temp_max_sd,\n      ymax = temp_max_mean + temp_max_sd\n    ),\n    width=0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Temperatures (°C)\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 30, by = 2))\n\n\n\n\n\n\n\n\nFigure 1.39: Average annual maximum temperatures in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = annual_temp_peru_annual_avg) +\n  geom_bar(\n    mapping = aes(x = year, y = precip_sum_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = year,\n      ymin = precip_sum_mean - precip_sum_sd,\n      ymax = precip_sum_mean + precip_sum_sd\n    ),\n    width=0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Precipiation (mm)\"\n  )\n\n\n\n\n\n\n\n\nFigure 1.40: Average annual precipitation in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = annual_temp_peru_annual_avg) +\n  geom_bar(\n    mapping = aes(x = year, y = precip_piscop_sum_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = year,\n      ymin = precip_piscop_sum_mean - precip_piscop_sum_sd,\n      ymax = precip_piscop_sum_mean + precip_piscop_sum_sd\n    ),\n    width=0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(\n    x = NULL, y = \"Precipiation (mm)\"\n  )\n\n\n\n\n\n\n\n\nFigure 1.41: Average annual precipitation in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n  data = annual_temp_peru_annual_avg |&gt; \n    dplyr::select(\n      year,\n      gdd_rice_mean, gdd_rice_sd, \n      gdd_maize_mean, gdd_maize_sd,\n      gdd_potato_mean, gdd_potato_sd,\n      gdd_cassava_mean, gdd_cassava_sd\n    ) |&gt; \n    pivot_longer(cols = c(-year)) |&gt; \n    mutate(\n      culture = str_extract(name, \"gdd_(.*)_\") |&gt; \n        str_remove(\"^gdd_\") |&gt; \n        str_remove(\"_$\"),\n      culture = factor(\n        culture, \n        levels = c(\"rice\", \"maize\", \"potato\", \"cassava\"),\n        labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\")\n      )\n    ) |&gt; \n    mutate(name = ifelse(str_detect(name, \"_mean$\"), \"mean\", \"sd\")) |&gt; \n    pivot_wider(names_from = name, values_from = value)\n) +\n  geom_bar(\n    mapping = aes(x = year, y = mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = year,\n      ymin = mean - sd,\n      ymax = mean + sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(x = NULL, y = \"Growing Degree Days\") +\n  facet_wrap(~culture)\n\n\n\n\n\n\n\n\nFigure 1.42: Average annual Degree Days in Peru (1988-2015)\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n  data = annual_temp_peru_annual_avg |&gt; \n    dplyr::select(\n      year,\n      hdd_rice_mean, hdd_rice_sd, \n      hdd_maize_mean, hdd_maize_sd,\n      hdd_potato_mean, hdd_potato_sd,\n      hdd_cassava_mean, hdd_cassava_sd\n    ) |&gt; \n    pivot_longer(cols = c(-year)) |&gt; \n    mutate(\n      culture = str_extract(name, \"hdd_(.*)_\") |&gt; \n        str_remove(\"^hdd_\") |&gt; \n        str_remove(\"_$\"),\n      culture = factor(\n        culture, \n        levels = c(\"rice\", \"maize\", \"potato\", \"cassava\"),\n        labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\")\n      )\n    ) |&gt; \n    mutate(name = ifelse(str_detect(name, \"_mean$\"), \"mean\", \"sd\")) |&gt; \n    pivot_wider(names_from = name, values_from = value)\n) +\n  geom_bar(\n    mapping = aes(x = year, y = mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = year,\n      ymin = mean - sd,\n      ymax = mean + sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(x = NULL, y = \"Harmful Degree Days\") +\n  facet_wrap(~culture)\n\n\n\n\n\n\n\n\nFigure 1.43: Average annual Harmful Degree Days in Peru (1988-2015)\n\n\n\n\n\n\n\n\nNow, let us make some choropleth maps with the weather values.\n\nmap_peru_regional_weather &lt;- \n  map_peru |&gt; \n  left_join(weather_annual_regions_df)\n\nJoining with `by = join_by(DEPARTAMEN, IDDPTO)`\n\n\n\nMean temp.Min temp.Max temp.PrecipitationPrecipitation (piscop)\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$temp_mean,\n    probs = seq(0,1, by = .1),\n    na.rm=TRUE\n  )\n)\ncols &lt;- viridis::magma(6) |&gt; rev()\ncols &lt;- cols[-length(cols)]\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather,\n    mapping = aes(fill = temp_mean),\n    colour = \"white\"\n  ) +\n  scale_fill_gradientn(name = \"Temperature (°C)\", colours = cols) +\n  facet_wrap(~year, ncol = 10)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np + guides(fill = guide_colorbar(barheight = panel_height))\n\n\n\n\n\n\n\n\nFigure 1.44: Regional mean temperatures in Peru\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$temp_min,\n    probs = seq(0,1, by = .1),\n    na.rm = TRUE\n  )\n)\ncols &lt;- viridis::magma(6) |&gt; rev()\ncols &lt;- cols[-length(cols)]\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather,\n    mapping = aes(fill = temp_min),\n    colour = \"white\"\n  ) +\n  scale_fill_gradientn(name = \"Temperature (°C)\", colours = cols) +\n  facet_wrap(~year, ncol = 10)\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np + guides(fill = guide_colorbar(barheight = panel_height))\n\n\n\n\n\n\n\n\nFigure 1.45: Regional minimum temperatures in Peru\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$temp_max,\n    probs = seq(0,1, by = .1),\n    na.rm=TRUE\n  )\n)\ncols &lt;- viridis::magma(6) |&gt; rev()\ncols &lt;- cols[-length(cols)]\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather,\n    mapping = aes(fill = temp_max),\n    colour = \"white\"\n  ) +\n  scale_fill_gradientn(name = \"Temperature (°C)\", colours = cols) +\n  facet_wrap(~year, ncol = 10)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np + guides(fill = guide_colorbar(barheight = panel_height))\n\n\n\n\n\n\n\n\nFigure 1.46: Regional maximum temperatures in Peru\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$precip_sum,\n    probs = seq(0,1, by = .1)\n  )\n)\n\ncols &lt;- rainbow(6)\ncols &lt;- cols[-length(cols)]\n\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather,\n    mapping = aes(fill = precip_sum),\n    colour = \"white\"\n  ) +\n  scale_fill_continuous(\n    \"Rainfall (mm/quarter)\", \n    low = \"white\", high = \"#005A8B\", na.value = \"grey50\",\n    breaks = round(b), labels = round(b)\n  ) +\n  facet_wrap(~year, ncol = 10)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np_2 &lt;- p + guides(fill = guide_colorbar(barheight = panel_height))\np_2\n\n\n\n\n\n\n\n\nFigure 1.47: Regional precipitation in Peru\n\n\n\n\n\n\n\n\n\nCode\nb &lt;- as.numeric(\n  quantile(\n    map_peru_regional_weather$precip_piscop_sum,\n    probs = seq(0,1, by = .1)\n  )\n)\n\ncols &lt;- rainbow(6)\ncols &lt;- cols[-length(cols)]\n\np &lt;- \n  ggplot() + \n  geom_sf(\n    data = map_peru_regional_weather,\n    mapping = aes(fill = precip_piscop_sum),\n    colour = \"white\"\n  ) +\n  scale_fill_continuous(\n    \"Rainfall (mm/quarter)\", \n    low = \"white\", high = \"#005A8B\", na.value = \"grey50\",\n    breaks = round(b), labels = round(b)\n  ) +\n  facet_wrap(~year, ncol = 10)\n\npanel_height &lt;- unit(1,\"npc\") - \n  sum(ggplotGrob(p)[[\"heights\"]][-3]) - unit(1,\"line\")\n\np_2 &lt;- p + guides(fill = guide_colorbar(barheight = panel_height))\np_2\n\n\n\n\n\n\n\n\nFigure 1.48: Regional precipitation in Peru",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Weather Data</span>"
    ]
  },
  {
    "objectID": "data-weather.html#content-of-the-dataset",
    "href": "data-weather.html#content-of-the-dataset",
    "title": "1  Weather Data",
    "section": "1.8 Content of the Dataset",
    "text": "1.8 Content of the Dataset\n\n\n\nTable 1.1: Variables in the weather_regions_df.rda file\n\n\n\n\n\n\n\n\n\n\nVariable name\nType\nDescription\n\n\n\n\nyear\nnumeric\nYear (YYYY)\n\n\nmonth\nnumeric\nMonth (MM)\n\n\nquarter\nnumeric\nQuarter (1–4)\n\n\ntemp_min\nnumeric\nMonthly average of daily min temperature\n\n\ntemp_max\nnumeric\nMonthly average of daily max temperature\n\n\ntemp_mean\nnumeric\nMonthly average of daily mean temperature\n\n\nprecip_sum\nnumeric\nMonthly sum of daily rainfall\n\n\nprecip_piscop_sum\nnumeric\nMonthly sum of daily rainfall (Piscop)\n\n\nperc_gamma_precip\nnumeric\nPercentile of the monthly precipitation (Estimated Gamma Distribution)\n\n\nperc_gamma_precip_piscop\nnumeric\nPercentile of the monthly precipitation (Estimated Gamma Distribution, Piscop)\n\n\ntemp_min_dev\nnumeric\nDeviation of monthly min temperatures (temp_min) from climate normals (1986 – 2015)\n\n\ntemp_max_dev\nnumeric\nDeviation of monthly max temperatures (temp_max) from climate normals (1986 – 2015)\n\n\ntemp_mean_dev\nnumeric\nDeviation of monthly mean temperatures (temp_mean) from climate normals (1986 – 2015)\n\n\nprecip_sum_dev\nnumeric\nDeviation of monthly total rainfall (precip_sum)\n\n\nfrom climate normals (1986 – 2015)\n\n\n\n\nprecip_piscop_sum_dev\nnumeric\nDeviation of monthly total rainfall (precip_sum, Piscop)\n\n\nfrom climate normals (1986 – 2015)\n\n\n\n\nspi_1, spi_piscop_1\nnumeric\nSPI Drought Index, Scale = 1\n\n\nspi_3, spi_piscop_3\nnumeric\nSPI Drought Index, Scale = 3\n\n\nspi_6, spi_piscop_6\nnumeric\nSPI Drought Index, Scale = 6\n\n\nspi_12, spi_piscop_12\nnumeric\nSPI Drought Index, Scale = 12\n\n\nspei_1, spei_piscop_1\nnumeric\nSPEI Drought Index, Scale = 1\n\n\nspei_3, spei_piscop_3\nnumeric\nSPEI Drought Index, Scale = 3\n\n\nspei_6, spei_piscop_6\nnumeric\nSPEI Drought Index, Scale = 6\n\n\nspei_12, spei_piscop_12\nnumeric\nSPEI Drought Index, Scale = 12\n\n\nIDDPTO\ncharacter\nRegion numerical ID\n\n\nDEPARTAMEN\ncharacter\nRegion name\n\n\n\n\n\n\n\n\n\n\nAragón, Fernando M, Francisco Oteiza, and Juan Pablo Rud. 2021. “Climate Change and Agriculture: Subsistence Farmers’ Response to Extreme Heat.” American Economic Journal: Economic Policy 13 (1): 1–35. https://doi.org/10.1257/pol.20190316.\n\n\nAybar, Cesar, Carlos Fernández, Adrian Huerta, Waldo Lavado, Fiorella Vega, and Oscar Felipe-Obando. 2020. “Construction of a High-Resolution Gridded Rainfall Dataset for Peru from 1981 to the Present Day.” Hydrological Sciences Journal 65 (5): 770–85.\n\n\nBurke, Marshall, Erick Gong, and Kelly Jones. 2014. “Income Shocks and HIV in Africa.” The Economic Journal 125 (585): 1157–89. https://doi.org/10.1111/ecoj.12149.\n\n\nFunk, Chris, Pete Peterson, Martin Landsfeld, Diego Pedreros, James Verdin, Shraddhanand Shukla, Gregory Husak, et al. 2015. “CHIRPS: Rainfall Estimates from Rain Gauge and Satellite Observations.” https://doi.org/10.15780/G2RP4Q.\n\n\nNatoli, Filippo. 2024. “The Macroeconomic Effects of Unexpected Temperature Shocks.” https://doi.org/10.2139/ssrn.4160944.",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Weather Data</span>"
    ]
  },
  {
    "objectID": "data-agriculture.html",
    "href": "data-agriculture.html",
    "title": "2  Agricultural Data",
    "section": "",
    "text": "2.1 Functions\nIn this section, we define several functions to download and format various datasets. These functions will be utilized later in Section 2.3 for importing and formatting the agricultural data in R.",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Agricultural Data</span>"
    ]
  },
  {
    "objectID": "data-agriculture.html#functions",
    "href": "data-agriculture.html#functions",
    "title": "2  Agricultural Data",
    "section": "",
    "text": "2.1.1 Downloading Data\n\n#' Directly download the Excels versions of the monthly reports available on the\n#' Peruvian website MINAGRI.\n#' \n#' @param y\ndownload.data &lt;- function(y){\n\n  # HOME PAGE\n  page &lt;- str_c(\n    \"https://www.midagri.gob.pe/portal/\",\n    \"boletin-estadistico-mensual-el-agro-en-cifras?start=\",\n    19-y\n  )\n\n  # EXTRACTION OF THE LINKS\n  text_rvest &lt;- read_html(page)\n  results &lt;- text_rvest |&gt; html_nodes(\".mainbody\")\n  first_result &lt;- results[1]\n\n  # LIST OF THE MONTHS\n  list_of_months &lt;- first_result |&gt;\n    html_nodes(\".cabecera\") |&gt;\n    html_text(trim = TRUE)\n  list_of_months &lt;- list_of_months[str_detect(list_of_months, \"[aeiou]\")]\n\n  # LIST OF THE urls (=liens)\n  liens &lt;-first_result |&gt; html_nodes(\"a[href]\") |&gt; html_attr(\"href\")\n  liens &lt;- liens[(str_detect(liens, \"zip\"))]\n  if (sum(as.numeric(str_detect(liens, \"\\\\.pdf\"))) &gt; 0) {\n    liens &lt;- liens[-which(str_detect(liens, \"\\\\.pdf\"))]\n  }\n  if (sum(as.numeric(str_detect(liens, \"pdf\\\\.zip\"))) &gt; 0) {\n    liens &lt;- liens[-which(str_detect(liens, \"pdf\\\\.zip\"))]\n  }\n  if ((y == 13) | (y == 14) | (y ==15)) {\n    liens &lt;- liens[which(str_detect(liens, \"cuadros\"))]\n  }\n  if (y == 16) {\n    liens &lt;- liens[which(str_detect(liens, \"data\"))]\n  }\n  if (y == 14) {\n    if (sum(as.numeric(str_detect(liens, \"abril\"))) &gt; 1) {\n      line &lt;- liens[which(str_detect(liens, \"abril\"))[1]]\n      liens &lt;- liens[-which(str_detect(liens, \"abril\"))]\n    }\n  }\n\n  if (sum(as.numeric(duplicated(liens))) &gt; 0) {\n    liens &lt;- liens[-which(duplicated(liens) == T)]\n  }\n  if (y == 14) {\n    liens &lt;- c(liens[1:8], line, liens[9:11])\n  }\n\n  annee &lt;-  as.numeric(str_c(\"20\", ifelse(y &lt;10 , str_c(\"0\",y), y)))\n\n  # CONDITION 1: Checking the number of urls\n  if (length(liens) == length(list_of_months)) {\n\n    if (y == 7) {\n      list_of_months &lt;- list_of_months[-8]\n      liens &lt;- liens[-8]\n    }\n\n    dir.create(str_c(\"../data/\", annee))\n\n    # LOOP 1 - On months for downloading and extracting the ZIP files\n    for (m in 1:length(liens)) {\n      mois_Lettres &lt;- list_of_months[m]\n\n      mois &lt;- case_when(\n        str_detect(mois_Lettres, \"iciembre\") ~ \"12\",\n        str_detect(mois_Lettres, \"oviembre\") ~ \"11\",\n        str_detect(mois_Lettres, \"ctubre\")   ~ \"10\",\n        str_detect(mois_Lettres, \"tiembre\")  ~ \"09\",\n        str_detect(mois_Lettres, \"osto\")     ~ \"08\",\n        str_detect(mois_Lettres, \"ulio\")     ~ \"07\",\n        str_detect(mois_Lettres, \"unio\")     ~ \"06\",\n        str_detect(mois_Lettres, \"ayo\")      ~ \"05\",\n        str_detect(mois_Lettres, \"bril\")     ~ \"04\",\n        str_detect(mois_Lettres, \"arzo\")     ~ \"03\",\n        str_detect(mois_Lettres, \"brero\")    ~ \"02\",\n        str_detect(mois_Lettres, \"nero\")     ~ \"01\"\n      )\n\n      # DOWNLOADING THE FILE\n      AdresseFichier &lt;- str_c(\"../data/raw/minagri/\", annee, \"/\", mois, \".zip\")\n      download.file(\n        url = str_c(\"https://www.midagri.gob.pe/\", liens[m]),\n        destfile = AdresseFichier\n      )\n\n      # Names of the files in the archive\n      files &lt;- unzip(zipfile = AdresseFichier, list = TRUE)\n      folder_name &lt;- files$Name[str_which(\n        files$Name, regex(\"agr.*la\", ignore_case = TRUE)\n      )] |&gt;\n        str_extract(\"^(.*)/\")\n\n      # Unzip on Mac OS when files in the archive contain accents in names\n      system(str_c(\"open \", AdresseFichier))\n      print(str_c(\"FILE \", mois, \"-\", annee, \" UNZIPED\"))\n\n      folder_name &lt;- files$Name[str_which(\n        files$Name, regex(\"agr.*la\", ignore_case = TRUE)\n      )] |&gt;\n        str_extract(\"^(.*)/\")\n\n      # Special case to handle year 2005\n      if (annee == 2005) {\n        AdresseFichier &lt;- str_c(\"../data/raw/minagri/\", annee, \"/\", mois)\n        fichierZip &lt;- list.files(AdresseFichier)[str_detect(list.files(AdresseFichier), \".zip\")]\n        for (i in 1:length(fichierZip)) {\n          unzip(\n            zipfile = str_c(AdresseFichier, \"/\", fichierZip[i]),\n            exdir = AdresseFichier\n          )\n        }\n      }\n\n    }# End of LOOP 1\n\n    # MANUAL TREATMENT FOR 2007\n    if (annee == 2007) {\n      list_of_months &lt;- c(list_of_months[1:7], \"Mayo\", list_of_months[8:11])\n      dir.create(str_c(\"../data/raw/minagri/\", annee, \"/05\"))\n      file.copy(\n        from = \"./MAYO_2007_NO_DELETE/AGRICOLA.xls\" ,\n        to = str_c(\"../data/raw/minagri/\",annee,\"/05/AGRICOLA.xls\")\n      )\n    }\n\n    # LOOP 2 - On months for creating the Excel files\n    for (m in 1:length(list_of_months)) {\n\n      if (y == 15 & m == 7) { next } else {\n        # Checking the appopriate name of each month\n        mois_Lettres &lt;- list_of_months[m]\n        mois &lt;- case_when(\n          str_detect(mois_Lettres, \"iciembre\") ~ \"12\",\n          str_detect(mois_Lettres, \"oviembre\") ~ \"11\",\n          str_detect(mois_Lettres, \"ctubre\")   ~ \"10\",\n          str_detect(mois_Lettres, \"tiembre\")  ~ \"09\",\n          str_detect(mois_Lettres, \"osto\")     ~ \"08\",\n          str_detect(mois_Lettres, \"ulio\")     ~ \"07\",\n          str_detect(mois_Lettres, \"unio\")     ~ \"06\",\n          str_detect(mois_Lettres, \"ayo\")      ~ \"05\",\n          str_detect(mois_Lettres, \"bril\")     ~ \"04\",\n          str_detect(mois_Lettres, \"arzo\")     ~ \"03\",\n          str_detect(mois_Lettres, \"brero\")    ~ \"02\",\n          str_detect(mois_Lettres, \"nero\")     ~ \"01\"\n        )\n\n        # Selecting the right Zip file\n        AdresseFichier &lt;- str_c(\"../data/raw/minagri\", annee, \"/\", mois, \".zip\")\n\n        if ((y == 12 & m == 10) | (y == 7 & m == 8)) {\n          files &lt;- ifelse(\n            m == 10,\n            yes = data.table(Name = list.files(\n              \"../data/raw/minagri/2012/cuadros-marzo12\"\n            )),\n            no = data.table(Name = list.files(\n              str_c(\"../data/raw/minagri/\", annee, \"/05/\")\n            ))\n          )\n        } else {\n          files &lt;- unzip(zipfile = AdresseFichier, list = TRUE)\n        }\n\n        if (y == 06 & (m == 10 | m == 12)) {\n          for (i in 1:dim(files)[1]) {\n            name &lt;- files[i, 1]\n            unzip(\n              zipfile = str_c(\n                \"../data/raw/minagri/\", annee, \"/\", mois, \"/\", name\n              ),\n              exdir = str_c(\n                \"../data/raw/minagri/\", annee, \"/\", mois\n              )\n            )\n          }\n        }\n\n        if (y == 16 & m &lt; 12) {\n          N_1 &lt;- str_c(\"../data/raw/minagri/\", annee, \"/\", files[1, 1])\n          N &lt;- str_c(N_1,\"x\")\n          file.copy(from = N_1, to = N)\n\n          if (m &gt;= 6) {\n            N &lt;- str_c(\"../data/raw/minagri/\", annee, \"/\", files[1, 1])\n          } else {\n            unlink(N_1, recursive = TRUE)\n          }\n        } else {\n          folder_name &lt;- files$Name[str_which(\n            files$Name,\n            regex(\"agr.*la\", ignore_case = TRUE)\n          )] |&gt;\n            str_extract(\"^(.*)/\")\n\n          if (is.na(folder_name[1])) {folder_name &lt;- mois}\n          if (y == 12 & m == 10) {folder_name &lt;- \"cuadros-marzo12\"}\n\n          # Adding .xls to the 2008-07 file\n          if (y == 08 & m == 6) {\n            N &lt;- list.files(\n              str_c(\"../data/raw/minagri/\", annee, \"/\", folder_name),\n              full.names = TRUE\n            ) |&gt;\n              {\\(x) str_which(x, regex(\"clima\", ignore_case = TRUE))}()\n            file.rename(from = N, to = str_c(N,\".xls\"))\n          }\n\n          # CHANGING \"HIDRO..\" FILES INTO \"CLIMA\"\n          if (\n            any(\n              str_detect(\n                list.files(\n                  str_c(\"../data/raw/minagri/\",annee,\"/\",folder_name),\n                  full.names = TRUE\n                ),\n                regex(\"hidro.*\",ignore_case = TRUE)\n              )\n            ) == TRUE) {\n            N &lt;- list.files(\n              str_c(\"../data/raw/minagri/\",annee,\"/\",folder_name),\n              full.names = TRUE\n            ) |&gt;\n              {\\(x) str_which(w, regex(\"hidro.*\", ignore_case = TRUE))}()\n            file.rename(\n              from = N,\n              to = str_c( substr(N,0,nchar(N)-4),\"clima.xls\")\n            )\n          }\n\n          # Final list of files in the unziped file\n          N &lt;- list.files(\n            str_c(\"../data/raw/minagri/\", annee, \"/\", folder_name),\n            full.names = TRUE,\n            pattern = \"\\\\.xlsx?\",\n            ignore.case = TRUE\n          )\n          print(N)\n\n          # SELECTING THE FILE FOR AGRICULTURAL DATA\n          N1 &lt;- N[str_which(N, regex(\"agr.*la\", ignore_case = TRUE))]\n\n          if (sum(as.numeric(duplicated(N1))) &gt; 0) {\n            N1 &lt;- N1[-which(duplicated(N1) == T)]\n          }\n          if (length(N1) &gt; 1) {\n            N1 &lt;- case_when(\n              (y == 07 & m == 10) ~ N1[1],\n              (y == 08 & m == 1)  ~ N1[2],\n              (y == 08 & m == 8)  ~ N1[1],\n              (y == 15 & m == 4)  ~ N1[2]\n            )\n          }\n          print(N1)\n\n          # SELECTING THE FILE FOR PRICES\n          if (y &lt; 14 | (y == 14 & m &gt; 1) ) {\n            N2 &lt;- N[str_which(N, regex(\"pre.*os\", ignore_case = TRUE))]\n          } else {\n            N2 &lt;- N[str_which(N, regex(\"agr.*la\", ignore_case = TRUE))]\n          }\n\n          if (y == 16) { N2 &lt;- N }\n\n          if (sum(as.numeric(duplicated(N2))) &gt; 0) {\n            N2 &lt;- N2[-which(duplicated(N2)==T)]\n          }\n          if (length(N2) &gt; 1) {\n            N2 &lt;- case_when(\n              (y == 07 & m == 10) ~ N2[1],\n              (y == 08 & m == 1)  ~ N2[2],\n              (y == 08 & m == 8)  ~ N2[1],\n              (y == 15 & m == 4)  ~ N2[2]\n            )\n          }\n          print(N2)\n          if (y == 10 & m == 3) {\n            file.copy(\n              from = N2 ,\n              to = str_c(\n                \"../data/raw/minagri/2010/\",\n                \"BEAM SETIEMBRE_DEF_2010/BEAM SETIEMBRE//Precios_oct.xls\"\n              )\n            )\n          }\n\n          # SELECTING THE FILE FOR CLIMATE DATA\n          N3 &lt;- N[str_which(N, regex(\"c.*ima\", ignore_case = TRUE))]\n          if (y == 16) {N3 &lt;- N}\n          if (sum(as.numeric(duplicated(N3))) &gt; 0){\n            N3 &lt;- N3[-which(duplicated(N3) == T)]\n          }\n          if (sum(as.numeric(str_detect(N3, regex(\"mp.*t\", ignore_case = TRUE)))) &gt; 0) {\n            N3 &lt;- N3[-which(str_detect(N3, regex(\"mp.*t\", ignore_case = TRUE)))]\n          }\n          if (is_empty(N3)) {\n            N3 &lt;- N[str_which(N, regex(\"Bem.*\", ignore_case = TRUE))]\n            N3 &lt;- N3[-which(str_detect(N3, regex(\"mp.*t\", ignore_case = TRUE)))]\n          }\n          if (length(N3) &gt; 1) {\n            N3 &lt;- case_when(\n              (y == 07 & m == 10) ~ N3[1],\n              (y == 08 & m == 1)  ~ N3[1],\n              (y == 08 & m == 8)  ~ N3[7],\n              (y == 08 & m == 9)  ~ N3[2],\n              (y == 10 & m == 12) ~ N3[1],\n              (y == 15 & m == 4)  ~ N3[2]\n            )\n          }\n          print(N3)\n          if (is_empty(N3)) {N3 &lt;- \"\"}\n\n          # Special attribution for the 2006_01 files\n          if (y == 06 & m == 12) {\n            N1 &lt;- str_c(\n              \"../data/raw/minagri/\", annee, \"/\", mois, \"/AGRICOLA/AGRICOLA.xls\"\n            )\n            N2 &lt;- str_c(\n              \"../data/raw/minagri/\", annee, \"/\", mois, \"/PRECIOS/PRECIOS.xls\"\n            )\n            N3 &lt;- str_c(\n              \"../data/raw/minagri/\", annee, \"/\", mois, \"/CLIMA/Bemene2006.xls\"\n            )\n          }\n\n          type_xl &lt;- \"xls\"\n          if (y == 15) {\n            type_xl &lt;- \"xlsx\"\n            if (m == 2) {type_xl &lt;- \"xls\"}\n          }\n\n          if (y == 5) {\n            folder_name &lt;- files$Name[str_which(\n              files$Name,\n              regex(\"agri.*zip\", ignore_case = TRUE)\n            )]\n            unzip(\n              str_c(\"../data/raw/minagri/\", annee, \"/\", mois, \"/\", folder_name),\n              exdir = str_c(\"../data/raw/minagri/\", annee,\"/\", mois)\n            )\n            N1 &lt;- list.files(\n              str_c(\"../data/raw/minagri/\", annee, \"/\", mois),\n              full.names = TRUE,\n              pattern = \"\\\\.xlsx?\",\n              ignore.case = TRUE\n            )\n          }\n        }\n\n        mois &lt;- case_when(\n          (y == 12 & mois == \"04\") ~ \"05\",\n          (y == 12 & mois == \"05\") ~ \"04\",\n          (y == 14 & mois == \"09\") ~ \"10\",\n          (y == 14 & mois == \"10\") ~ \"09\",\n          TRUE ~ as.character(mois)\n        )\n\n        # FILES FOR 2005 - 2013\n        if (y &lt;= 13) {\n          # PRODUCTION\n          name &lt;- str_c(\n            \"../data/raw/minagri/Production/Production_\", annee, \".xlsx\"\n          )\n          for (i in 1:2) {\n            if (y &lt; 11) {\n              table &lt;- ifelse(i == 1, \"c-26\", \"c-27\")\n            } else {\n              table &lt;- ifelse(i == 1, \"c-28\", \"c-29\")\n            }\n            if (y == 11 & m == 9 ) {table &lt;- ifelse(i == 1, \"c-27\", \"c-28\")}\n            if (y == 11 & m &gt; 9 ) {table &lt;- ifelse(i == 1, \"c-26\", \"c-27\")}\n            Tableau &lt;- read_excel(path = N1, sheet = table)\n\n            write.xlsx(\n              Tableau,\n              name,\n              sheetName = str_c(mois, annee, i),\n              append = TRUE,\n              showNA = FALSE\n            )\n            print(table)\n          }\n\n          # PLANTED AREAS\n          name &lt;- str_c(\n            \"../data/raw/minagri/Surface/Superficies_\", annee, \".xlsx\"\n          )\n          for (i in 1:2) {\n            if (y &lt; 11) {\n              table &lt;- ifelse(i == 1, \"c-19\", \"c-20\")\n            } else {\n              table &lt;- ifelse(i == 1, \"c-21\", \"c-22\")\n            }\n            if( y == 11 & m &gt; 9 ) {table &lt;- ifelse(i == 1, \"c-19\", \"c-20\")}\n            if( y == 11 & m == 9 ) {table &lt;- ifelse(i == 1, \"c-20\", \"c-21\")}\n\n            Tableau &lt;- read_excel(path = N1, sheet = table, col_types = \"text\")\n            write.xlsx(\n              Tableau,\n              name,\n              sheetName = str_c(mois, annee, i),\n              append = TRUE,\n              showNA = FALSE\n            )\n          }\n\n          # HARVESTED SUPERFICIES\n          name &lt;- str_c(\n            \"../data/raw/minagri/Surface_R/Superficies_R_\", annee, \".xlsx\"\n          )\n          for (i in 1:2) {\n            if (y &lt; 11) {\n              table &lt;- ifelse(i == 1, \"c-23\", \"c-24\")\n            } else {\n              table &lt;- ifelse(i == 1, \"c-25\", \"c-26\")\n            }\n            if (y == 11 & m &gt; 9 ) {table &lt;- ifelse(i == 1, \"c-23\" ,\"c-24\")}\n            if (y == 11 & m == 9 ) {table &lt;- ifelse(i == 1, \"c-24\" ,\"c-25\")}\n\n            Tableau &lt;- read_excel(path = N1, sheet = table, col_types = \"text\")\n\n            write.xlsx(\n              Tableau,\n              name,\n              sheetName = str_c(mois, annee, i),\n              append = TRUE,\n              showNA = FALSE\n            )\n          }\n\n          # PRICES\n          # Name of the file\n          name &lt;- str_c(\"../data/raw/minagri/Prices/Prices_\", annee, \".xlsx\")\n\n          # Selecting the names of the sheets\n          if (y &lt;= 10) {\n            table &lt;- \"C-65\"\n          } else {\n            table &lt;- case_when(\n              (annee == 2011 & m &lt; 9  )   ~ \"C-69\",\n              (annee == 2011 & m  == 9  ) ~ \"C-68\",\n              (annee == 2011 & m &gt;= 10 )  ~ \"C-65\",\n              (annee &gt; 2011 )             ~ \"C-69\"\n            )\n          }\n          if (y == 13 & m == 1) {table &lt;- regex(\"C-76\", ignore_case = TRUE)}\n          if (y == 10 & m == 4) {\n            N2 &lt;-  str_c(\n              \"../data/raw/minagri/2010/BEAM SETIEMBRE_DEF_2010/\",\n              \"BEAM SETIEMBRE//Precios_oct.xls\"\n            )\n          }\n          if (y == 07 & m == 8) {\n            N2 &lt;-  \"../data/raw/minagri/2007/04/PRECIOS.xls\"\n          }\n\n          Tableau &lt;- read_excel(path = N2, sheet = table, col_types = \"text\")\n          write.xlsx(\n            Tableau,\n            name,\n            sheetName = str_c(mois, annee),\n            append = TRUE,\n            showNA = FALSE\n          )\n        } else {\n          # FILES FOR 2014 - 2018\n          if (y == 16) {\n            N1 &lt;- N\n            N2 &lt;- N\n            N3 &lt;- N\n          }\n          if (y == 16 & m == 12) {\n            N1 &lt;- N[2]\n            N2 &lt;- N[2]\n            N3 &lt;- N[2]\n          }\n          # PRODUCTION\n          name &lt;- str_c(\n            \"../data/raw/minagri/Production/Production_\", annee, \".xlsx\"\n          )\n          if(y == 14) {\n            table &lt;- case_when(\n              m ==1  ~ \"c-12\",\n              m &gt; 1 ~ \"c-26\"\n            )\n          }\n          if (y == 15) {table &lt;-\"c-11\"}\n          if (y == 18) {table &lt;-\"c-18\"}\n          if (y == 15 & m ==  2) {table &lt;-\"C-11\"}\n          if (y == 15 & m ==  4) {table &lt;-\"C-11\"}\n          if (y == 15 & m ==  6) {table &lt;-\"C-11\"}\n          if (y == 15 & m == 10) {table &lt;-\"C-11\"}\n          if (y == 16 & m &lt;   7) {table &lt;-\"C.12\"}\n          if (y == 16 & m ==  7) {table &lt;-\"C. 12\"}\n          if (y == 16 & m &gt;=  8) {table &lt;-\"C.12\"}\n          if (y == 16 & m == 12) {table &lt;-\"c-11\"}\n          if (y == 14 & m &gt;7) {\n            for (i in 1:2) {\n              table &lt;- ifelse(i == 1, \"c-28\", \"c-29\")\n              if (m == 11) {table &lt;- ifelse(i == 1, \"C-28\", \"C-29\")}\n              if (m == 8) {table &lt;- ifelse(i == 1, \"c-26\", \"c-26-a\")}\n              Tableau &lt;- read_excel(path = N1, sheet = table)\n              write.xlsx(\n                Tableau,\n                name,\n                sheetName = str_c(mois, annee, i),\n                append = TRUE,\n                showNA = FALSE\n              )\n            }\n          } else {\n            Tableau &lt;- read_excel(\n              path = N1,\n              sheet = regex(table, ignore_case = TRUE)\n            )\n            write.xlsx(\n              Tableau,\n              name,\n              sheetName = str_c(mois, annee),\n              append = TRUE,\n              showNA = FALSE\n            )\n          }\n\n          # PLANTED SUPERFICIES\n          name &lt;- str_c(\n            \"../data/raw/minagri/Surface/Superficies_\",annee,\".xlsx\"\n          )\n          if(y ==14) {\n            table &lt;- case_when(\n              m ==1 ~ \"c-7\",\n              m &gt; 1 ~ \"c-21\"\n            )\n          }\n          if (y ==15) {table &lt;-\"c-6\"}\n          if (y ==18) {table &lt;-\"c-16\"}\n          if (y == 16) {table &lt;-\"C.9\"}\n          if (y == 15 & m == 2) {table &lt;-\"C-6\"}\n          if (y == 15 & m == 4) {table &lt;-\"C-6\"}\n          if (y == 15 & m == 6) {table &lt;-\"C-6\"}\n          if (y == 15 & m == 10) {table &lt;-\"C-6\"}\n          if (y == 16 & m ==  7) {table &lt;-\"C. 7\"}\n          if (y == 16 & m %in% c(8, 9, 10, 11, 12)) {table &lt;-\"C.6\"}\n\n          if (y == 14 & m &gt; 7 ) {\n            for (i in 1:2) {\n              table &lt;- ifelse(i == 1, \"c-21\", \"c-22\")\n              if(m == 8 ) {table &lt;- ifelse(i == 1, \"c-21\", \"c-21-a\")}\n              Tableau &lt;- read_excel(path = N1, sheet = table)\n              write.xlsx(\n                Tableau,\n                name,\n                sheetName = str_c(mois, annee, i),\n                append = TRUE,\n                showNA = FALSE\n              )\n            }\n          } else {\n            Tableau &lt;- read_excel(path = N1, sheet = table, col_types = \"text\")\n            write.xlsx(\n              Tableau,\n              name,\n              sheetName = str_c(mois, annee),\n              append = TRUE,\n              showNA = FALSE\n            )\n          }\n\n          # HARVESTED SUPERFICIES\n          name &lt;- str_c(\n            \"../data/raw/minagri/Surface_R/Superficies_R_\", annee, \".xlsx\"\n          )\n          if (y ==14) {\n            table &lt;- case_when(\n              m ==1 ~ \"c-10\",\n              m &gt; 1 ~ \"c-24\"\n            )\n          }\n          if (y ==15) {table &lt;-\"c-9\"}\n          if (y ==18) {table &lt;-\"c-16\"}\n          if (y == 16) {table &lt;-\"C.9\"} #FALSE : Table do not exist in the 2016 file\n          if (y == 15 & m == 2) {table &lt;-\"C-9\"}\n          if (y == 15 & m == 4) {table &lt;-\"C-9\"}\n          if (y == 15 & m == 6) {table &lt;-\"C-9\"}\n          if (y == 15 & m == 10) {table &lt;-\"c-9\"}\n          if (y == 16 & m ==  7) {table &lt;-\"C. 7\"} #FALSE : Table do not exist in the 2016 file\n          if (y == 16 & m %in% c(8, 9, 10, 11, 12)) {table &lt;-\"C.9\"}\n\n          if (y == 14 & m &gt; 7) {\n            for (i in 1:2) {\n              table &lt;- ifelse(i == 1,\"c-25\", \"c-26\")\n              if (m == 11) {table &lt;- ifelse(i == 1, \"C25\", \"C26\")}\n              if (m == 8 ) {table &lt;- ifelse(i == 1, \"c-24\", \"c-24-a\")}\n              Tableau &lt;- read_excel(path = N1, sheet = table)\n              write.xlsx(\n                Tableau,\n                name,\n                sheetName = str_c(mois, annee, i),\n                append = TRUE,\n                showNA = FALSE\n              )\n            }\n          } else {\n            Tableau &lt;- read_excel(path = N1, sheet = table, col_types = \"text\")\n            write.xlsx(\n              Tableau,\n              name,\n              sheetName = str_c(mois, annee),\n              append = TRUE,\n              showNA = FALSE\n            )\n          }\n          # PRICES\n          # Name of the file\n          name &lt;- str_c(\n            \"../data/raw/minagri/Prices/Prices_\", annee, \".xlsx\"\n          )\n\n          # Selecting the names of the sheets\n          table &lt;- case_when(\n            (annee == 2014 & m == 1)            ~ as.character(regex(\"c-17\", ignore_case = TRUE)),\n            (annee == 2014 & m == 2)            ~ as.character(regex(\"C-80\", ignore_case = TRUE)),\n            (annee == 2014 & m == 3)            ~ as.character(regex(\"C-72\", ignore_case = TRUE)),\n            (annee == 2014 & m == 4)            ~ as.character(regex(\"C-80\", ignore_case = TRUE)),\n            (annee == 2014 & m %in% 4:8)        ~ as.character(regex(\"C-72\", ignore_case = TRUE)),\n            (annee == 2014 & m &gt;8)              ~ as.character(regex(\"C-76\", ignore_case = TRUE)),\n            (annee == 2015 & ! m %in% c(2,4,6)) ~ as.character(regex(\"c-16\", ignore_case = TRUE)),\n            (annee == 2015 & m %in% c(2,4,6))   ~ as.character(regex(\"C-16\", ignore_case = TRUE)),\n            (annee == 2016 & m &lt;= 6)            ~ as.character(regex(\"C.15\", ignore_case = TRUE)),\n            (annee == 2016 & m &gt;  6)            ~ as.character(regex(\"C.13\", ignore_case = TRUE)),\n            (annee == 2016 & m == 7)            ~ as.character(regex(\"C. 13\", ignore_case = TRUE)),\n          )\n          if (y == 16 & m == 7) {table &lt;-\"C. 13\"}\n\n          Tableau &lt;- read_excel(path = N2, sheet = table, col_types = \"text\")\n          write.xlsx(\n            Tableau,\n            name,\n            sheetName = str_c(mois, annee),\n            append = TRUE,\n            showNA = FALSE\n          )\n        }\n        if (y == 16 & m &lt; 12) {\n          unlink(N, recursive = TRUE)\n        } else {\n          unlink(\n            str_c(\"../data/raw/minagri/\", annee, \"/\", folder_name),\n            recursive = TRUE\n          )\n        }\n      }\n    }# End of LOOP 2\n    # END OF CONDITION 1\n  } else {\n    print(\"ERROR - NB MOIS =! NB LIENS\")\n  }\n}\n\nWe define a function to extract data from the PDFs:\n\n#' Extract data from the PDF monthly reports from MINAGRI\n#' \n#' @param annee year of the report\n#' @param mois month of the report\n#' @param adresse url of the report on www.midagri.gob.pe\n#' @param page page number to extract\n#' @param Cell1 name of the first cell to import\nextract_pdf_data &lt;- function(annee,\n                             mois,\n                             adresse,\n                             page,\n                             Cell1) {\n\n  ## Defining the number of cultures in the table----\n  PageDeDonnes &lt;- pdf_data(adresse)[[page]]\n  if (annee == 2 & mois == 2) {\n    PageDeDonnes[which((PageDeDonnes[,\"text\"] == \"Año.?\")),\"x\"] &lt;-\n      as.numeric(PageDeDonnes[which((PageDeDonnes[,\"text\"] == Cell1)),\"x\"])\n  }\n\n  # Determining the begining of the table\n  x_Cell1 &lt;- as.numeric(PageDeDonnes[which((PageDeDonnes[,\"text\"] == Cell1)), \"x\"])\n  y_Cell1 &lt;- as.numeric(PageDeDonnes[which((PageDeDonnes[,\"text\"] == Cell1)), \"y\"])\n\n  # Looking for the lines with the same position as Cell1 or above\n  AboveLine1 &lt;- PageDeDonnes[which((PageDeDonnes[,\"x\"] &lt;= x_Cell1 + 1)), ]\n\n  ## Special case for 01/2003 ----\n  if ((annee == 3 & mois == 1) | (annee == 2 & mois &lt; 8)) {\n    AboveLine1 &lt;- PageDeDonnes[which((PageDeDonnes[, \"x\"] &lt;= x_Cell1 + 10)), ]\n    x_Cell1 &lt;- max(AboveLine1$x)}\n  # end of special case\n\n  x &lt;-  which(\n    str_detect(AboveLine1$text, \"Año.?\") |\n      str_detect(AboveLine1$text, \"paña\")\n  )\n  AboveLine1[x, \"x\"] &lt;- x_Cell1\n  if (any((AboveLine1[,\"text\"] == \"Mensual\"))) {\n    AboveLine1 &lt;- AboveLine1[- which(AboveLine1[, \"text\"] == \"Mensual\"), ]\n  }\n\n  if (any((AboveLine1[, \"x\"] &gt; x_Cell1))) {\n    AboveLine1[which(AboveLine1[,\"x\"]&gt;x_Cell1) ,\"x\"] &lt;- x_Cell1\n  }\n\n  AboveLine1 &lt;- AboveLine1[order(AboveLine1$x, -AboveLine1$y), ]\n  positions  &lt;- which((AboveLine1[, \"text\"] == Cell1))\n  x &lt;- as.numeric(AboveLine1[positions, \"x\"])\n\n  # Line with the culture list\n  Cultures &lt;- AboveLine1[AboveLine1$x == x, ]\n  if (dim(Cultures)[1] &lt; 3) {\n    Cultures &lt;- AboveLine1[AboveLine1$x %in% c(x,x-1), ]\n  }\n\n  if ((annee == 3 & mois == 1) | (annee ==2 & mois &lt; 6)) {\n    colnames(Cultures) &lt;- c(\n      \"width_1\", \"height_1\", \"x_1\", \"y_1\", \"space_1\", \"text_1\"\n    )\n    Cultures_1_0103 &lt;- Cultures\n  }\n\n  # Selection of the previous line to determine the complete name\n  x_1 &lt;- unique(AboveLine1$x)\n  x_1 &lt;- x_1[-which((x_1 == x))]\n  Cultures_1 &lt;- AboveLine1[AboveLine1$x == max(x_1), ]\n  if ((annee == 3 & mois == 1) | (annee ==2 & mois &lt; 6)) {\n    Cultures_0103 &lt;- Cultures_1\n  }\n  colnames(Cultures_1) &lt;- c(\"width_1\",\"height_1\",\"x_1\",\"y_1\",\"space_1\",\"text_1\")\n\n  # Row matching to obtain full crop names\n  if ((annee == 3 & mois == 1) | (annee ==2 & mois &lt; 6)) {\n    Cultures &lt;- merge(\n      Cultures_0103,\n      Cultures_1_0103,\n      by.x = \"y\",\n      by.y = \"y_1\",\n      all.x = TRUE,\n      all.y=TRUE\n    )\n  } else {\n    Cultures &lt;- merge(\n      Cultures,\n      Cultures_1,\n      by.x = \"y\",\n      by.y = \"y_1\",\n      all.x = TRUE,\n      all.y=TRUE\n    )\n  }\n\n  for (ii in 1:dim(Cultures)[1]) {\n    Cultures$nomcomplet[ii]  &lt;- if (is.na(Cultures$text_1[ii])) Cultures$text[ii] else paste(Cultures$text_1[ii], Cultures$text[ii])\n    if (annee == 3 & mois == 1 ) {Cultures$nomcomplet[ii]  &lt;- if (is.na(Cultures$text[ii])) Cultures$text_1[ii] else paste(Cultures$text_1[ii], Cultures$text[ii])}\n    Cultures$nomcomplet[ii]  &lt;- str_replace_all(Cultures$nomcomplet[ii], \"- \", \"\")\n    Cultures$nomcomplet[ii]  &lt;- if (Cultures$nomcomplet[ii] == \"de azúcar\") paste(\"Caña\", Cultures$nomcomplet[ii]) else Cultures$nomcomplet[ii]\n    Cultures$nomcomplet[ii]  &lt;- if (Cultures$nomcomplet[ii] == \"loctao\") paste(\"Frijol\", Cultures$nomcomplet[ii]) else Cultures$nomcomplet[ii]\n    Cultures$nomcomplet[ii]  &lt;- if (Cultures$nomcomplet[ii] == \"palo\") paste(\"Frijol de\", Cultures$nomcomplet[ii]) else Cultures$nomcomplet[ii]\n    Cultures$nomcomplet[ii]  &lt;- if (Cultures$nomcomplet[ii] == \"grano seco**\") paste(\"Frijol\", Cultures$nomcomplet[ii]) else Cultures$nomcomplet[ii]\n    Cultures$nomcomplet[ii]  &lt;- if (Cultures$nomcomplet[ii] == \"amiláceo\") paste(\"Maíz\", Cultures$nomcomplet[ii]) else Cultures$nomcomplet[ii]\n    Cultures$nomcomplet[ii]  &lt;- if (Cultures$nomcomplet[ii] == \"hua\") paste(\"Cañi\" ,Cultures$nomcomplet[ii], sep=\"\") else Cultures$nomcomplet[ii]\n    Cultures$nomcomplet[ii]  &lt;- if (Cultures$nomcomplet[ii] == \"cáscara\") paste(\"Arroz\", Cultures$nomcomplet[ii]) else Cultures$nomcomplet[ii]\n    Cultures$nomcomplet[ii]  &lt;- if (Cultures$nomcomplet[ii] == \"rama\") paste(\"Algodón\", Cultures$nomcomplet[ii]) else Cultures$nomcomplet[ii]\n\n    if (! is.na(Cultures$text[ii]) & is.na(Cultures$text_1[ii]) & ii &gt; 1 & ii &lt; dim(Cultures)[1]) {\n      Cultures$nomcomplet[ii] &lt;- if(Cultures$y[ii] == Cultures$y[ii-1] + 1) paste(Cultures$text_1[ii-1], Cultures$text[ii]) else Cultures$nomcomplet[ii]\n      Cultures$nomcomplet[ii] &lt;- if(Cultures$y[ii] == Cultures$y[ii+1] - 1) paste(Cultures$text_1[ii+1], Cultures$text[ii]) else Cultures$nomcomplet[ii]\n    }\n  }\n\n  # Discarding unmatched rows\n  ii = which(is.na(Cultures$x))\n  if (is.integer(ii) && length(ii) != 0) {Cultures &lt;- Cultures[-ii, ]}\n\n  ii = which(Cultures$text == \"a.\")\n  if(is.integer(ii) && length(ii) != 0) {Cultures &lt;- Cultures[-ii, ]}\n\n  ii = which(Cultures$text == \"de\")\n  if(is.integer(ii) && length(ii) != 0) {Cultures &lt;- Cultures[-ii, ]}\n\n  isPlatano &lt;-any(str_detect(PageDeDonnes$text, \"Plátano\"))\n  isFrijol &lt;-any(str_detect(PageDeDonnes$text, \"Frijol |seco\"))\n\n  ## Special case for 01/2003\n  if ((annee == 3 & mois == 1) | annee == 2) {\n    if (page %in% c(30, 31)) {\n      Cultures &lt;- as.data.table(\n        c(\"Departamento\",\"Campaña\", \"Total\",\"Arroz cáscara\", \"Maíz amiláceo\",\n          \"Frijol grano seco**\",\"Frijol castlla\", \"Pallar\",\"Zarandaja\",\n          \"Frijol de palo\", \"Garbanzo\",\"Frijol loctao\", \"Lentaja\", \"Papa\",\n          \"Trigo\", \"Algodón rama\", \"Maíz duro\", \"Soya\", \"Sorgo grano\",\n          \"Marigold\")\n      )\n      colnames(Cultures) &lt;- \"nomcomplet\"\n    }\n\n    if (page %in% c(32, 33)) {\n      Cultures &lt;- as.data.table(\n        c(\"Departamento\", \"Campaña\", \"Total\", \"Cebada grano\", \"Quinua\",\n          \"Cañihua\", \"Kiwicha\",\"Haba grano\", \"Arveja grano\", \"Chocho tarhui\",\n          \"Olluco\", \"Oca\", \"Mashua\", \"Camote\", \"Yuca\", \"Cebolla\", \"Ajo\",\n          \"Tomate\")\n      )\n      colnames(Cultures) &lt;- \"nomcomplet\"\n    }\n\n    if (page %in% c(38, 39)) {\n      Cultures &lt;- as.data.table(\n        c(\"Departamento\",\"Años\", \"Total\",\"Arroz cáscara\", \"Maíz amiláceo\",\n          \"Frijol grano seco**\",\"Frijol castlla\", \"Pallar\",\"Zarandaja\",\n          \"Frijol de palo\", \"Garbanzo\",\"Frijol loctao\", \"Lentaja\", \"Papa\",\n          \"Trigo\", \"Maíz duro\",\"Soya\", \"Sorgo grano\", \"Marigold\")\n      )\n      colnames(Cultures) &lt;- \"nomcomplet\"\n    }\n\n    if (page %in% c(40,41)) {\n      Cultures &lt;- as.data.table(\n        c(\"Departamento\", \"Años\", \"Cebada grano\", \"Quinua\",\"Cañihua\",\"Kiwicha\",\n          \"Haba grano\", \"Arveja grano\", \"Chocho tarhui\", \"Olluco\", \"Oca\",\n          \"Mashua\", \"Camote\", \"Yuca\", \"Cebolla\", \"Ajo\", \"Tomate\")\n      )\n      colnames(Cultures) &lt;- \"nomcomplet\"\n    }\n\n    if (page %in% c(44, 45)) {\n      Cultures &lt;- as.data.table(\n        c(\"Departamento\",\"Años\", \"Arroz cáscara\", \"Maíz amiláceo\",\n          \"Frijol grano seco**\",\"Frijol castlla\", \"Pallar\",\"Zarandaja\",\n          \"Frijol de palo\", \"Garbanzo\",\"Frijol loctao\", \"Lentaja\", \"Papa\",\n          \"Trigo\",\"Plátano\", \"Algodón rama\", \"Maíz duro\", \"Soya\",\n          \"Sorgo grano\", \"Caña de azúcar\", \"Café\", \"Espárrago\", \"Marigold\")\n      )\n      colnames(Cultures) &lt;- \"nomcomplet\"\n    }\n\n    if (page %in% c(46, 47)) {\n      Cultures &lt;- as.data.table(\n        c(\"Departamento\", \"Años\", \"Cebada grano\", \"Quinua\",\"Cañihua\",\n          \"Kiwicha\",\"Haba grano\", \"Arveja grano\", \"Chocho tarhui\", \"Olluco\",\n          \"Oca\", \"Mashua\", \"Camote\", \"Yuca\", \"Cebolla\", \"Ajo\", \"Tomate\")\n      )\n      colnames(Cultures) &lt;- \"nomcomplet\"\n    }\n\n    if (page %in% c(112,113)) {\n      Cultures &lt;- as.data.table(\n        c(\"Departamento\", \"Año\", \"Arroz cáscara\", \"Maíz amiláceo\",\"Trigo\",\n          \"Camote\", \"Papa\", \"Yuca\", \"Algodón rama\", \"Espárrago\", \"Maíz duro\",\n          \"Marigold\", \"Soya\", \"Café\")\n      )\n      colnames(Cultures) &lt;- \"nomcomplet\"\n    }\n\n    if (page %in% c(114,115)) {\n      Cultures &lt;- as.data.table(\n        c(\"Departamento\", \"Año\", \"Ajo\", \"Cebolla\", \"Maíz Choclo\", \"Tomate\",\n          \"Arveja grano\", \"Haba grano\",\"Limón\", \"Mandarina\", \"Manzana\",\n          \"Naranja\", \"Papaya\", \"Palta\", \"Piña\")\n      )\n      colnames(Cultures) &lt;- \"nomcomplet\"\n    }\n  } else {\n    Cultures &lt;- Cultures[order(-Cultures$y), ]\n    Cultures$nomcomplet[2] &lt;-\n      if (str_detect(Cultures$nomcomplet[2],\"ña\")) \"Campaña\" else Cultures$nomcomplet[2]\n\n    # Selection des donnees d'interet\n    Cultures &lt;-Cultures[, c(\"x\",\"y\",\"nomcomplet\")]  |&gt;\n      mutate(\n        nomcomplet = case_when(\n          str_detect(nomcomplet,\"Cañi\")      ~ \"Cañihua\",\n          str_detect(nomcomplet,\"seco\")      ~ \"Frijol grano seco**\",\n          str_detect(nomcomplet,\"paña\")      ~ \"Campaña\",\n          str_detect(nomcomplet,\"daja\")      ~ \"Zarandaja\",\n          str_detect(nomcomplet,\"duro\")      ~  \"Maíz duro\",\n          str_detect(nomcomplet,\"banzo\")     ~  \"Garbanzo\",\n          str_detect(nomcomplet,\"de azúcar\") ~  \"Caña de azúcar\",\n          str_detect(nomcomplet,\"Kiwi\")      ~  \"Kiwicha\",\n          TRUE ~ nomcomplet\n        )\n      ) |&gt;\n      mutate(n = duplicated(nomcomplet)) |&gt;\n      filter(! n ==T ) |&gt;\n      select(-n)\n\n  }\n\n  if (y == 4 & m &gt; 6 & page %in% c(38, 39) ) {\n    Cultures &lt;- as.data.table(\n      c(\"Departamento\", \"Años\",\"Total\", \"Arroz cáscara\",    \"Maíz amiláceo\",\n        \"Frijol grano seco**\",  \"Frijol castlla\",   \"Pallar\",   \"Zarandaja\",\n        \"Frijol de palo\",   \"Garbanzo\", \"Frijol loctao\",    \"Lenteja\",  \"Papa\",\n        \"Trigo\",    \"Maíz duro\",    \"Soya\", \"Sorgo grano\",  \"Caña de azúcar\" )\n    )\n    colnames(Cultures) &lt;- \"nomcomplet\"\n  }\n\n  if (page &gt; 100) {\n    if (page %in% c(108, 109, 110, 112, 113)) {\n      if (isFrijol == T) {\n        Cultures &lt;- as.data.table(\n          c(\"Departamento\", \"Año\", \"Arroz cáscara\", \"Maíz amiláceo\",\n            \"Trigo\", \"Frijol grano seco\", \"Camote\", \"Papa\", \"Yuca\",\n            \"Algodón rama\", \"Espárrago\", \"Maíz duro\",  \"Marigold\", \"Soya\",\n            \"Café\")\n        )\n        colnames(Cultures) &lt;- \"nomcomplet\"\n      } else {\n        Cultures &lt;- as.data.table(\n          c(\"Departamento\", \"Año\", \"Arroz cáscara\", \"Maíz amiláceo\",\"Trigo\",\n            \"Camote\", \"Papa\", \"Yuca\", \"Algodón rama\", \"Espárrago\", \"Maíz duro\",\n            \"Marigold\", \"Soya\", \"Café\")\n        )\n        colnames(Cultures) &lt;- \"nomcomplet\"\n      }\n    }\n\n    if (page %in% c(111, 114, 115)) {\n      if (isPlatano == T) {\n        Cultures &lt;- as.data.table(\n          c(\"Departamento\", \"Año\", \"Ajo\", \"Cebolla\", \"Maíz Choclo\", \"Tomate\",\n            \"Arveja grano\", \"Haba grano\",\"Limón\", \"Mandarina\", \"Manzana\",\n            \"Naranja\", \"Papaya\", \"Palta\", \"Piña\",\"Plátano\")\n        )\n        colnames(Cultures) &lt;- \"nomcomplet\"\n      } else {\n        Cultures &lt;- as.data.table(\n          c(\"Departamento\", \"Año\", \"Ajo\", \"Cebolla\", \"Maíz Choclo\", \"Tomate\",\n            \"Arveja grano\", \"Haba grano\",\"Limón\", \"Mandarina\", \"Manzana\",\n            \"Naranja\", \"Papaya\", \"Palta\", \"Piña\")\n        )\n        colnames(Cultures) &lt;- \"nomcomplet\"\n      }\n    }\n\n    if (annee == 2 & mois &lt; 8 & page %in% c(111, 112)) {\n      if (isPlatano == T) {\n        Cultures &lt;- as.data.table(\n          c(\"Departamento\", \"Año\", \"Ajo\", \"Cebolla\", \"Maíz Choclo\", \"Tomate\",\n            \"Arveja grano\", \"Haba grano\",\"Limón\", \"Mandarina\", \"Manzana\",\n            \"Naranja\", \"Papaya\", \"Palta\", \"Piña\",\"Plátano\")\n        )\n        colnames(Cultures) &lt;- \"nomcomplet\"\n      } else {\n        Cultures &lt;- as.data.table(\n          c(\"Departamento\", \"Año\", \"Ajo\", \"Cebolla\", \"Maíz Choclo\", \"Tomate\",\n            \"Arveja grano\", \"Haba grano\",\"Limón\",\"Mandarina\", \"Manzana\",\n            \"Naranja\", \"Papaya\", \"Palta\", \"Piña\")\n        )\n        colnames(Cultures) &lt;- \"nomcomplet\"}\n    }\n\n    if (annee == 2 & mois == 7 & page == 110) {\n      Cultures &lt;- as.data.table(\n        c(\"Departamento\", \"Año\", \"Ajo\", \"Cebolla\", \"Maíz Choclo\", \"Tomate\",\n          \"Arveja grano\", \"Haba grano\",\"Limón\", \"Mandarina\", \"Manzana\",\n          \"Naranja\", \"Papaya\", \"Palta\", \"Piña\",\"Plátano\")\n      )\n      colnames(Cultures) &lt;- \"nomcomplet\"\n    }\n  }\n\n  ## Downloading the table----\n  tx &lt;- pdf_text(adresse)[[page]]\n  tx2 &lt;- unlist(str_split(tx, \"[\\\\r\\\\n]+\"))\n  tx3 &lt;- as.data.frame(\n    str_split_fixed(str_trim(tx2), \"\\\\s{2,}\", dim(Cultures)[1])\n  )\n\n  # Replacing the name of the row \"Cultures\"\n  n &lt;-  which(str_detect(tx3$V1, Cell1))\n\n  tx3[n, ] &lt;- Cultures$nomcomplet\n  if(str_detect(tx3[n + 1, 2], \"[:digit:]{4}|[:digit:]{2}[:punct:][:digit:]{2}\") == F) {\n    tx3 &lt;- tx3[-(n + 1), ]\n  }\n  tx3 &lt;- tx3[-(n - 1), ]\n\n  if (any(str_detect(tx3$V1, \"Estadística\"))) {\n    tx3 &lt;- tx3[-which(str_detect(tx3$V1, \"Estadística\")), ]\n  }\n\n  if (any(str_detect(tx3$V1, \"cáscara\"))) {\n    n &lt;- which(str_detect(tx3$V1, \"cáscara\"))\n\n    if (sum(str_detect(tx3[n-1,],\"Arroz\")) &gt; 0) {\n      tx3[n-1,which(str_detect(tx3[n - 1,],\"Arroz\"))[1]] &lt;- \"Arroz cáscara\"\n    }\n    if (sum(str_detect(tx3[n-1,],\"Maíz\")) &gt; 0) {\n      tx3[n-1,which(str_detect(tx3[n - 1,],\"Maíz\"))[1]] &lt;- \"Maíz amiláceo\"\n    }\n    if (sum(str_detect(tx3[n-1,],\"Maíz\")) &gt; 1) {\n      tx3[n-1,which(str_detect(tx3[n - 1, ],\"Maíz\"))[2]] &lt;- \"Maíz duro\"\n    }\n    if (sum(str_detect(tx3[n-1,],\"Frijol\")) &gt; 0) {\n      tx3[n-1,which(str_detect(tx3[n - 1, ],\"Frijol\"))[1]] &lt;- \"Frijol grano seco\"\n    }\n    if (sum(str_detect(tx3[n-1,],\"Frijol\")) &gt; 1) {\n      tx3[n-1,which(str_detect(tx3[n - 1, ],\"Frijol\"))[2]] &lt;- \"Frijol castlla\"\n    }\n    if (sum(str_detect(tx3[n-1,],\"Frijol\")) &gt; 2) {\n      tx3[n-1,which(str_detect(tx3[n - 1, ],\"Frijol\"))[3]] &lt;- \"Frijol de palo\"\n    }\n    if (sum(str_detect(tx3[n-1,],\"Frijol\")) &gt; 3) {\n      tx3[n-1,which(str_detect(tx3[n - 1, ],\"Frijol\"))[4]] &lt;- \"Frijol loctao\"\n    }\n    if (sum(str_detect(tx3[n-1,],\"Algodón\")) &gt; 0) {\n      tx3[n-1,which(str_detect(tx3[n - 1, ],\"Algodón\"))[1]] &lt;- \"Algodón rama\"\n    }\n    if (sum(str_detect(tx3[n-1,],\"Sorgo\")) &gt; 0) {\n      tx3[n-1,which(str_detect(tx3[n - 1, ],\"Sorgo\"))[1]] &lt;- \"Sorgo grano\"\n    }\n    if (sum(str_detect(tx3[n-1,],\"Caña\")) &gt; 0) {\n      tx3[n-1,which(str_detect(tx3[n - 1, ],\"Caña\"))[1]] &lt;- \"Caña de azúcar\"\n    }\n    tx3 &lt;- tx3[-n, ]\n  }\n  if (annee == 2 & mois == 1 ) {\n    tx3 &lt;- tx3 |&gt;\n      mutate(\n        V1 = replace(V1, V1== 2001, 2002),\n        V1 = replace(V1, V1== 2000, 2001),\n        V2 = replace(V2, V2== 2001, 2002),\n        V2 = replace(V2, V2== 2000, 2001)\n      )\n  }\n  annee &lt;-  ifelse(\n    annee &lt; 10,\n    yes = paste(\"200\",annee, sep=\"\"),\n    no = paste(\"20\",annee, sep=\"\")\n  )\n\n  if (any(str_detect(tx3$V1, c(\"Total nacional 200|Andahuaylas 200\")))) {\n    n &lt;- str_which(\n      tx3$V1,\n      \"(Total nacional) |(Huancavelica) |(Madre de Dios) |(Andahuaylas) [[:digit:]]{4}\"\n    )\n    for (ii in n) {\n      for (j in dim(Cultures)[1]:3) {\n        tx3[ii, j] &lt;- tx3[ii, j - 1]\n      }\n      text &lt;- as.data.frame(str_split(tx3[ii,1], \"200\"))\n      tx3[ii,1] &lt;- text[1,]\n      tx3[ii,2] &lt;- paste(\"200\", text[2,], sep = \"\")\n    }\n  }\n\n  n &lt;- which(\n    tx3$V1 == as.character(as.numeric(annee)-1) | tx3$V1 == as.character(as.numeric(annee)) |\n      tx3$V1 == paste(str_sub(as.numeric(annee) - 2, 3, 4), \"-\", str_sub(as.numeric(annee) - 1, 3, 4), sep = \"\") |\n      tx3$V1 == paste(str_sub(as.numeric(annee) - 1, 3, 4), \"-\", str_sub(as.numeric(annee), 3, 4), sep = \"\") |\n      tx3$V1 == paste(str_sub(as.numeric(annee), 3, 4), \"-\", str_sub(as.numeric(annee) + 1, 3, 4), sep = \"\")\n  )\n\n  for (ii in n) {\n    for (j in dim(Cultures)[1]:2) {\n      tx3[ii, j] &lt;- tx3[ii, j - 1]\n    }\n    ifelse(tx3[ii - 1, 1] == Cell1, tx3[ii, 1] &lt;-\n             tx3[ii + 1, 1],  tx3[ii, 1] &lt;- tx3[ii - 1, 1])\n  }\n  tx3\n}\n\n\n#' Importing data where table is missing\n#'\n#' @param y last digit of the year (20..)\n#' @param mm month (numeric)\n#' @param type type of variable (\"Production\" for production, \"Superficies_R\"\n#'   for harvested surface, \"Superficies\" for planted surface)\nmissing_table &lt;- function(y,\n                          mm,\n                          type = c(\"Production\", \"Superficies_R\",\n                                   \"Superficies\")) {\n  data_mm &lt;- NULL\n\n  for (ii in 0:1) {\n    if (mm == 12) {\n      m_1 &lt;- mm - 1\n      y1  &lt;- y + 1\n      year &lt;- 2000 + y\n      year1 &lt;- 2000 + y + 1\n\n      if (type == \"Production\") {\n        ## Production, m==12----\n        # File name of the next year\n        file_name &lt;- str_c(\n          \"../data/raw/minagri/\", type, \"/\", type, \"_\", year1, \".xlsx\"\n        )\n        sheet &lt;- str_c(mm, year1, ii + 1)\n        data_m_y1 &lt;- import_monthly_regional_values_P_year_P(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_y1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          select(-value)\n\n        sheet &lt;- str_c(m_1, year1, ii + 1)\n        data_m_1_y1 &lt;- import_monthly_regional_values_P_year_P(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_1_y1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          rename(month_1 = month) |&gt;\n          select(-value)\n\n        # Current year\n        file_name &lt;- str_c(\n          \"../data/raw/minagri/\", type, \"/\", type, \"_\", year, \".xlsx\"\n        )\n        sheet &lt;- str_c(m_1, year, ii + 1)\n        data_m_1_y &lt;- import_monthly_regional_values_P_year_P(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_1_y = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          select(-value)\n\n        data_mm_y &lt;- merge(data_m_y1,data_m_1_y1) |&gt;\n          mutate(diff = value_m_y1 - value_m_1_y1) |&gt;\n          select(region, product, diff) |&gt;\n          merge(data_m_1_y) |&gt;\n          mutate(value_num = value_m_1_y + diff) |&gt;\n          select(-diff, - value_m_1_y) |&gt;\n          mutate(month = as.numeric(month + 1)) |&gt;\n          relocate(product, .after = month)\n\n        data_mm &lt;- rbind(data_mm, data_mm_y)\n      }\n\n      if (type == \"Superficies_R\") {\n        ## Superficies_R, m==12----\n        # Following year\n        file_name &lt;- str_c(\n          \"../data/raw/minagri/Surface_R/\", type, \"_\", year1, \".xlsx\"\n        )\n        sheet &lt;- str_c(mm,year1, ii + 1)\n        data_m_y1 &lt;- import_monthly_regional_values_year_SR(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_y1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          select(-value)\n\n        sheet &lt;- str_c(m_1, year1, ii + 1)\n        data_m_1_y1 &lt;- import_monthly_regional_values_year_SR(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_1_y1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          rename(month_1 = month) |&gt;\n          select(-value)\n\n        # Current year\n        file_name &lt;- str_c(\n          \"../data/raw/minagri/Surface_R/\", type, \"_\", year, \".xlsx\"\n        )\n        sheet &lt;- str_c(m_1, year, ii + 1)\n        data_m_1_y &lt;- import_monthly_regional_values_year_SR(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0)  |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_1_y = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          select(-value)\n\n        data_mm_y &lt;- merge(data_m_y1, data_m_1_y1) |&gt;\n          mutate(diff = value_m_y1 - value_m_1_y1) |&gt;\n          select(region, product, diff) |&gt;\n          merge(data_m_1_y) |&gt;\n          mutate(value_num = value_m_1_y + diff) |&gt;\n          select(-diff, - value_m_1_y) |&gt;\n          mutate(month = as.numeric(month + 1)) |&gt;\n          relocate(product, .after = month)\n\n        data_mm &lt;- rbind(data_mm, data_mm_y)\n      }\n\n      if (type == \"Superficies\") {\n        ## Superficies, m==12----\n        # Following year\n        file_name &lt;- str_c(\n          \"../data/raw/minagri/Surface/\", type, \"_\", year1, \".xlsx\"\n        )\n\n        sheet &lt;- str_c(mm, year1, ii + 1)\n        data_m_y1 &lt;- import_monthly_regional_values_year_S(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_y1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          select(-value)\n\n        sheet &lt;- str_c(m_1, year1, ii + 1)\n        data_m_1_y1 &lt;- import_monthly_regional_values_year_S(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_1_y1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          rename(month_1 = month) |&gt;\n          select(-value)\n\n        # Current year\n        file_name &lt;- str_c(\n          \"../data/raw/minagri/Surface/\", type, \"_\", year, \".xlsx\"\n        )\n        sheet &lt;- str_c(m_1, year, ii + 1)\n        data_m_1_y &lt;- import_monthly_regional_values_year_S(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        )  |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_1_y = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          select(-value)\n\n        data_mm_y &lt;- merge(data_m_y1, data_m_1_y1) |&gt;\n          mutate(diff = value_m_y1 - value_m_1_y1) |&gt;\n          select(region, product, diff) |&gt;\n          merge(data_m_1_y) |&gt;\n          mutate(value_num = value_m_1_y + diff) |&gt;\n          select(-diff, - value_m_1_y) |&gt;\n          mutate(month = as.numeric(month + 1)) |&gt;\n          relocate(product, .after = month)\n\n        data_mm &lt;- rbind(data_mm, data_mm_y)\n      }\n    } else {\n      # If m != 12\n      m_1 &lt;- mm - 1\n      m1  &lt;- mm + 1\n      year &lt;- 2000 + y\n\n      if (type == \"Production\") {\n        ## Production, m!=12----\n        file_name &lt;- str_c(\n          \"../data/raw/minagri/\", type, \"/\", type, \"_\", year, \".xlsx\"\n        )\n\n        # previous month\n        sheet &lt;- str_c(\n          str_pad(m_1, width = 2, side = \"left\", pad = \"0\"),\n          year,\n          ii + 1\n        )\n        data_m_1 &lt;- import_monthly_regional_values_P_year_P(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          rename(month_1 = month) |&gt;\n          select(-value)\n\n        # following month\n        sheet &lt;- str_c(\n          str_pad(m1, width = 2, side = \"left\", pad = \"0\"),\n          year,\n          ii + 1\n        )\n        data_m1 &lt;- import_monthly_regional_values_P_year_P(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          rename(month1 = month) |&gt;\n          select(-value)\n      }\n\n      if (type == \"Superficies_R\"){\n        ## Superficies_R, m!=12----\n        file_name &lt;- str_c(\n          \"../data/raw/minagri/Surface_R/\", type,\"_\", year, \".xlsx\"\n        )\n\n        # previous month\n        sheet &lt;- str_c(\n          str_pad(m_1, width = 2, side = \"left\", pad = \"0\"),\n          year,\n          ii + 1\n        )\n        data_m_1 &lt;- import_monthly_regional_values_year_SR(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          rename(month_1 = month) |&gt;\n          select(-value)\n\n        # following month\n        sheet &lt;- str_c(\n          str_pad(m1, width = 2, side = \"left\", pad = \"0\"),\n          year,\n          ii + 1\n        )\n        data_m1 &lt;- import_monthly_regional_values_year_SR(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          rename(month1 = month) |&gt;\n          select(-value)\n      }\n\n      if (type == \"Superficies\") {\n        ## Superficies, m!=12----\n        file_name &lt;- str_c(\n          \"../data/raw/minagri/Surface/\", type, \"_\", year, \".xlsx\"\n        )\n\n        # previous month\n        sheet &lt;- str_c(\n          str_pad(m_1, width = 2, side = \"left\", pad = \"0\"),\n          year,\n          ii + 1\n        )\n        data_m_1 &lt;- import_monthly_regional_values_year_S(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m_1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          rename(month_1 = month) |&gt;\n          select(-value)\n\n        # following month\n        sheet &lt;- str_c(\n          str_pad(m1, width = 2, side = \"left\", pad = \"0\"),\n          year,\n          ii + 1\n        )\n        data_m1 &lt;- import_monthly_regional_values_year_S(\n          sheet_name = sheet,\n          file = file_name,\n          anneesup = 0\n        ) |&gt;\n          mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n          mutate(value_m1 = as.numeric(gsub(\" \", \"\", value))) |&gt;\n          rename(month1 = month) |&gt;\n          select(-value)\n      }\n\n      data_mm_y &lt;- merge(data_m1,data_m_1) |&gt;\n        mutate(\n          month = as.numeric(month_1 + 1),\n          value_num = (value_m1 - value_m_1) / 2 + value_m_1) |&gt;\n        select(-value_m1, -month1, -value_m_1, -month_1) |&gt;\n        relocate(product, .after = month)\n\n      data_mm &lt;- rbind(data_mm, data_mm_y)\n\n    }\n  }\n  data_mm\n}\n\n\n\n2.1.2 Load Data\nWe define a function to format the header of the data.\n\n#' Format header (removing unecessary values)\n#' \n#' @param x (vector) of string\nformat_header &lt;- function(x) {\n  if (all(is.na(x))) {\n    return(\"\")\n  }\n  replace_na(x, \"\") |&gt;\n    str_replace_all(\"\\\\.{3}\", \"\") |&gt;\n    str_remove(\"[[:digit:]]/\") |&gt;\n    str_c(collapse = \" \") |&gt;\n    str_to_lower() |&gt;\n    str_replace_all(\"[[:blank:]]{2,}\", \" \") |&gt;\n    str_remove(\"- \") |&gt;\n    str_trim()\n}\n\n\n2.1.2.1 Regional Production\n\n#' Import monthly regional agricultural values from the Excel file,\n#' for a given month\n#' \n#' @param sheet_name name of the Excel sheet\n#' @param file path to the Excel File\n#' @param anneesup \nimport_monthly_regional_values_P_year_P &lt;- function(sheet_name,\n                                                    file,\n                                                    anneesup) {\n  # The first two digits of the name: month\n  # the next four: the four digits of the year\n  # last digit: index of the sheet for a given month and\n  # year (we will not use it)\n  year  &lt;- str_sub(sheet_name, 3, 6) |&gt; as.numeric()\n  month &lt;- str_sub(sheet_name, 1, 2) |&gt; as.numeric()\n  tmp   &lt;- suppressMessages(\n    read_excel(\n      path = file,\n      sheet = sheet_name,\n      col_types = \"text\", n_max = 15, col_names = FALSE\n    )\n  )\n\n  # We can use the first occurrence of \"Años\" to determine\n  # the beginning of the table\n  ind_row_year &lt;- str_which(tmp[[3]], regex(\"años?\", ignore_case = TRUE)) |&gt;\n    first()\n\n\n  # The first row of the header of the table is contained in the previous line\n  skip_head &lt;- ifelse(year &gt;= 2008,ind_row_year ,ind_row_year-1)\n  skip_head &lt;- ifelse(year == 2008 & month &lt; 3, skip_head -1 ,skip_head)\n  \n  # The body of the table\n  prod_region_tmp &lt;-\n    suppressMessages(\n      read_excel(\n        path = file,\n        sheet = sheet_name,\n        col_types = \"text\", skip = skip_head, col_names = F\n      )\n    ) |&gt; \n    dplyr::select(\n      - where( \n        ~ all(is.na(.))\n        )\n      )\n\n  header_1 &lt;- suppressMessages(\n    read_excel(\n      path = file,\n      sheet = sheet_name,\n      skip = skip_head-2, n_max = 3, col_names = F\n    )\n  ) |&gt; \n    dplyr::select(\n      - where( \n        ~ all(is.na(.))\n        )\n      )\n\n\n\n  header_2 &lt;- header_1 |&gt;\n    summarise(\n      across(\n        .cols = everything(),\n        .fns = ~format_header(.x)\n      )\n    )\n\n  colnames(prod_region_tmp) &lt;- as.character(header_2)\n  if (colnames(prod_region_tmp)[2] == \"\") {\n    colnames(prod_region_tmp)[2] &lt;- \"departamento\"\n  }\n  if (any(str_detect(colnames(prod_region_tmp), \"departamento\"), na.rm = T)) {\n    ind_dep_current &lt;- which(str_detect(colnames(prod_region_tmp), \"departamento\"))\n    colnames(prod_region_tmp)[ind_dep_current] &lt;- \"departamento\"\n  }\n\n  # Removing columns with no name\n  ind &lt;- !is.na(colnames(prod_region_tmp)) & (colnames(prod_region_tmp) != \"\")\n  prod_region_tmp &lt;-\n    prod_region_tmp |&gt;\n    dplyr::select(!!!colnames(prod_region_tmp)[ind])\n\n  # Removing rows with all NAs\n  prod_region_tmp &lt;-\n    prod_region_tmp |&gt;\n    filter_all(any_vars(!is.na(.)))\n\n  # Removing rows where \"Continúa\" is found\n  prod_region_tmp &lt;-\n    prod_region_tmp |&gt;\n    filter_all(\n      any_vars(\n        !str_detect(., regex(pattern = \"Continúa\", ignore_case = TRUE))\n      )\n    )\n\n  # Removing the first colomn if row index\n  if (str_detect(colnames(prod_region_tmp)[1], \"[aeiou]\") == FALSE) {\n    prod_region_tmp &lt;- prod_region_tmp[-1]\n  }\n\n  # The name of the first column differs accross sheets (departemento or region)\n  name_first_col &lt;- colnames(prod_region_tmp)[1]\n\n  if (year &lt; 2015) {\n    if (! name_first_col %in% c(\"departamento\", \"región\")) {\n      warning(str_c(\"Issue with sheet: \", sheet_name))\n      return(NULL)\n    }\n  }\n\n  prod_region_tmp &lt;-\n    prod_region_tmp |&gt;\n    rename(region := !!name_first_col)\n\n  if (any(str_detect(colnames(prod_region_tmp), \"^año$\"))) {\n    prod_region_tmp &lt;-\n      prod_region_tmp |&gt; rename(year = año)\n  } else {\n    prod_region_tmp &lt;-\n      prod_region_tmp |&gt; rename(year = años)\n  }\n\n  prod_region_tmp &lt;-\n    prod_region_tmp |&gt;\n    filter(!is.na(year), ! year %in% c(\"Años\", \"Año\"))\n\n  # The sheet may contain values for the department Cajamarca\n  # AND the capital of that department named also Cajamarca\n  prod_region_tmp &lt;-\n    prod_region_tmp |&gt;\n    group_by(region, year) |&gt;\n    mutate(\n      region = ifelse(\n        region == \"Cajamarca\" & row_number() == 1,\n        yes = \"Cajamarca_R\",\n        no = region\n      )\n    ) |&gt;\n    ungroup()\n\n  prod_region_tmp &lt;-\n    prod_region_tmp |&gt;\n    mutate(\n      region = ifelse(region == \"Lima provincias\", yes = \"Lima\", no = region)\n    )\n\n\n  # The production of each product is given in two rows, but the product name\n  # is not repeated\n  prod_region_tmp &lt;-\n    prod_region_tmp |&gt;\n    fill(region, .direction = \"down\")\n  if (year == 2005) {\n    prod_region_tmp$region[1]&lt;- \"Total Nacional\"\n  }\n\n  # Removing sub-regional data\n  prod_region_tmp &lt;-\n    prod_region_tmp |&gt;\n    filter(! region %in% c(\"Cajamarca\", \"Chota\", \"Jaén\",\n                           \"Abancay\",\"Andahuaylas\")\n    )\n\n  if (year == 2015 & any(colnames(prod_region_tmp) == \"región\")) {\n    q &lt;- which(colnames(prod_region_tmp) == \"región\")\n    if (q &gt; 0) {\n      prod_region_tmp &lt;- prod_region_tmp |&gt;\n        dplyr::select(-all_of(q))\n    }\n  }\n\n  resul &lt;-\n    prod_region_tmp |&gt;\n    pivot_longer(cols = -c(region, year), names_to = \"product\")\n\n  if (anneesup == 1) {\n    resul &lt;-\n      resul |&gt;\n      filter(year == max(year))\n  } else {\n    # Keeping only the last year available\n    resul &lt;-\n      resul |&gt;\n      filter(year == min(year))\n  }\n\n  resul &lt;-\n    resul |&gt;\n    mutate(\n      product = ifelse(product == \"arveja gr. seco\", yes = \"arveja grano seco\", no = product),\n      product = ifelse(product == \"frijol grano seco**\", yes = \"frijol grano seco\", no = product),\n      product = ifelse(product == \"frijol palo\", yes = \"frijol de palo\", no = product),\n      product = ifelse(product == \"frijol castlla\", yes = \"frijol castilla\", no = product),\n      product = ifelse(product == \"lentaja\", yes = \"lenteja\", no = product),\n      product = ifelse(product == \"frijol gr. seco\", yes = \"frijol grano seco\", no = product),\n      product = ifelse(product == \"haba gr. seco\", yes = \"haba grano seco\", no = product),\n      product = ifelse(product == \"maíz a. duro\", yes = \"maíz amarillo duro\", no = product),\n      product = ifelse(product == \"arveja gr. verde\", yes = \"arveja grano verde\", no = product),\n      product = ifelse(product == \"arveja gr. verde\", yes = \"arveja grano verde\", no = product),\n      product = ifelse(product == \"haba gr. verde\", yes = \"haba grano verde\", no = product),\n      product = ifelse(product == \"espá-rrago\", yes = \"espárrago\", no = product),\n      product = ifelse(product == \"manda-rina\", yes = \"mandarina\", no = product),\n      product = ifelse(product == \"maíz duro\", yes = \"maíz amarillo duro\", no = product),\n      product = ifelse(product == \"maíz am. duro\", yes = \"maíz amarillo duro\", no = product),\n      product = ifelse(product == \"pallar gr. seco\", yes = \"pallar grano seco\", no = product),\n      product = ifelse(product == \"pallar seco\", yes = \"pallar grano seco\", no = product),\n      product = ifelse(product == \"maíz amarillo duro\", yes = \"maíz amarillo duro\", no = product)\n    )\n\n  resul |&gt;\n    dplyr::mutate(month = month) |&gt;\n    dplyr::select(region, year, month, product, value)\n}# End of import_monthly_regional_values_P_year_P()\n\n\n#' Import monthly regional agricultural values from the Excel file,\n#' for all the months of the file.\n#'\n#' @param file path to the Excel File\n#' @param anneesup \n#' @param timescale\nimport_monthly_regional_values_P &lt;- function(file,\n                                             anneesup,\n                                             timescale) {\n\n  sheet_names &lt;- excel_sheets(file)\n  prod_region_monthly &lt;- map(\n    sheet_names,\n    import_monthly_regional_values_P_year_P,\n    file = file,\n    anneesup = anneesup\n  ) |&gt;\n    list_rbind()\n\n  # Cleaning years and changing type of values: from str to num\n  prod_region_monthly &lt;-\n    prod_region_monthly |&gt;\n    mutate(\n      year = str_remove(year, \"p/?\") |&gt; as.numeric(),\n      value_num = as.numeric(gsub(\" \", \"\", value)),\n      month = as.numeric(month)\n    )\n\n  # Values in 2014 to 2016 are expressed in thousands of tonnes\n  # These need to be expressed in tonnes\n  prod_region_monthly &lt;-\n    prod_region_monthly |&gt;\n    mutate(\n      value_num = ifelse(year %in% 2014:2015, value_num * 10^3, value_num)\n    )\n\n  # In addition, for the `Production_2014.xlsx` file, months 10 to 12\n  # report values in thousands of tonnes as well\n  if (str_detect(file, \"Production_2014.xlsx$\")) {\n    prod_region_monthly &lt;-\n      prod_region_monthly |&gt;\n      mutate(\n        value_num = ifelse(\n          month %in% c(10,11,12),\n          yes = value_num * 10^3,\n          no = value_num\n        )\n      )\n  }\n\n\n  # Removing the `value` columns to keep only the column with numerical values\n  prod_region_monthly &lt;-\n    prod_region_monthly |&gt;\n    dplyr::select(-value)\n\n  if (str_detect(file, \"Production_2002.xlsx$\")) {\n    for (i in c(3,6,9,12)) {\n      temp &lt;- missing_table(y = 2, mm = i, type = \"Production\")\n      prod_region_monthly &lt;- rbind(prod_region_monthly, temp)\n    }\n  }\n\n  if (str_detect(file, \"Production_2003.xlsx$\")) {\n    temp &lt;- missing_table(y = 3, mm = 3, type = \"Production\")\n    prod_region_monthly &lt;- rbind(prod_region_monthly, temp)\n\n  }\n# \n#   if (str_detect(file, \"Production_2008.xlsx$\")) {\n#     # Problem with value in June 2007 for Cassava: same value as in May\n#     # Let us put the value for June as NA\n#     # (For some regions)\n#     prod_region_monthly &lt;-\n#       prod_region_monthly |&gt;\n#       mutate(\n#         value_num = ifelse(\n#           month == 6 & product == \"yuca\" & region %in% c(\n#             \"Apurímac\", \"Arequipa\", \"Ayacucho\", \"Cusco\", \"Huancavelica\",\n#             \"Junín\", \"Loreto\", \"Madre de Dios\", \"Moquegua\", \"Moquegua\",\n#             \"Puno\", \"San Martín\", \"Tacna\", \"Ucayali\"),\n#           yes = NA,\n#           no = value_num\n#             )\n#       )\n#   }\n\n  # Cleaning region names\n  # prod_region_monthly$region |&gt; unique() |&gt; sort()\n  prod_region_monthly &lt;-\n    prod_region_monthly |&gt;\n    mutate(region = replace(region, region==\"Apurimac\", \"Apurímac\")) |&gt;\n    mutate(region = replace(region, region==\"Cajamarca_R\", \"Cajamarca\")) |&gt;\n    mutate(region = replace(\n      x = region,\n      list = region %in% c(\"Nacional\", \"Total nacional\", \"TOTAL NACIONAL\"),\n      values = \"Total Nacional\")\n    ) |&gt;\n    filter(! product == 0)\n\n\n  if (str_detect(file, \"Production_201(4|5).xlsx\")) {\n    prod_region_monthly &lt;- prod_region_monthly |&gt;\n      mutate(\n        product = case_when(\n          str_detect(product, \"arroz\")            ~  \"arroz cáscara\",\n          str_detect(product, \"banano\")           ~  \"plátano\",\n          str_detect(product, \"café\")             ~  \"café\",\n          str_detect(product, \"banano\")           ~  \"plátano\",\n          str_detect(product, \"caña\")             ~  \"caña de azúcar\",\n          str_detect(product, \"chocho\")           ~  \"chocho\",\n          str_detect(product, \"gar\")              ~  \"garbanzo\",\n          str_detect(product, \"caña\")             ~  \"caña de azúcar\",\n          str_detect(product, \"algodón\")          ~  \"algodón rama\",\n          str_detect(product, \"maíz amilaceo\")    ~  \"maíz amiláceo\",\n          str_detect(product, \"maíz amílaceo\")    ~  \"maíz amiláceo\",\n          str_detect(product, \"caña de azúcar para azúcar\")   ~  \"caña de azúcar\",\n          str_detect(product, \"arveja seca\")      ~  \"arveja grano seco\",\n          str_detect(product, \"arveja verde\")     ~  \"arveja grano verde\",\n          str_detect(product, \"loctao\")           ~  \"frijol loctao\",\n          str_detect(product, \"frijol seco\")      ~  \"frijol grano seco\",\n          str_detect(product, \"frijol castill\")   ~  \"frijol castilla\",\n          str_detect(product, \"haba seca\")        ~  \"haba grano seco\",\n          str_detect(product, \"haba seco\")        ~  \"haba grano seco\",\n          str_detect(product, \"haba verde\")       ~  \"haba grano verde\",\n          str_detect(product, \"limón sutil\")      ~  \"limón\",\n          str_detect(product, \"haba verde\")       ~  \"haba grano verde\",\n          str_detect(product, \"algodón rama\")     ~  \"algodón rama\",\n          str_detect(product, \"café\")             ~  \"café\",\n          str_detect(product, \"cañihua\")          ~  \"cañihua\",\n          str_detect(product, \"espárrago\")        ~  \"espárrago\",\n          str_detect(product, \"limón\")            ~  \"limón\",\n          str_detect(product, \"maíz amiláceo\")    ~  \"maíz amiláceo\",\n          str_detect(product, \"maíz choclo\")      ~  \"maíz choclo\",\n          str_detect(product, \"piña\")             ~  \"piña\",\n          str_detect(product, \"zaran-daja\")       ~  \"zarandaja\",\n          TRUE ~ product\n        )\n      )\n\n  }\n\n  Production &lt;- prod_region_monthly |&gt;\n    arrange(region, product, desc(month)) |&gt;\n    mutate(value_num = ifelse(is.na(value_num), yes = 0, no = value_num))\n\n  if (str_detect(file, \"Production_2008.xlsx$\")) {\n    # Problem with value in June 2007 for Cassava: same value as in May\n    # Let us put the value for June as NA\n    # (For some regions)\n    Production &lt;-\n      Production |&gt;\n      mutate(\n        value_num = ifelse(\n          month == 6 & product == \"yuca\" & region %in% c(\n            \"Apurímac\", \"Arequipa\", \"Ayacucho\", \"Cusco\", \"Huancavelica\",\n            \"Junín\", \"Loreto\", \"Madre de Dios\", \"Moquegua\", \"Moquegua\",\n            \"Puno\", \"San Martín\", \"Tacna\", \"Ucayali\"),\n          yes = NA,\n          no = value_num\n        )\n      )\n  }\n\n  Production &lt;- Production |&gt;\n    pivot_wider(names_from = month, values_from = value_num)\n\n  if (timescale == 1) {\n    # The production data in the file is a cumulative sum over the months\n    Production &lt;-\n      cbind(\n        Production[1:3],\n        Production[4:14] - Production[5:15],\n        Production[15]\n      ) |&gt;\n      pivot_longer(cols = -c(region, year, product), names_to = \"month\")\n    Production  &lt;-\n      Production[with(Production, order(region, year, as.numeric(month), product, value)), ]\n  }\n\n  if (timescale == 3) {\n    Production &lt;-\n      cbind(\n        Production[1:3],\n        Production[4] - Production[7],\n        Production[7] - Production[10],\n        Production[10] - Production[13],\n        Production[13]\n      ) |&gt;\n      pivot_longer(cols = -c(region, year, product), names_to = \"trim\")\n    Production  &lt;-\n      Production[with(Production, order(region, year, as.numeric(trim), product, value)), ]\n  }\n\n  if (timescale == 4) {\n    Production &lt;-\n      cbind(\n        Production[1:3],\n        Production[4] - Production[8],\n        Production[8] - Production[12],\n        Production[12]\n      ) |&gt;\n      pivot_longer(cols = -c(region, year, product), names_to = \"quadrim\")\n    Production  &lt;-\n      Production[with(Production, order(region, year, as.numeric(quadrim), product, value)), ]\n  }\n\n  if (timescale == 6) {\n    Production &lt;-\n      cbind(\n        Production[1:3],\n        Production[4] - Production[10],\n        Production[10]\n      ) |&gt;\n      pivot_longer(cols = -c(region, year, product), names_to = \"biannual\")\n    Production  &lt;-\n      Production[with(Production, order(region, year, as.numeric(biannual), product, value)), ]\n  }\n\n  if (timescale == 12) {\n    Production &lt;-\n      cbind(Production[1:3], Production[4]) |&gt;\n      rename(Prod_annual = \"12\")\n  }\n\n  if (str_detect(file, \"Production_2015.xlsx$\")) {\n    if(timescale == 12) {\n      Production &lt;- Production |&gt;\n        filter(!is.na(Prod_annual))\n    } else {\n      Production &lt;- Production |&gt;\n        filter(!is.na(value))\n    }\n\n  }\n  Production\n}# End of import_monthly_regional_values_P()\n\n\n\n2.1.2.2 Regional Planted Surface\n\n#' Import monthly regional agricultural values from the Excel file,\n#' for a given month\n#' \n#' @param sheet_name name of the Excel sheet\n#' @param file path to the Excel File\n#' @param anneesup\nimport_monthly_regional_values_year_S &lt;- function(sheet_name,\n                                                  file,\n                                                  anneesup) {\n  # The first two digits of the name: month\n  # the next four: the four digits of the year\n  # last digit: index of the sheet for a given month and year (we will note use it)\n\n  month &lt;- str_sub(sheet_name, 1, 2) |&gt; as.numeric()\n  year  &lt;- str_sub(sheet_name, 3, 6) |&gt; as.numeric()\n\n  tmp   &lt;- suppressMessages(\n    read_excel(\n      path = file,\n      sheet = sheet_name,\n      col_types = \"text\", n_max = 15, col_names = FALSE)\n  )\n\n  # We can use the first occurrence of \"Campaña\" to determine the beginning of the table\n  ind_row_year &lt;- str_which(tmp[[3]], regex(\"Cam?\", ignore_case = TRUE)) |&gt;\n    first()\n  if (sheet_name == \"062015\") {ind_row_year &lt;-  7}\n\n  # The first row of the header of the table is contained in the previous line\n  skip_head &lt;- ifelse(\n    year &gt;= 2008,\n    ind_row_year - 1,\n    ind_row_year\n    )\n\n  # The body of the table\n  sup_region_tmp &lt;-\n    suppressMessages(\n      read_excel(\n        path = file,\n        sheet = sheet_name,\n        col_types = \"text\", skip = skip_head, col_names = F\n      )\n    ) |&gt; \n    dplyr::select(\n      - where( \n        ~ all(\n          is.na(.)\n          )\n        )\n      )\n  \n  ind_row_year2 &lt;- \n    str_which(tmp[[2]], regex(\"Total?\", ignore_case = TRUE)) |&gt; \n    first()\n  \n  nmax = ifelse(ind_row_year2 - ind_row_year == 1, 1, 3)\n\n  header_1 &lt;-\n    suppressMessages(\n      read_excel(\n        path = file,\n        sheet = sheet_name,\n        skip = skip_head- 1, n_max = nmax, col_names = F\n      )\n    ) |&gt; \n    select(\n      - where( \n        ~ all(\n          is.na(.)\n          )\n        )\n      )\n\n\n  if (str_detect(file, \"Superficies_2016.xlsx$\")) {\n    header_1 &lt;-\n      suppressMessages(\n        read_excel(\n          path = file,\n          sheet = sheet_name,\n          skip = skip_head-1, n_max = 1, col_names = F\n        )\n      )\n  }\n\n\n  header_2 &lt;-\n    header_1 |&gt;\n    summarise(\n      across(\n        .cols = everything(),\n        .fns = ~format_header(.x)\n      )\n    )\n\n  colnames(sup_region_tmp) &lt;- as.character(header_2)\n  if (colnames(sup_region_tmp)[2] == \"\"){\n    colnames(sup_region_tmp)[2] &lt;- \"departamento\"\n  }\n  if (any(str_detect(colnames(sup_region_tmp), \"departamento\"), na.rm = T)) {\n    ind_dep_current &lt;- which(str_detect(colnames(sup_region_tmp), \"departamento\"))\n    colnames(sup_region_tmp)[ind_dep_current] &lt;- \"departamento\"\n  }\n\n  # Removing columns with no name\n  ind &lt;- !is.na(colnames(sup_region_tmp)) & (colnames(sup_region_tmp) != \"\")\n  sup_region_tmp &lt;-\n    sup_region_tmp |&gt;\n    dplyr::select(!!!colnames(sup_region_tmp)[ind])\n\n\n  if (str_detect(file, \"Superficies_2016.xlsx$\")) {\n    sup_region_tmp &lt;- sup_region_tmp |&gt;\n      mutate(\n        campaña = ifelse(\n          str_detect(campaña, regex(pattern = \"Ago 14\", ignore_case = TRUE)),\n          yes = \"14-15\",\n          no = campaña\n        ),\n        campaña = ifelse(\n          str_detect(campaña, regex(pattern = \"Ago 2014\", ignore_case = TRUE)),\n          yes = \"14-15\",\n          no = campaña\n        ),\n        campaña = ifelse(\n          str_detect(campaña, regex(pattern = \"Ago 15\", ignore_case = TRUE)),\n          yes = \"15-16\",\n          no = campaña\n        ),\n        campaña = ifelse(\n          str_detect(campaña, regex(pattern = \"Ago 2015\", ignore_case = TRUE)),\n          yes = \"15-16\",\n          no = campaña\n        ),\n        campaña = ifelse(\n          str_detect(campaña, regex(pattern = \"Ago 16\", ignore_case = TRUE)),\n          yes = \"16-17\",\n          no = campaña\n        ),\n        campaña = ifelse(\n          str_detect(campaña, regex(pattern = \"Ago 2016\", ignore_case = TRUE)),\n          yes = \"16-17\",\n          no = campaña\n        )\n      )\n  }\n\n  # Removing rows with all NAs\n  sup_region_tmp &lt;-\n    sup_region_tmp |&gt;\n    filter_all(any_vars(!is.na(.)))\n\n  # Removing rows where \"Continúa\" is found\n  sup_region_tmp &lt;-\n    sup_region_tmp |&gt;\n    filter_all(\n      any_vars(\n        !str_detect(., regex(pattern = \"Continúa\", ignore_case = TRUE))\n      )\n    )\n\n  # Removing the first colomn if row index\n  if (str_detect(colnames(sup_region_tmp)[1], \"[aeiou]\") == FALSE) {\n    sup_region_tmp &lt;-   sup_region_tmp[-1]\n  }\n  # The name of the first column differs accross sheets (departemento or region)\n  name_first_col &lt;- colnames(sup_region_tmp)[1]\n  if (! name_first_col %in% c(\"departamento\", \"región\")) {\n    warning(str_c(\"Issue with sheet: \", sheet_name))\n    return(NULL)\n  }\n\n  sup_region_tmp &lt;-\n    sup_region_tmp |&gt;\n    rename(region := !!name_first_col)\n\n  if (sheet_name == \"062015\") {\n    sup_region_tmp &lt;-\n      sup_region_tmp |&gt;\n      rename(campaña = colnames(sup_region_tmp)[2])\n  }\n\n  sup_region_tmp$campaña &lt;-\n    str_c(\"20\",str_sub(sup_region_tmp$campaña, 1, 2)) |&gt;\n    as.numeric()\n\n  sup_region_tmp &lt;-\n    sup_region_tmp |&gt; rename(year = campaña)\n\n  if (sheet_name == \"062015\") {\n    p &lt;- str_which(sup_region_tmp$region, \"Cajamarca\")\n    sup_region_tmp$region[p] &lt;- \"Cajamarca_R\"\n  } else {\n    p &lt;- str_which(sup_region_tmp$region, \"Cajamarca\") |&gt;\n      first()\n    sup_region_tmp$region[p] &lt;- \"Cajamarca_R\"\n  }\n\n  sup_region_tmp &lt;-\n    sup_region_tmp |&gt;\n    filter(!is.na(year), ! year %in% \"campaña\")\n\n  # The production of each product is given in two rows, but the product name is not repeated\n  sup_region_tmp &lt;-\n    sup_region_tmp |&gt;\n    fill(region, .direction = \"down\")\n\n  # Removing sub-regional data\n  sup_region_tmp &lt;-\n    sup_region_tmp |&gt;\n    filter(\n      ! region %in% c(\"Cajamarca\", \"Chota\", \"Jaén\", \"Abancay\",\"Andahuaylas\")\n    )\n  if (any(str_detect(colnames(sup_region_tmp), regex(pattern = \"ago\", ignore_case = TRUE)) == T)) {\n    p &lt;- which(str_detect(colnames(sup_region_tmp), regex(pattern = \"ago\", ignore_case = TRUE))==T)\n    colnames(sup_region_tmp)[p] &lt;- \"TOTAL\"\n  }\n\n  resul &lt;-\n    sup_region_tmp |&gt;\n    pivot_longer(cols = -c(region, year), names_to = \"product\")\n\n  if (anneesup == 1) {\n    resul &lt;-\n      resul |&gt;\n      filter(year == max(year))\n  } else {\n    resul &lt;-\n      resul |&gt;\n      filter(year == min(year))\n  }\n\n  resul &lt;-\n    resul |&gt; rename(campana = year)\n\n  resul &lt;- resul |&gt;\n    mutate(month = month) |&gt;\n    dplyr::select(region, campana, month, product, value)\n\n  resul &lt;-\n    resul |&gt;\n    mutate(\n      product = ifelse(\n        product == \"arveja gr. seco\",\n        yes = \"arveja grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol grano seco**\",\n        yes = \"frijol grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol palo\",\n        yes = \"frijol de palo\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol castlla\",\n        yes = \"frijol castilla\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"lentaja\",\n        yes = \"lenteja\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol gr. seco\",\n        yes = \"frijol grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"haba gr. seco\",\n        yes = \"haba grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"maíz a. duro\",\n        yes = \"maíz amarillo duro\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"arveja gr. verde\",\n        yes = \"arveja grano verde\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"arveja gr. verde\",\n        yes = \"arveja grano verde\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"haba gr. verde\",\n        yes = \"haba grano verde\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"espá-rrago\",\n        yes = \"espárrago\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"maíz duro\",\n        yes = \"maíz amarillo duro\",\n        no = product\n      ),\n    ) |&gt;\n    filter(! str_detect(product, \"contin\")) |&gt;\n    filter(! str_detect(product, \"conclusi\"))\n  resul\n}# End of import_monthly_regional_values_year_S ()\n\n\n#' Import monthly regional agricultural values from the Excel file,\n#' for all the months of the file.\n#'\n#' @param file path to the Excel File\n#' @param anneesup\n#' @param timescale\nimport_monthly_regional_values_S &lt;- function(file,\n                                             anneesup,\n                                             timescale){\n\n  sheet_names &lt;- excel_sheets(file)\n  surf_region_monthly &lt;- map(\n    sheet_names,\n    import_monthly_regional_values_year_S,\n    file = file,\n    anneesup = anneesup) |&gt;\n    list_rbind()\n\n  # Cleaning years and changing type of values: from str to num\n  surf_region_monthly &lt;-\n    surf_region_monthly |&gt;\n    mutate(year = str_sub(sheet_names[1], 3, 6) |&gt; as.numeric()) |&gt;\n    mutate(year = ifelse(anneesup == 0, year - 1,  year)) |&gt;\n    mutate(campana = str_remove(campana, \"p/?\") |&gt; as.numeric()) |&gt;\n    mutate(value_num = as.numeric(gsub(\" \", \"\", value)))\n\n  # Values in 2014 to 2016 are expressed in thousands of tonnes\n  # These need to be expressed in tonnes\n  surf_region_monthly &lt;-\n    surf_region_monthly |&gt;\n    mutate(\n      value_num = ifelse(year %in% 2014:2015, value_num * 10^3, value_num),\n      value_num = ifelse(\n        str_detect(!!file, \"Superficies_2014.xlsx$\") & month %in% c(10,11,12),\n        yes = value_num * 10^3,\n        no = value_num\n      )\n    )\n\n\n  # Removing the `value` columns to keep only the column with numerical values\n  surf_region_monthly &lt;-\n    surf_region_monthly |&gt;\n    dplyr::select(-value)\n\n\n  if (str_detect(file, \"Superficies_2002.xlsx$\")) {\n    for (i in c(3,6,9,12)) {\n      temp &lt;- missing_table(y = 2, mm = i, type = \"Superficies\") |&gt;\n        mutate(year = year - 1)\n      surf_region_monthly &lt;- rbind(surf_region_monthly, temp)\n    }\n  }\n\n  if (str_detect(file, \"Superficies_2003.xlsx$\")) {\n    temp &lt;- missing_table(y = 3, mm = 3, type = \"Superficies\") |&gt;\n      mutate(year = year -1)\n    surf_region_monthly &lt;- rbind(surf_region_monthly, temp)\n\n  }\n\n  # Cleaning region names\n  surf_region_monthly$region |&gt; unique() |&gt; sort()\n  surf_region_monthly &lt;-\n    surf_region_monthly |&gt;\n    mutate(region = replace(region, region==\"Apurimac\", \"Apurímac\")) |&gt;\n    mutate(region = replace(region, region==\"Cajamarca_R\", \"Cajamarca\")) |&gt;\n    mutate(region = replace(\n      x = region,\n      list = region %in% c(\"Nacional\", \"Total nacional\", \"TOTAL NACIONAL\",\n                           \"TOTAL  NACIONAL\"),\n      values = \"Total Nacional\")\n    )\n\n  surf_region_monthly &lt;- surf_region_monthly |&gt;\n    arrange(region, product, desc(month))\n\n  # Cleaning product names\n  surf_region_monthly &lt;-\n    surf_region_monthly |&gt;\n    # Replace extra spacing with only one space\n    mutate(product = str_replace_all(product, \"[[:blank:]]{2,}\", \" \"))\n\n  surf_region_monthly &lt;-\n    surf_region_monthly |&gt;\n    mutate(\n      product = ifelse(\n        product == \"arveja gr. seco\",\n        yes = \"arveja grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol castlla\",\n        yes = \"frijol castilla\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol gr. seco\",\n        yes = \"frijol grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"haba gr. seco\",\n        yes = \"haba grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"maíz a. duro\",\n        yes = \"maíz amarillo duro\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"algodón\",\n        yes = \"algodón rama\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"arroz\",\n        yes = \"arroz cáscara\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"arveja grano\",\n        yes = \"arveja grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol\",\n        yes = \"frijol grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"haba grano\",\n        yes = \"haba grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"    maíz am duro\",\n        yes = \"maíz amarillo duro\",\n        no = product\n      )\n    )\n\n\n  if (str_detect(file, \"Superficies_201(4|5).xlsx$\")) {\n    surf_region_monthly &lt;- surf_region_monthly |&gt;\n      mutate(\n        product = case_when(\n          str_detect(product, \"arroz\")           ~  \"arroz cáscara\",\n          str_detect(product, \"banano\")          ~  \"plátano\",\n          str_detect(product, \"café\")            ~  \"café\",\n          str_detect(product, \"banano\")          ~  \"plátano\",\n          str_detect(product, \"caña\")            ~  \"caña de azúcar\",\n          str_detect(product, \"chocho\")          ~  \"chocho\",\n          str_detect(product, \"gar\")             ~  \"garbanzo\",\n          str_detect(product, \"caña\")            ~  \"caña de azúcar\",\n          str_detect(product, \"algodón\")         ~  \"algodón rama\",\n          str_detect(product, \"maíz a. duro\")    ~  \"maíz amarillo duro\",\n          str_detect(product, \"maíz amilaceo\")   ~  \"maíz amiláceo\",\n          str_detect(product, \"maíz amílaceo\")   ~  \"maíz amiláceo\",\n          str_detect(product, \"maíz amiláceo\")   ~  \"maíz amiláceo\",\n          str_detect(product, \"caña de azúcar para azúcar\")   ~  \"caña de azúcar\",\n          str_detect(product, \"arveja seca\")     ~  \"arveja grano seco\",\n          str_detect(product, \"arveja verde\")    ~  \"arveja grano verde\",\n          str_detect(product, \"loctao\")          ~  \"frijol loctao\",\n          str_detect(product, \"frijol seco\")     ~  \"frijol grano seco\",\n          str_detect(product, \"frijol castill\")  ~  \"frijol castilla\",\n          str_detect(product, \"haba seca\")       ~  \"haba grano seco\",\n          str_detect(product, \"haba seco\")       ~  \"haba grano seco\",\n          str_detect(product, \"haba verde\")      ~  \"haba grano verde\",\n          str_detect(product, \"limón sutil\")     ~  \"limón\",\n          str_detect(product, \"haba verde\")      ~  \"haba grano verde\",\n          str_detect(product, \"algodón rama\")    ~  \"algodón rama\",\n          str_detect(product, \"café\")            ~  \"café\",\n          str_detect(product, \"cañihua\")         ~  \"cañihua\",\n          str_detect(product, \"espárrago\")       ~  \"espárrago\",\n          str_detect(product, \"limón\")           ~  \"limón\",\n          str_detect(product, \"maíz amiláceo\")   ~  \"maíz amiláceo\",\n          str_detect(product, \"maíz choclo\")     ~  \"maíz choclo\",\n          str_detect(product, \"piña\")            ~  \"piña\",\n          str_detect(product, \"zaran-daja\")      ~  \"zarandaja\",\n          TRUE ~ product\n        )\n      )\n  }\n\n  surf_region_monthly &lt;-\n    surf_region_monthly |&gt;\n    mutate(value_num = ifelse(is.na(value_num), yes = 0, no = value_num))|&gt;\n    mutate(date = str_c(year, month, sep = \"-\")) |&gt;\n    unique()\n\n  if (timescale == 12){\n    surf_region_monthly &lt;- surf_region_monthly |&gt;\n      filter(month == 7 ) |&gt;\n      dplyr::select(region, product, year, value_num) |&gt;\n      rename(surf_annual = value_num)\n  }\n  surf_region_monthly\n}# End of import_monthly_regional_values_S()\n\n\n\n2.1.2.3 Regional Harvested Surface\n\n#' Import monthly regional agricultural values from the Excel file,\n#' for a given month\n#' \n#' @param sheet_name name of the Excel sheet\n#' @param file path to the Excel File\nimport_monthly_regional_values_year_SR &lt;- function(sheet_name,\n                                                   file,\n                                                   anneesup) {\n  # The first two digits of the name: month\n  # the next four: the four digits of the year\n  # last digit: index of the sheet for a given month and year (we will note use it)\n\n  year  &lt;- str_sub(sheet_name, 3, 6) |&gt; as.numeric()\n  month &lt;- str_sub(sheet_name, 1, 2) |&gt; as.numeric()\n  tmp   &lt;- suppressMessages(\n    read_excel(\n      path = file,\n      sheet = sheet_name,\n      col_types = \"text\", n_max = 15, col_names = FALSE\n    )\n  )\n\n  # We can use the first occurrence of \"Años\" to determine the beginning of the table\n  ind_row_year &lt;- str_which(tmp[[3]], regex(\"años?\", ignore_case = TRUE))\n  if(length(ind_row_year) &gt; 0) ind_row_year &lt;- first(ind_row_year)\n  name_camp &lt;- 0\n  \n  if (length(ind_row_year) == 0) {\n    ind_row_year &lt;- str_which(tmp[[3]], regex(\"Campaña?\", ignore_case = TRUE)) |&gt;\n      first()\n    name_camp &lt;- 1\n  }\n  # The first row of the header of the table is contained in the previous line\n  skip_head &lt;- ifelse(\n    year &gt;= 2008, \n    ind_row_year - 1,\n    ind_row_year\n    )\n\n  # The body of the table\n  surfR_region_tmp &lt;-\n    suppressMessages(\n      read_excel(\n        path = file,\n        sheet = sheet_name,\n        col_types = \"text\", skip = skip_head, col_names = F\n      )\n    ) |&gt; \n    dplyr::select(\n       - where(\n         ~ all(is.na(.)\n               )\n         )\n       )\n\n  header_1 &lt;-\n    suppressMessages(\n      read_excel(\n        path = file,\n        sheet = sheet_name,\n        skip = skip_head-2, n_max = 2, col_names = F\n      )\n    )\n\n\n  if(year &lt; 2004){\n    header_1 &lt;- \n      suppressMessages(\n        read_excel(\n          path = file,\n          sheet = sheet_name, \n          skip = skip_head-1, \n          n_max = 1, \n          col_names = F))\n  }\n  \n  if(year &gt;= 2008){\n    header_1 &lt;- \n      suppressMessages(\n        read_excel(\n          path = file,\n          sheet = sheet_name, skip = skip_head-1, n_max = 2, col_names = F)) |&gt; \n      select(\n        - where( \n          ~ all(is.na(.))\n          )\n        )\n  }\n  header_2 &lt;-\n    header_1 |&gt;\n    summarise(across(.cols = everything(), .fns = ~format_header(.x)))\n\n  colnames(surfR_region_tmp) &lt;- as.character(header_2)\n  if (colnames(surfR_region_tmp)[2] == \"\") {\n    colnames(surfR_region_tmp)[2] &lt;- \"departamento\"\n  }\n\n  # Removing columns with no name\n  ind &lt;- !is.na(colnames(surfR_region_tmp)) & (colnames(surfR_region_tmp) != \"\")\n  surfR_region_tmp &lt;-\n    surfR_region_tmp |&gt; dplyr::select(!!!colnames(surfR_region_tmp)[ind])\n\n  # Removing rows with all NAs\n  surfR_region_tmp &lt;-\n    surfR_region_tmp |&gt;\n    filter_all(any_vars(!is.na(.)))\n\n  # Removing rows where \"Continúa\" is found\n  surfR_region_tmp &lt;-\n    surfR_region_tmp |&gt;\n    filter_all(\n      any_vars(!str_detect(., regex(pattern = \"Continúa\", ignore_case = TRUE)))\n    )\n\n  # Removing the first colomn if row index\n  if (str_detect(colnames(surfR_region_tmp)[1], \"[aeiou]\") == FALSE) {\n    surfR_region_tmp &lt;-  surfR_region_tmp[-1]\n  }\n  # The name of the first column differs accross sheets (departemento or region)\n  name_first_col &lt;- colnames(surfR_region_tmp)[1]\n  if (name_first_col %in% c(\"(ha) departamento\",\"mes : enero 2002-2003* departamento\")) {\n    colnames(surfR_region_tmp)[1] &lt;- \"departamento\"\n    name_first_col &lt;- colnames(surfR_region_tmp)[1]\n  }\n  if (! name_first_col %in% c(\"departamento\", \"región\")) {\n    warning(str_c(\"Issue with sheet: \", sheet_name))\n    return(NULL)\n    # stop(\"First column is not region\")\n  }\n\n  surfR_region_tmp &lt;-\n    surfR_region_tmp |&gt;\n    rename(region := !!name_first_col)\n  if (name_camp == 1) {\n    surfR_region_tmp &lt;-\n      surfR_region_tmp |&gt; rename(años = campaña)\n  }\n  if (any(str_detect(colnames(surfR_region_tmp), \"^año$\"))) {\n    surfR_region_tmp &lt;-\n      surfR_region_tmp |&gt; rename(year = año)\n  } else {\n    surfR_region_tmp &lt;-\n      surfR_region_tmp |&gt; rename(year = años)\n  }\n\n\n  if (any(str_detect(colnames(surfR_region_tmp), \"ene\"))) {\n    p &lt;- which(str_detect(colnames(surfR_region_tmp), \"ene\"))\n    surfR_region_tmp &lt;- surfR_region_tmp |&gt;\n      dplyr::select(-p)\n  }\n\n  surfR_region_tmp &lt;-\n    surfR_region_tmp |&gt;\n    filter(!is.na(year), ! year %in% c(\"Años\", \"Año\"))\n\n  p &lt;- str_which(surfR_region_tmp$region,\"Cajamarca\") |&gt;\n    first()\n  surfR_region_tmp$region[p] &lt;- \"Cajamarca_R\"\n\n  # The production of each product is given in two rows, but the product name is not repeated\n  surfR_region_tmp &lt;-\n    surfR_region_tmp |&gt;\n    fill(region, .direction = \"down\")\n  if (year == 2005) {\n    surfR_region_tmp$region[1]&lt;- \"Total Nacional\"\n  }\n\n  # Removing sub-regional data\n  surfR_region_tmp &lt;-\n    surfR_region_tmp |&gt;\n    filter(\n      ! region %in% c(\"Cajamarca\", \"Chota\", \"Jaén\", \"Abancay\",\"Andahuaylas\")\n    ) |&gt;\n    filter(! region == \"Región\")\n\n  resul &lt;-\n    surfR_region_tmp |&gt;\n    pivot_longer(cols = -c(region, year), names_to = \"product\")\n\n  if (sheet_name == \"062015\") {\n    resul &lt;- resul |&gt;\n      filter(! year %in% c(\"Continúa\", \"29\"))\n  }\n  if (anneesup == 1) {\n    resul &lt;-\n      resul |&gt;\n      filter(year == max(year))\n  } else {\n    resul &lt;-\n      resul |&gt;\n      filter(year == min(year))\n  }\n\n\n  resul &lt;-\n    resul |&gt;\n    mutate(\n      product = ifelse(\n        product == \"arveja gr. seco\",\n        yes = \"arveja grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol grano seco**\",\n        yes = \"frijol grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol palo\",\n        yes = \"frijol de palo\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol castlla\",\n        yes = \"frijol castilla\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"lentaja\",\n        yes = \"lenteja\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol gr. seco\",\n        yes = \"frijol grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"haba gr. seco\",\n        yes = \"haba grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"maíz a. duro\",\n        yes = \"maíz amarillo duro\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"arveja gr. verde\",\n        yes = \"arveja grano verde\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"haba gr. verde\",\n        yes = \"haba grano verde\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"espá-rrago\",\n        yes = \"espárrago\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"maíz duro\",\n        yes = \"maíz amarillo duro\",\n        no = product\n      ),\n    )\n\n  resul |&gt;\n    mutate(month = month) |&gt;\n    dplyr::select(region, year, month, product, value) |&gt;\n    filter(! product == \"total\") |&gt;\n    filter(! str_detect(product, \"contin\")) |&gt;\n    filter(! str_detect(product, \"conclusi\"))\n\n}# End of import_monthly_regional_values_P_year_P()\n\n\n#' Import monthly regional agricultural values from the Excel file,\n#' for all the months of the file.\n#'\n#' @param file path to the Excel File\n#' @param anneesup\n#' @param timescale\nimport_monthly_regional_values_SR &lt;- function(file,\n                                              anneesup,\n                                              timescale) {\n  sheet_names &lt;- excel_sheets(file)\n  surfR_region_monthly &lt;- map(\n    sheet_names,\n    import_monthly_regional_values_year_SR,\n    file = file,\n    anneesup = anneesup) |&gt;\n    list_rbind()\n\n  # Cleaning years and changing type of values: from str to num\n  surfR_region_monthly &lt;-\n    surfR_region_monthly |&gt;\n    mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n    mutate(value_num = as.numeric(gsub(\" \", \"\", value)))\n\n\n  # Values in 2014 to 2016 are expressed in thousands of tonnes\n  # These need to be expressed in tonnes\n  surfR_region_monthly &lt;-\n    surfR_region_monthly |&gt;\n    mutate(\n      value_num = ifelse(year %in% 2014:2015, value_num * 10^3, value_num),\n      value_num = ifelse(\n        str_detect(file, \"Superficies_R_2014.xlsx$\") & month %in% c(10,11,12),\n        yes = value_num * 10^3,\n        no = value_num\n      )\n    )\n\n\n\n  # Removing the `value` columns to keep only the column with numerical values\n  surfR_region_monthly &lt;-\n    surfR_region_monthly |&gt;\n    dplyr::select(-value)\n\n\n  if (str_detect(file, \"Superficies_R_2002.xlsx$\")) {\n    for (i in c(3,6,9,12)) {\n      temp &lt;- missing_table(y = 2, mm = i, type = \"Superficies_R\")\n      surfR_region_monthly &lt;- rbind(surfR_region_monthly, temp)\n    }\n  }\n\n  if (str_detect(file, \"Superficies_R_2003.xlsx$\")) {\n    temp &lt;- missing_table(y = 3, mm = 3, type = \"Superficies_R\")\n    surfR_region_monthly &lt;- rbind(surfR_region_monthly, temp)\n  }\n  # Cleaning region names\n  surfR_region_monthly &lt;-\n    surfR_region_monthly |&gt;\n    unique() |&gt;\n    mutate(region = replace(region, region==\"Apurimac\", \"Apurímac\")) |&gt;\n    mutate(region = replace(region, region==\"Cajamarca_R\", \"Cajamarca\")) |&gt;\n    mutate(\n      region = replace(\n        x = region,\n        list = region %in% c(\"Nacional\", \"Total nacional\", \"TOTAL NACIONAL\"),\n        values = \"Total Nacional\"\n      )\n    )\n\n\n  surfR_region_monthly &lt;-\n    surfR_region_monthly |&gt;\n    mutate(\n      product = ifelse(\n        product == \"arveja gr. seco\",\n        yes = \"arveja grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol castlla\",\n        yes = \"frijol castilla\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol gr. seco\",\n        yes = \"frijol grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"haba gr. seco\",\n        yes = \"haba grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"maíz a. duro\",\n        yes = \"maíz amarillo duro\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"algodón\",\n        yes = \"algodón rama\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"arroz\",\n        yes = \"arroz cáscara\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"arveja grano\",\n        yes = \"arveja grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol\",\n        yes = \"frijol grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"haba grano\",\n        yes = \"haba grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"    maíz am duro\",\n        yes = \"maíz amarillo duro\",\n        no = product\n      )\n    )\n\n\n  if (str_detect(file, \"Superficies_R_201(4|5).xlsx$\")) {\n    surfR_region_monthly &lt;- surfR_region_monthly |&gt;\n      mutate(\n        product = case_when(\n          str_detect(product, \"arroz\")     ~  \"arroz cáscara\",\n          str_detect(product, \"banano\")    ~  \"plátano\",\n          str_detect(product, \"café\")      ~  \"café\",\n          str_detect(product, \"banano\")    ~  \"plátano\",\n          str_detect(product, \"caña\")      ~  \"caña de azúcar\",\n          str_detect(product, \"chocho\")    ~  \"chocho\",\n          str_detect(product, \"gar\")       ~  \"garbanzo\",\n          str_detect(product, \"caña\")      ~  \"caña de azúcar\",\n          str_detect(product, \"algodón\")   ~  \"algodón rama\",\n          str_detect(product, \"maíz amilaceo\")   ~  \"maíz amiláceo\",\n          str_detect(product, \"maíz amílaceo\")   ~  \"maíz amiláceo\",\n          str_detect(product, \"caña de azúcar para azúcar\")   ~  \"caña de azúcar\",\n          str_detect(product, \"arveja seca\")     ~  \"arveja grano seco\",\n          str_detect(product, \"arveja verde\")    ~  \"arveja grano verde\",\n          str_detect(product, \"loctao\")          ~  \"frijol loctao\",\n          str_detect(product, \"frijol seco\")     ~  \"frijol grano seco\",\n          str_detect(product, \"frijol castill\")  ~  \"frijol castilla\",\n          str_detect(product, \"haba seca\")       ~  \"haba grano seco\",\n          str_detect(product, \"haba seco\")       ~  \"haba grano seco\",\n          str_detect(product, \"haba verde\")      ~  \"haba grano verde\",\n          str_detect(product, \"limón sutil\")     ~  \"limón\",\n          str_detect(product, \"haba verde\")      ~  \"haba grano verde\",\n          str_detect(product, \"algodón rama\")    ~  \"algodón rama\",\n          str_detect(product, \"café\")            ~  \"café\",\n          str_detect(product, \"cañihua\")         ~  \"cañihua\",\n          str_detect(product, \"espárrago\")       ~  \"espárrago\",\n          str_detect(product, \"limón\")           ~  \"limón\",\n          str_detect(product, \"maíz amiláceo\")   ~  \"maíz amiláceo\",\n          str_detect(product, \"maíz choclo\")     ~  \"maíz choclo\",\n          str_detect(product, \"piña\")            ~  \"piña\",\n          str_detect(product, \"zaran-daja\")      ~  \"zarandaja\",\n          TRUE ~ product\n        )\n      )\n  }\n\n  SurfaceR &lt;- surfR_region_monthly |&gt;\n    arrange(region, product, desc(month)) |&gt;\n    mutate(value_num = ifelse(is.na(value_num), yes = 0, no = value_num))|&gt;\n    pivot_wider(names_from = month, values_from = value_num)\n\n  if (timescale == 1) {\n    SurfaceR &lt;-\n      cbind(SurfaceR[1:3], SurfaceR[4:14] - SurfaceR[5:15], SurfaceR[15]) |&gt;\n      pivot_longer(cols = -c(region, year, product), names_to = \"month\")\n    SurfaceR  &lt;- SurfaceR[with(SurfaceR, order(region, year, as.numeric(month), product, value)), ]\n  }\n\n  if (timescale == 3) {\n    SurfaceR &lt;-\n      cbind(\n        SurfaceR[1:3],\n        SurfaceR[4] - SurfaceR[7],\n        SurfaceR[7] - SurfaceR[10],\n        SurfaceR[10] - SurfaceR[13],\n        SurfaceR[13]\n      ) |&gt;\n      pivot_longer(cols = -c(region, year, product), names_to = \"trim\")\n    SurfaceR &lt;- SurfaceR[with(SurfaceR, order(region, year, as.numeric(trim), product, value)), ]\n  }\n\n  if (timescale == 4) {\n    SurfaceR &lt;-\n      cbind(\n        SurfaceR[1:3],\n        SurfaceR[4] - SurfaceR[8],\n        SurfaceR[8] - SurfaceR[12],\n        SurfaceR[12]\n      ) |&gt;\n      pivot_longer(cols = -c(region, year, product), names_to = \"quadrim\")\n    SurfaceR &lt;- SurfaceR[with(SurfaceR, order(region, year, as.numeric(quadrim), product, value)), ]\n  }\n\n  if (timescale == 6) {\n    SurfaceR &lt;-\n      cbind(\n        SurfaceR[1:3],\n        SurfaceR[4] - SurfaceR[10],\n        SurfaceR[10]\n      ) |&gt;\n      pivot_longer(cols = -c(region, year, product), names_to = \"biannual\")\n    SurfaceR &lt;- SurfaceR[with(SurfaceR, order(region, year, as.numeric(biannual), product, value)), ]\n  }\n\n  if (timescale == 12) {\n    SurfaceR &lt;-\n      cbind(SurfaceR[1:3], SurfaceR[4]) |&gt;\n      rename(surf_R_annual = \"12\")\n  }\n\n  SurfaceR\n}# End of import_monthly_regional_values_SR()\n\n\n\n2.1.2.4 Regional Prices\n\n#' Import monthly regional agricultural values from the Excel file,\n#' for a given month\n#' \n#' @param sheet_name name of the Excel sheet\n#' @param file path to the Excel File\n#' @param anneesup \nimport_monthly_regional_values_year_Px &lt;- function(sheet_name,\n                                                   file,\n                                                   anneesup) {\n  # The first two digits of the name: month\n  # the next four: the four digits of the year\n  # last digit: index of the sheet for a given month and year (we will note use it)\n  year  &lt;- str_sub(sheet_name, 3, 6) |&gt; as.numeric()\n  month &lt;- str_sub(sheet_name, 1, 2) |&gt; as.numeric()\n  tmp   &lt;- suppressMessages(\n    read_excel(\n      path = file,\n      sheet = sheet_name,\n      col_types = \"text\",\n      n_max = 15,\n      col_names = FALSE\n    )\n  )\n\n  # We can use the first occurrence of \"Año\" to determine the beginning of the table\n  ind_row_year &lt;- str_which(tmp[[4]], regex(\"año.?\", ignore_case = TRUE))\n  if (length(ind_row_year) == 0) {\n    ind_row_year &lt;- str_which(tmp[[3]], regex(\"año.?\", ignore_case = TRUE)) |&gt;\n      first()\n  } else {\n    ind_row_year &lt;- first(ind_row_year)\n  }\n\n  if (sheet_name == \"062015\") {ind_row_year &lt;- 8}\n\n  # Replacing column 2 into column 3\n  if (y &gt; 4) {\n    tmp[1:ind_row_year+2, 3] &lt;- tmp[1:ind_row_year + 2, 2]\n    tmp[1:ind_row_year +2, 2] &lt;- NA\n  }\n\n  # The first row of the header of the table is contained in the previous line\n  skip_head &lt;- ind_row_year\n\n  # The body of the table\n  prx_region_tmp &lt;-\n    suppressMessages(\n      read_excel(\n        path = file,\n        sheet = sheet_name,\n        col_types = \"text\", col_names = F\n      )\n    )\n\n  positions &lt;-\n    which(prx_region_tmp[,4] == regex(pattern = \"Año\", ignore_case = TRUE))\n\n\n  if (str_detect(file, \"Prices_2015.xlsx$\")) {\n    positions &lt;-\n      which(prx_region_tmp[,3] == regex(pattern = \"Año\", ignore_case = TRUE))\n  }\n  if (length(positions) == 0) {\n    positions &lt;-\n      which(prx_region_tmp[,3] == regex(pattern = \"Año\", ignore_case = TRUE))\n  }\n\n  resul &lt;-NULL\n\n  for (i in 1:length(positions)) {\n    prx_region_tmp &lt;-\n      suppressMessages(\n        read_excel(\n          path = file,\n          sheet = sheet_name,\n          col_types = \"text\",\n          col_names = F,\n          skip = positions[i] - 1,\n          n_max = ifelse(\n            i == length(positions),\n            yes = 1000,\n            no = positions[i + 1] - positions[i]\n          )-1\n        )\n      )\n\n    header_1 &lt;- prx_region_tmp[1:2, ]\n    if (y &lt; 6) {header_1 &lt;- prx_region_tmp[1, ]}\n    header_2 &lt;-\n      header_1 |&gt;\n      summarise(\n        across(\n          .cols = everything(),\n          .fns = ~format_header(.x)\n        )\n      )\n    if (str_detect(header_2[1,2], regex(\"Gobierno Regional\", ignore_case = TRUE)) == T){\n      header_2[2] &lt;-  \"departamento\"\n    }\n\n    header_2[3] &lt;- header_2[2]\n    header_2[2] &lt;- \"\"\n\n    if (str_detect(file, \"Prices_2015.xlsx$\")) {\n      header_2 &lt;-\n        header_1 |&gt;\n        summarise(\n          across(\n            .cols = everything(),\n            .fns = ~format_header(.x)\n          )\n        )\n    }\n\n    colnames(prx_region_tmp) &lt;- as.character(header_2)\n\n    if (y &lt; 6) {\n      if (colnames(prx_region_tmp)[2] == \"\") {\n        colnames(prx_region_tmp)[2] &lt;- \"departamento\"\n        colnames(prx_region_tmp)[3] &lt;- \"año\"\n      }\n      prx_region_tmp &lt;- prx_region_tmp[-1, ]\n    } else {\n      if (colnames(prx_region_tmp)[3] == \"\") {\n        colnames(prx_region_tmp)[2] &lt;- \"departamento\"\n      }\n      prx_region_tmp &lt;- prx_region_tmp[-c(1:2), ]\n    }\n\n    cell &lt;- ifelse(str_detect(file, \"Prices_2015.xlsx$\"), yes = 2, no = 3)\n\n    if (!is.na(prx_region_tmp[1, 2]) &\n        str_detect(prx_region_tmp[1, 2], regex(\"Promedio Nacional\", ignore_case = TRUE)\n        ) == T){\n      prx_region_tmp[1, cell]  &lt;- \"PROMEDIO NACIONAL\"\n    }\n    if (!is.na(prx_region_tmp[1, 2]) &\n        str_detect(prx_region_tmp[1, 2], regex(\"Gobierno Regional\", ignore_case = TRUE)) == T) {\n      prx_region_tmp[1, cell]  &lt;- \"PROMEDIO NACIONAL\"\n    }\n\n    if (sheet_name == \"072007\") {prx_region_tmp[, 3] &lt;- prx_region_tmp[2]}\n    # Removing columns with no name\n    ind &lt;- !is.na(colnames(prx_region_tmp)) & (colnames(prx_region_tmp) != \"\")\n    prx_region_tmp &lt;-\n      prx_region_tmp |&gt; dplyr::select(!!!colnames(prx_region_tmp)[ind]) |&gt;\n      {\\(x) x[, -1]}()\n\n    # Removing rows with all NAs\n    prx_region_tmp &lt;-\n      prx_region_tmp |&gt;\n      filter_all(any_vars(!is.na(.)))\n\n    # Removing rows where \"Continúa\" is found\n    prx_region_tmp &lt;-\n      prx_region_tmp |&gt;\n      filter_all(\n        any_vars(\n          !str_detect(., regex(pattern = \"Continúa\", ignore_case = TRUE))\n        )\n      )\n\n    # Removing the first colomn if row index\n    if (str_detect(colnames(prx_region_tmp)[1], \"[aeiou]\") == FALSE) {\n      prx_region_tmp &lt;- prx_region_tmp[-1]\n    }\n    # The name of the first column differs accross sheets (departemento or region)\n    name_first_col &lt;- colnames(prx_region_tmp)[1]\n    if (! name_first_col %in% c(\"departamento\", \"región\")) {\n      warning(str_c(\"Issue with sheet: \", sheet_name))\n      return(NULL)\n    }\n\n    prx_region_tmp &lt;-\n      prx_region_tmp |&gt;\n      rename(region := !!name_first_col)\n\n    prx_region_tmp &lt;-\n      prx_region_tmp |&gt; rename(year = año)\n\n    if (sheet_name == \"062015\") {\n      p &lt;- str_which(prx_region_tmp$region,\"Cajamarca\")\n      prx_region_tmp$region[p] &lt;- \"Cajamarca_R\"\n    } else {\n      p &lt;- str_which(prx_region_tmp$region,\"Cajamarca\") |&gt;\n        first()\n      prx_region_tmp$region[p] &lt;- \"Cajamarca_R\"\n    }\n\n    # The production of each product is given in two rows, but the product name is not repeated\n    prx_region_tmp &lt;-\n      prx_region_tmp |&gt;\n      fill(region, .direction = \"down\")\n\n    # Removing sub-regional data\n    prx_region_tmp &lt;-\n      prx_region_tmp |&gt;\n      filter(\n        ! region %in% c(\"Cajamarca\", \"Chota\", \"Jaén\", \"Abancay\",\"Andahuaylas\")\n      ) |&gt;\n      filter(! year %in% c(NA))\n\n    resul_current &lt;-\n      prx_region_tmp |&gt;\n      pivot_longer(cols = -c(region, year), names_to = \"product\")\n\n    resul &lt;- resul |&gt;\n      bind_rows(resul_current)\n\n    if(sheet_name == \"062015\"){\n      resul &lt;- resul |&gt;\n        filter(year %in% c(2014, 2015))\n    }\n  }\n\n  # Correction for product names\n  resul &lt;-\n    resul |&gt;\n    mutate(\n      product = ifelse(\n        product == \"arveja gr. seco\",\n        yes = \"arveja grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol grano seco**\",\n        yes = \"frijol grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol palo\",\n        yes = \"frijol de palo\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol castlla\",\n        yes = \"frijol castilla\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"lentaja\",\n        yes = \"lenteja\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"frijol gr. seco\",\n        yes = \"frijol grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"haba gr. seco\",\n        yes = \"haba grano seco\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"maíz a. duro\",\n        yes = \"maíz amarillo duro\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"arveja gr. verde\",\n        yes = \"arveja grano verde\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"haba gr. verde\",\n        yes = \"haba grano verde\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"espá-rrago\",\n        yes = \"espárrago\",\n        no = product\n      ),\n      product = ifelse(\n        product == \"maíz duro\",\n        yes = \"maíz amarillo duro\",\n        no = product\n      ),\n    )\n\n  if (anneesup == 1) {\n    resul &lt;-\n      resul |&gt;\n      filter(year == max(year))\n  } else {\n    # Keeping only the last year available\n    resul &lt;-\n      resul |&gt;\n      filter(year == min(year))\n  }\n\n  resul |&gt;\n    mutate(month = month) |&gt;\n    dplyr::select(region, year, month, product, value)\n}# End of import_monthly_regional_values_year_Px ()\n\n\n#' Import monthly regional agricultural values from the Excel file,\n#' for all the months of the file.\n#'\n#' @param file path to the Excel File\n#' @param anneesup\n#' @param timescale\nimport_monthly_regional_values_Px &lt;- function(file,\n                                              anneesup,\n                                              timescale) {\n  sheet_names &lt;- excel_sheets(file)\n  prx_region_monthly &lt;- map(\n    sheet_names,\n    import_monthly_regional_values_year_Px,\n    file = file,anneesup= anneesup\n  ) |&gt;\n    list_rbind()\n  # Cleaning years and changing type of values: from str to num\n  prx_region_monthly &lt;-\n    prx_region_monthly |&gt;\n    mutate(year = str_remove(year, \"p/?\") |&gt; as.numeric()) |&gt;\n    mutate(\n      value_num = str_remove_all(value, pattern = \" \") |&gt;\n        str_replace_all(pattern = \",\", replacement = \".\") |&gt;\n        as.numeric()\n    )\n\n  # Values in 2015 are expressed in thousands of tonnes\n  # These need to be expressed in tonnes\n  if (str_detect(file, \"Prices_2015.xlsx$\")) {\n    prx_region_monthly &lt;-\n      prx_region_monthly |&gt;\n      mutate(\n        value_num = value_num,\n        value_num = ifelse(month &gt; 4,\n          yes = value_num / 10^3,\n          no = value_num\n        )\n      )\n  }\n\n\n  # Removing the `value` columns to keep only the column with numerical values\n  prx_region_monthly &lt;-\n    prx_region_monthly |&gt;\n    dplyr::select(-value)\n\n\n  # Cleaning region names\n  prx_region_monthly$region |&gt; unique() |&gt; sort()\n  prx_region_monthly &lt;-\n    prx_region_monthly |&gt;\n    mutate(region = replace(region, region==\"Apurimac\", \"Apurímac\")) |&gt;\n    mutate(region = replace(region, region==\"Cajamarca_R\", \"Cajamarca\")) |&gt;\n    mutate(\n      region = replace(\n        x = region,\n        list = region %in% c(\"Nacional\", \"Promedio nacional\", \"PROMEDIO NACIONAL\"),\n        values = \"Total Nacional\"\n      )\n    )\n\n  # Cleaning product names\n  prx_region_monthly &lt;- prx_region_monthly\n  prx_region_monthly &lt;- prx_region_monthly |&gt;\n    # Replace extra spacing with only one space\n    mutate(product = str_replace_all(product, \"[[:blank:]]{2,}\", \" \"))\n\n  prx_region_monthly &lt;-\n    prx_region_monthly |&gt;\n    mutate(\n      product = ifelse(\n        product == \"arveja gr. seco\", yes = \"arveja grano seco\", no = product\n      ),\n      product = ifelse(\n        product == \"frijol castlla\", yes = \"frijol castilla\", no = product\n      ),\n      product = ifelse(\n        product == \"frijol gr. seco\", yes = \"frijol grano seco\", no = product\n      ),\n      product = ifelse(\n        product == \"haba gr. seco\", yes = \"haba grano seco\", no = product\n      ),\n      product = ifelse(\n        product == \"maíz a. duro\", yes = \"maíz amarillo duro\", no = product\n      )\n    )\n\n\n  if (str_detect(file, \"Prices_201(4|5).xlsx$\")) {\n    prx_region_monthly &lt;- prx_region_monthly |&gt;\n      mutate(\n        product = case_when(\n          str_detect(product, \"arroz\")           ~  \"arroz cáscara\",\n          str_detect(product, \"banano\")          ~  \"plátano\",\n          str_detect(product, \"café\")            ~  \"café\",\n          str_detect(product, \"banano\")          ~  \"plátano\",\n          str_detect(product, \"caña\")            ~  \"caña de azúcar\",\n          str_detect(product, \"chocho\")          ~  \"chocho\",\n          str_detect(product, \"gar\")             ~  \"garbanzo\",\n          str_detect(product, \"caña\")            ~  \"caña de azúcar\",\n          str_detect(product, \"algodón\")         ~  \"algodón rama\",\n          str_detect(product, \"maíz amilaceo\")   ~  \"maíz amiláceo\",\n          str_detect(product, \"maíz amílaceo\")   ~  \"maíz amiláceo\",\n          str_detect(product, \"caña de azúcar para azúcar\")   ~  \"caña de azúcar\",\n          str_detect(product, \"arveja seca\")     ~  \"arveja grano seco\",\n          str_detect(product, \"arveja verde\")    ~  \"arveja grano verde\",\n          str_detect(product, \"loctao\")          ~  \"frijol loctao\",\n          str_detect(product, \"frijol seco\")     ~  \"frijol grano seco\",\n          str_detect(product, \"frijol castill\")  ~  \"frijol castilla\",\n          str_detect(product, \"haba seca\")       ~  \"haba grano seco\",\n          str_detect(product, \"haba seco\")       ~  \"haba grano seco\",\n          str_detect(product, \"haba verde\")      ~  \"haba grano verde\",\n          str_detect(product, \"limón sutil\")     ~  \"limón\",\n          str_detect(product, \"haba verde\")      ~  \"haba grano verde\",\n          str_detect(product, \"algodón rama\")    ~  \"algodón rama\",\n          str_detect(product, \"café\")            ~  \"café\",\n          str_detect(product, \"cañihua\")         ~  \"cañihua\",\n          str_detect(product, \"espárrago\")       ~  \"espárrago\",\n          str_detect(product, \"limón\")           ~  \"limón\",\n          str_detect(product, \"maíz amiláceo\")   ~  \"maíz amiláceo\",\n          str_detect(product, \"maíz choclo\")     ~  \"maíz choclo\",\n          str_detect(product, \"piña\")            ~  \"piña\",\n          str_detect(product, \"zaran-daja\")      ~  \"zarandaja\",\n          TRUE ~ product\n        )\n      )\n  }\n\n  prx_region_monthly &lt;-\n    prx_region_monthly |&gt;\n    mutate(value_num = ifelse(is.na(value_num), yes = 0, no = value_num))|&gt;\n    mutate(date = str_c(year, month, sep = \"-\")) |&gt;\n    unique()\n\n  prx_region_monthly\n}# End of import_monthly_regional_values_Px ()",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Agricultural Data</span>"
    ]
  },
  {
    "objectID": "data-agriculture.html#download-the-data",
    "href": "data-agriculture.html#download-the-data",
    "title": "2  Agricultural Data",
    "section": "2.2 Download the Data",
    "text": "2.2 Download the Data\nOnce the functions to downloaded the data are defined, we can use them.\n\n# Production \ndata_P_TOTAL &lt;- NULL\ndata_P_TOTAL_A &lt;- NULL\n# Planted surface\ndata_S_TOTAL &lt;- NULL\ndata_S_TOTAL_A &lt;- NULL\n# Harvested surface \ndata_SR_TOTAL &lt;- NULL\ndata_SR_TOTAL_A &lt;- NULL\n# Prices\ndata_Px_TOTAL &lt;- NULL\n\n\ndownload_again &lt;- FALSE\nif (download_again) {\n  for (y in c(2:14)) {\n    year &lt;-  ifelse( y &lt; 10, paste(\"200\", y, sep=\"\"), paste(\"20\", y, sep=\"\"))\n    print(year)\n    # Code for years in PDF files\n    if (y %in% c(2,3,4)) {\n      for (m in 12:1) {\n        print(m)\n        #Months not available in PDF Files\n        if ( m == 3 & y == 3) {next}\n        if ( m == 3 & y == 2) {next}\n        if ( m == 6 & y == 2) {next}\n        if ( m == 9 & y == 2) {next}\n        if ( m == 12 & y == 2) {next}\n        \n        # URL to download the pdf files\n        link &lt;- str_c(\n          \"https://www.midagri.gob.pe/portal/download/pdf\",\n          \"/herramientas/boletines/boletineselectronicos/\",\n          \"estadisticaagrariamensual/\", year, \"/EAM\",\n          str_sub(year, 3, 4),\n          ifelse(m &lt; 10, paste(\"0\", m, sep = \"\"), m),\n          \".pdf\"\n        )\n        \n        # # Production - Pages 44,45, 46 and 47 of the PDF Files\n        name &lt;- str_c(\n          \"../data/raw/minagri/Production/Production_\", year, \".xlsx\"\n        )\n        for (i in 0:1) {\n          page1 &lt;- extract_pdf_data(\n            annee = y,\n            mois = m,\n            adresse = link,\n            page = 44 + 2 * i, \n            Cell1 = \"Departamento\"\n          )\n          page2 &lt;- extract_pdf_data(\n            annee = y, \n            mois = m, \n            adresse = link,\n            page = 45 + 2 * i,\n            Cell1 = \"Departamento\"\n          )\n          Tableau &lt;- rbind(page1, page2)\n          \n          write.xlsx(\n            Tableau, \n            name, \n            sheetName = str_c(\n              ifelse(m &lt; 10, paste(\"0\",m, sep=\"\"), m),\n              year,\n              i + 1\n            ),\n            append = TRUE,\n            showNA = FALSE\n          )\n          print(paste(\"Production\", i, \"ok \"))\n        }\n        # Planted surface - Pages 30, 31, 32, 33 of the PDF Files\n        name &lt;- str_c(\n          \"../data/raw/minagri/Surface/Superficies_\", year, \".xlsx\"\n        )\n        for (i in 0:1) {\n          page1 &lt;- extract_pdf_data(\n            annee = y,\n            mois = m,\n            adresse = link,\n            page = 30 + 2 * i,\n            Cell1 = \"Departamento\"\n          )\n          page2 &lt;- extract_pdf_data(\n            annee = y, \n            mois = m,\n            adresse = link,\n            page = 31 + 2 * i,\n            Cell1 = \"Departamento\"\n          )\n          Tableau &lt;-  rbind(page1, page2)\n          write.xlsx(\n            Tableau,\n            name,\n            sheetName = str_c(\n              ifelse(m &lt; 10, paste(\"0\", m, sep = \"\"), m), \n              year,\n              i + 1\n            ), \n            append = TRUE, \n            showNA = FALSE\n          )\n          print(paste(\"Surface\", i, \"ok \"))\n        }\n        \n        # Harvested surface - Pages 38,45, 46 and 47 of the PDF Files\n        name &lt;- str_c(\n          \"../data/raw/minagri/Surface_R/Superficies_R_\", year, \".xlsx\"\n        )\n        for (i in 0:1) {\n          page1 &lt;- extract_pdf_data(\n            annee = y,\n            mois = m, \n            adresse = link, \n            page = 38 + 2 * i,\n            Cell1 = \"Departamento\"\n          )\n          page2 &lt;- extract_pdf_data(\n            annee = y, \n            mois = m, \n            adresse = link, \n            page = 39 + 2 * i, \n            Cell1 = \"Departamento\"\n          )\n          Tableau &lt;-  rbind(page1, page2)\n          write.xlsx(\n            Tableau,\n            name,\n            sheetName = str_c(\n              ifelse(m &lt; 10, paste(\"0\", m, sep = \"\"), m),\n              year,\n              i + 1\n            ), \n            append = TRUE,\n            showNA = FALSE\n          )\n          print(paste(\"Surface R\", i, \"ok \"))\n        }\n        \n        # Prices - Pages 109,110, 111, 112, 113, 114 or 115 depending on the PDF Files\n        name &lt;- str_c(\n          \"../data/raw/minagri/Prices/Prices_\", year, \".xlsx\"\n        )\n        for (i in 0:1) {\n          if (y &lt; 3 & m &lt; 8) {\n            page2002 &lt;- case_when(m == 7 ~ 4, TRUE ~ 3)\n          } else {\n            page2002 &lt;- 0\n          }\n          page1 &lt;- extract_pdf_data(\n            annee = y,\n            mois = m,\n            adresse = link, \n            page = 112 +2 * i - page2002, \n            Cell1 = \"Departamento\"\n          )\n          page2 &lt;- extract_pdf_data(\n            annee = y,\n            mois = m,\n            adresse = link, \n            page = 113 + 2 * i - page2002,\n            Cell1 = \"Departamento\"\n          )\n          \n          if((ncol(page1) != ncol(page2)) & \n             any(str_detect(page2[,ncol(page2)], \"Plátano\"))) {\n            page2 &lt;- page2[,-ncol(page2)]\n          }\n          Tableau &lt;-  rbind(page1, page2)\n          write.xlsx(\n            Tableau, \n            name,\n            sheetName = str_c(\n              ifelse(m &lt; 10, paste(\"0\", m, sep = \"\"), m),\n              year,\n              i + 1\n            ),\n            append = TRUE,\n            showNA = FALSE\n          )\n        }\n      }\n    } else {\n      # Code for years in Excel files - 2006 and above\n      # Remark : 2005 data are extracted manually\n      if(y == 5) {\n        next\n      } else {\n        # Downloading and extracting the excel sheets of interest\n        download.data(y, out_folder = \"../data/raw/minagri/\")\n      }\n    }\n  }\n}",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Agricultural Data</span>"
    ]
  },
  {
    "objectID": "data-agriculture.html#sec-data-agriculture-import",
    "href": "data-agriculture.html#sec-data-agriculture-import",
    "title": "2  Agricultural Data",
    "section": "2.3 Import Data in R",
    "text": "2.3 Import Data in R\nWe loop over the years to import the downloaded data.\n\nfor (y in 2:16) {\n  if (y == 16) {\n    # Extracting the data of year y (only for the last year)\n    # last digits of the year\n    fn_digits_year &lt;- ifelse(y &lt; 10, str_c(\"0\", y - 1), y - 1)\n    \n    data_P  &lt;- import_monthly_regional_values_P(\n      file = str_c(\n        \"../data/raw/minagri/Production/Production_20\", \n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 1,\n      timescale = 1\n    )\n    data_Px &lt;- import_monthly_regional_values_Px(\n      file = str_c(\n        \"../data/raw/minagri/Prices/Prices_20\", \n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 1,\n      timescale = 1\n    )\n    data_S  &lt;- import_monthly_regional_values_S(\n      file = str_c(\n        \"../data/raw/minagri/Surface/Superficies_20\", \n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 1, timescale = 1\n    )\n    data_SR &lt;- import_monthly_regional_values_SR(\n      file = str_c(\n        \"../data/raw/minagri/Surface_R/Superficies_R_20\", \n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 1, \n      timescale = 1\n    )\n    \n    data_P_annual  &lt;- import_monthly_regional_values_P(\n      file = str_c(\n        \"../data/raw/minagri/Production/Production_20\",\n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 1,\n      timescale = 12\n    )\n    data_S_annual  &lt;- import_monthly_regional_values_S(\n      file = str_c(\n        \"../data/raw/minagri/Surface/Superficies_20\",\n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 1,\n      timescale = 12\n    )\n    data_SR_annual &lt;- import_monthly_regional_values_SR(\n      file = str_c(\n        \"../data/raw/minagri/Surface_R/Superficies_R_20\",\n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 1,\n      timescale = 12\n    )\n  } else {\n    # Extracting the data of year y-1\n    fn_digits_year &lt;- str_pad(y, width = 2, side = \"left\", pad = 0)\n    data_P  &lt;- import_monthly_regional_values_P(\n      file = str_c(\n        \"../data/raw/minagri/Production/Production_20\", \n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 0,\n      timescale = 1\n    )\n    data_S  &lt;- import_monthly_regional_values_S(\n      file = str_c(\n        \"../data/raw/minagri/Surface/Superficies_20\", \n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 0,\n      timescale = 1\n    )\n    data_SR &lt;- import_monthly_regional_values_SR(\n      file = str_c(\n        \"../data/raw/minagri/Surface_R/Superficies_R_20\",\n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 0, \n      timescale = 1\n    )\n    data_Px &lt;- import_monthly_regional_values_Px(\n      file = str_c(\n        \"../data/raw/minagri/Prices/Prices_20\",\n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 0,\n      timescale = 1\n    )\n    \n    data_P_TRIM &lt;- import_monthly_regional_values_P(\n      file = str_c(\n        \"../data/raw/minagri/Production/Production_20\",\n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 0,\n      timescale = 4\n    )\n    data_P_annual  &lt;- import_monthly_regional_values_P(\n      file = str_c(\n        \"../data/raw/minagri/Production/Production_20\",\n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 0,\n      timescale = 12\n    )\n    data_S_annual  &lt;- import_monthly_regional_values_S(\n      file = str_c(\n        \"../data/raw/minagri/Surface/Superficies_20\",\n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 0,\n      timescale = 12\n    )\n    data_SR_annual &lt;- import_monthly_regional_values_SR(\n      file = str_c(\n        \"../data/raw/minagri/Surface_R/Superficies_R_20\",\n        fn_digits_year, \".xlsx\"\n      ),\n      anneesup = 0,\n      timescale = 12\n    )\n  }\n  \n  print(str_c(y,\" ok\"))\n  \n  data_P &lt;-\n    data_P |&gt;\n    mutate(date = lubridate::ymd(str_c(year, month, \"01\", sep = \"-\")))\n  colnames(data_P) &lt;- c(\n    \"region\", \"year\", \"product\", \"month\", \"Value_prod\", \"date\"\n  )\n\n  data_S &lt;-\n    data_S |&gt;\n    mutate(date = lubridate::ymd(str_c(year, month, \"01\", sep = \"-\")))\n  colnames(data_S) &lt;- c(\n    \"region\", \"campaign\", \"month\", \"product\", \"year\",\"Value_surf\", \"date\"\n  )\n  \n  data_SR &lt;-\n    data_SR |&gt;\n    mutate(date = lubridate::ymd(str_c(year, month, \"01\", sep = \"-\")))\n  colnames(data_SR) &lt;- c(\n    \"region\", \"year\", \"product\", \"month\", \"Value_surfR\", \"date\"\n  )\n  \n  data_Px &lt;-\n    data_Px |&gt;\n    mutate(date = lubridate::ymd(str_c(year, month, \"01\", sep = \"-\")))\n  colnames(data_Px) &lt;- c(\n    \"region\", \"year\", \"month\", \"product\", \"Value_prices\", \"date\"\n  )\n  \n  # # Adding the new year to the global mensual files\n  y_ix &lt;- y - 1 \n  data_P_TOTAL[[y_ix]]  &lt;- data_P\n  data_S_TOTAL[[y_ix]]  &lt;- data_S\n  data_SR_TOTAL[[y_ix]] &lt;- data_SR\n  data_Px_TOTAL[[y_ix]] &lt;- data_Px\n  \n  rm(data_P, data_S, data_SR, data_Px)\n} # End of Loop 1\n\nAll the year-elements in a single tibble:\n\ndata_P_TOTAL    &lt;- list_rbind(data_P_TOTAL)\ndata_S_TOTAL    &lt;- list_rbind(data_S_TOTAL)\ndata_SR_TOTAL   &lt;- list_rbind(data_SR_TOTAL)\ndata_Px_TOTAL   &lt;- list_rbind(data_Px_TOTAL)\n\n\n2.3.1 Campaign Data\nWe add the agricultural campaign data to the Planted surfaces data.\n\ndata_S_TOTAL &lt;- data_S_TOTAL |&gt; \n  mutate(\n    campaign = as.numeric(str_sub(campaign, 3, 4)),\n    campaign_plain = str_c(campaign,\"/\",campaign + 1)) |&gt; \n  unique() |&gt; \n  # Retrieving the campaign month (starting in August)\n  mutate(\n    month_campaign = case_when(\n      month == 1  ~ 6,\n      month == 2  ~ 7,\n      month == 3  ~ 8,\n      month == 4  ~ 9,\n      month == 5  ~ 10,\n      month == 6  ~ 11,\n      month == 7  ~ 12,\n      month == 8  ~ 1,\n      month == 9  ~ 2,\n      month == 10 ~ 3,\n      month == 11 ~ 4,\n      month == 12 ~ 5\n    )\n  ) |&gt; \n  mutate(\n    product = ifelse(\n      product == \"maíz duro\",\n      yes =  \"maíz amarillo duro\",\n      no = product\n    )\n  ) |&gt; \n  group_by(region, product) |&gt; \n  # Harmonizing the cumulative values of surface (if the lead and the lag are equals)\n  mutate(\n    Value_surf = ifelse(\n      lead(Value_surf) == lag(Value_surf, default = 0) & \n        Value_surf &lt; lag(Value_surf, default = 0),\n      yes = lag(Value_surf),\n      no = Value_surf\n    )\n  ) |&gt; \n  filter(! region == \"Total Nacional\")\n\nThen, we check whether there are some errors:\n\n# Checking for errors  \ndup_surf &lt;- data_S_TOTAL |&gt;\n  filter(! product == \"total\") |&gt; \n  filter(! product == \"TOTAL\") |&gt; \n  group_by(region, date, product) |&gt; \n  mutate(n = n()) |&gt; \n  filter(n &gt; 1) |&gt; \n  select(date) |&gt; \n  unique()\n\n\n\n2.3.2 Growth Duration\nLet us determine the growth duration and the corresponding lags from the agricultural calendars.\n\n2.3.2.1 Calendar\nThe Excel files with the agricultural calendar for specific crops are in the data folder, within the Calendario agricola sub-folder. Note: soya seems to be missing.\n\nN &lt;- list.files(\n  path = \"../data/raw/Calendario agricola/\", \n  pattern = \"xls$\",\n  full.names = TRUE\n)\n\nLet us define a small function that imports a specific calendar.\n\n#' Import calendar from the Excel files\n#' \n#' @param x full path to a Calendar (Excel file)\nimport_calendar &lt;- function(x) {\n  region &lt;- str_extract(x, \"//cal_(.*)\\\\.xls\") |&gt; \n    str_remove(\"//cal_\") |&gt; \n    str_remove(\"\\\\.xls\")\n  \n  df_cal_1 &lt;- read_excel(x)\n  \n  row_prod_mes &lt;- str_which(\n    df_cal_1$`CALENDARIO AGRICOLA NACIONAL`, \"Producto/Mes\"\n  )\n  ind_first &lt;- first(row_prod_mes)\n  ind_last &lt;- last(row_prod_mes)\n  \n  df_cal_planting &lt;- read_excel(\n    x, \n    skip = ind_first, \n    n_max = ind_last-ind_first - 3\n  )\n  df_cal_planting &lt;- \n    df_cal_planting |&gt; \n    pivot_longer(\n      cols = -`Producto/Mes`,\n      names_to = \"month_spanish\",\n      values_to = \"pct\"\n    ) |&gt; \n    mutate(region = region, period = \"planting\")\n  \n  df_cal_harvest &lt;- read_excel(x, skip = ind_last)\n  df_cal_harvest &lt;- \n    df_cal_harvest |&gt; \n    filter(!is.na(`Producto/Mes`)) |&gt; \n    mutate(across(-`Producto/Mes`, ~as.numeric(.))) |&gt; \n    pivot_longer(\n      cols = -`Producto/Mes`,\n      names_to = \"month_spanish\",\n      values_to = \"pct\"\n    ) |&gt; \n    mutate(region = region, period = \"harvest\")\n  \n  df_cal_planting |&gt; \n    bind_rows(df_cal_harvest)\n}\n\nAll the calendars can then be imported:\n\ncalendar &lt;- map(N, import_calendar) |&gt; \n  list_rbind()\n\nThe dates are written using Spanish month names. Let us create a table with the corresponding month numbers.\n\ncalendar &lt;- \n  calendar |&gt; \n  filter(month_spanish != \"...14\")\n\nspanish_months &lt;- \n  tibble(\n    month_spanish = c(\n      \"Ene\", \"Feb\", \"Mar\", \"Abr\", \"May\", \"Jun\",\n      \"Jul\", \"Ago\", \"Set\", \"Oct\", \"Nov\", \"Dic\"\n    ),\n    month = 1:12\n  )\n\nThe corresponding month numbers can then be associated with the data in the calendars.\n\ncalendar &lt;- \n  calendar |&gt; \n  left_join(spanish_months) |&gt; \n  select(-month_spanish)\n\nSome renaming for the regions:\n\ncalendar &lt;- calendar |&gt; \n  mutate(\n    region = ifelse(region == \"lalibertad\", \"la libertad\", region),\n    region = ifelse(region == \"madrededios\", \"madre de dios\", region),\n    region = ifelse(region == \"sanmartin\", \"san martin\", region)\n  ) |&gt; \n  mutate(region = str_to_upper(region))\n\nWhen no data is filled in the Excel file, we assume it corresponds to no production.\n\ncalendar &lt;- \n  calendar |&gt; \n  rename(product = `Producto/Mes`) |&gt; \n  mutate(pct = ifelse(is.na(pct), 0, pct))\n\nThe calendar for planting dates:\n\ncalendar1 &lt;- calendar |&gt; \n  filter(period == \"planting\") |&gt; \n  group_by(region, product) |&gt; \n  mutate(val_max = max(pct)) |&gt; \n  ungroup() |&gt; \n  slice(which(pct == val_max)) |&gt; \n  select(-val_max) \n\nFor the harvest season:\n\ncalendar2 &lt;- calendar |&gt; \n  filter(period == \"harvest\") |&gt; \n  group_by(region, product) |&gt; \n  mutate(val_max = max(pct)) |&gt; \n  ungroup() |&gt; \n  slice(which(pct == val_max)) |&gt; \n  slice(-2) |&gt; \n  slice(-163) |&gt; \n  select(-val_max) |&gt; \n  full_join(calendar1, by = c(\"region\",\"product\")) |&gt; \n  mutate(\n    growth_duration = month.x - month.y,\n    growth_duration = ifelse(\n      growth_duration &lt; 0, \n      yes = month.x + 13 - month.y, \n      no = growth_duration\n    ),\n    growth_duration = ifelse(growth_duration == 0,12, growth_duration)\n  ) |&gt; \n  slice(-which(is.na(growth_duration))) |&gt; \n  mutate(\n    region = toupper(iconv(region, to = \"ASCII//TRANSLIT\")),\n    product = toupper(product)\n  )\n\n\ncalendar3 &lt;- calendar |&gt; \n  filter(period == \"harvest\") |&gt; \n  mutate(month =   str_c(\"month\", month)) |&gt; \n  pivot_wider(names_from = month, values_from = pct) |&gt; \n  mutate(\n    cum_sum1  = month1, \n    cum_sum2  = month2 + cum_sum1, \n    cum_sum3  = month3 + cum_sum2, \n    cum_sum4  = month4 + cum_sum3, \n    cum_sum5  = month5 + cum_sum4, \n    cum_sum6  = month6 + cum_sum5, \n    cum_sum7  = month7 + cum_sum6, \n    cum_sum8  = month8 + cum_sum7, \n    cum_sum9  = month9 + cum_sum8, \n    cum_sum10 = month10 + cum_sum9, \n    cum_sum11 = month11 + cum_sum10, \n    cum_sum12 = month12 + cum_sum11) |&gt; \n  select(\n    product, region,\n    cum_sum1, cum_sum2, cum_sum3, cum_sum4,cum_sum5, cum_sum6,\n    cum_sum7, cum_sum8, cum_sum9, cum_sum10, cum_sum11, cum_sum12\n  ) |&gt; \n  pivot_longer(\n    cols = c(\n      cum_sum1, cum_sum2, cum_sum3, cum_sum4,cum_sum5, cum_sum6,\n      cum_sum7, cum_sum8, cum_sum9, cum_sum10, cum_sum11, cum_sum12\n    ), \n    names_to = \"month\"\n  ) |&gt; \n  rename(perc_cum_harv = value) |&gt; \n  mutate(\n    month = case_when(\n      month == \"cum_sum1\"  ~ 1, \n      month == \"cum_sum2\"  ~ 2, \n      month == \"cum_sum3\"  ~ 3, \n      month == \"cum_sum4\"  ~ 4, \n      month == \"cum_sum5\"  ~ 5, \n      month == \"cum_sum6\"  ~ 6, \n      month == \"cum_sum7\"  ~ 7, \n      month == \"cum_sum8\"  ~ 8, \n      month == \"cum_sum9\"  ~ 9, \n      month == \"cum_sum10\" ~ 10, \n      month == \"cum_sum11\" ~ 11, \n      month == \"cum_sum12\" ~ 12, \n    )\n  )\n\nThe planting season:\n\ncalendar4 &lt;- calendar |&gt; \n  filter(period == \"planting\") |&gt; \n  mutate(month =   str_c(\"month\", month)) |&gt; \n  pivot_wider(names_from = month, values_from = pct) |&gt; \n  mutate(\n    cum_sum1  = month8, \n    cum_sum2  = month9 + cum_sum1, \n    cum_sum3  = month10 + cum_sum2, \n    cum_sum4  = month11 + cum_sum3, \n    cum_sum5  = month12 + cum_sum4, \n    cum_sum6  = month1 + cum_sum5, \n    cum_sum7  = month2 + cum_sum6, \n    cum_sum8  = month3 + cum_sum7, \n    cum_sum9  = month4 + cum_sum8, \n    cum_sum10 = month5 + cum_sum9, \n    cum_sum11 = month6 + cum_sum10, \n    cum_sum12 = month7 + cum_sum11\n  ) |&gt; \n  select(\n    product, region, \n    cum_sum1, cum_sum2, cum_sum3, cum_sum4,cum_sum5, cum_sum6, \n    cum_sum7, cum_sum8, cum_sum9, cum_sum10, cum_sum11, cum_sum12\n  ) |&gt; \n  pivot_longer(\n    cols = c(\n      cum_sum1, cum_sum2, cum_sum3, cum_sum4,cum_sum5, cum_sum6, \n      cum_sum7, cum_sum8, cum_sum9, cum_sum10, cum_sum11, cum_sum12\n    ), names_to = \"month\") |&gt; \n  rename(perc_cum_plan = value) |&gt; \n  mutate(\n    month = case_when(\n      month == \"cum_sum1\"  ~ 8, \n      month == \"cum_sum2\"  ~ 9, \n      month == \"cum_sum3\"  ~ 10, \n      month == \"cum_sum4\"  ~ 11, \n      month == \"cum_sum5\"  ~ 12, \n      month == \"cum_sum6\"  ~ 1, \n      month == \"cum_sum7\"  ~ 2, \n      month == \"cum_sum8\"  ~ 3, \n      month == \"cum_sum9\"  ~ 4, \n      month == \"cum_sum10\" ~ 5, \n      month == \"cum_sum11\" ~ 6, \n      month == \"cum_sum12\" ~ 7, \n    )\n  )\n\nWe save those calendars\n\nsave(calendar, file = \"../data/output/Calendario agricola/calendar.rda\")\nsave(calendar2, file = \"../data/output/Calendario agricola/calendar2.rda\")\nsave(calendar3, file = \"../data/output/Calendario agricola/calendar3.rda\")\nsave(calendar4, file = \"../data/output/Calendario agricola/calendar4.rda\")\n\n\ncalendar2 &lt;- calendar2 |&gt; \n  mutate(\n    region = str_replace_all(region, \"á\", \"a\"),\n    region = str_replace_all(region, \"í\", \"i\"),\n    region = str_replace_all(region, \"é\", \"e\"),\n    region = str_replace_all(region, \"ó\", \"o\"),\n    region = str_replace_all(region, \"ú\", \"u\"),\n    region = str_replace_all(region, \"ñ\", \"n\")\n  ) |&gt; \n  mutate(\n    region = toupper(region), \n    product = toupper(product),\n    product = ifelse(str_detect(product, \"ARROZ\"), \"ARROZ CÁSCARA\", product)\n  )\ncalendar2\n\n# A tibble: 142 × 9\n   product  pct.x region period.x month.x pct.y period.y month.y growth_duration\n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;           &lt;dbl&gt;\n 1 ARROZ C…  15.5 AMAZO… harvest        8  17.3 planting       3               5\n 2 FRIJOL …  24.6 AMAZO… harvest        7  31.5 planting      11               9\n 3 MAÍZ AM…  15.4 AMAZO… harvest        6  15.7 planting       2               4\n 4 MAÍZ AM…  20.9 AMAZO… harvest        8  27.1 planting      11              10\n 5 PAPA      11.8 AMAZO… harvest        4  12.8 planting      11               6\n 6 TRIGO     27.5 AMAZO… harvest        8  22.2 planting       3               5\n 7 YUCA      13.3 AMAZO… harvest       11  14.4 planting      11              12\n 8 ALGODÓN   19.2 ANCASH harvest        3  25.5 planting       8               8\n 9 ARROZ C…  25.4 ANCASH harvest        3  32.5 planting      11               5\n10 CEBADA …  62.2 ANCASH harvest        7  44.6 planting       1               6\n# ℹ 132 more rows\n\n\n\ndata_S_TOTAL &lt;- data_S_TOTAL |&gt;\n  group_by(region, product, campaign) |&gt;\n  arrange(region, product, campaign, month_campaign) |&gt; \n  mutate(\n    surf_m = case_when(\n      month == \"8\" ~ Value_surf,\n      month != \"8\" ~ Value_surf - lag(Value_surf))\n  ) |&gt;\n  ungroup() |&gt;  \n  group_by(region, product) |&gt;\n  mutate(id = row_number()) |&gt; \n  ungroup() |&gt; \n  mutate(\n    region = str_replace_all(region, \"á\", \"a\"),\n    region = str_replace_all(region, \"í\", \"i\"),\n    region = str_replace_all(region, \"é\", \"e\"),\n    region = str_replace_all(region, \"ó\", \"o\"),\n    region = str_replace_all(region, \"ú\", \"u\"),\n    region = str_replace_all(region, \"ñ\", \"n\")\n  ) |&gt; \n  mutate(\n    region = toupper(iconv(region, to = \"ASCII//TRANSLIT\")),\n    product = toupper(product)\n  ) |&gt;\n  left_join(calendar2, by = c(\"region\",\"product\"))\n\nLet us add a variable with the lagged surface:\n\ndata_S_TOTAL &lt;- \n  data_S_TOTAL |&gt; \n  arrange(region, product, id) |&gt; \n  group_by(region, product) |&gt;\n  mutate(\n    surf_lag_calend = case_when(\n      growth_duration == 1  & !is.na(lag(surf_m, 1))  ~ lag(surf_m, 1),\n      growth_duration == 2  & !is.na(lag(surf_m, 2))  ~ lag(surf_m, 2),\n      growth_duration == 3  & !is.na(lag(surf_m, 3))  ~ lag(surf_m, 3),\n      growth_duration == 4  & !is.na(lag(surf_m, 4))  ~ lag(surf_m, 4),\n      growth_duration == 5  & !is.na(lag(surf_m, 5))  ~ lag(surf_m, 5),\n      growth_duration == 6  & !is.na(lag(surf_m, 6))  ~ lag(surf_m, 6),\n      growth_duration == 7  & !is.na(lag(surf_m, 7))  ~ lag(surf_m, 7),\n      growth_duration == 8  & !is.na(lag(surf_m, 8))  ~ lag(surf_m, 8),\n      growth_duration == 9  & !is.na(lag(surf_m, 9))  ~ lag(surf_m, 9),\n      growth_duration == 10 & !is.na(lag(surf_m, 10)) ~ lag(surf_m, 10),\n      growth_duration == 11 & !is.na(lag(surf_m, 11)) ~ lag(surf_m, 11),\n      growth_duration == 12 & !is.na(lag(surf_m, 12)) ~ lag(surf_m, 12),\n      TRUE ~ NA\n    )\n  )\n\nHarmonizing the data:\n\ndata_S_TOTAL &lt;- data_S_TOTAL |&gt;  \n  mutate(\n    surf_lag_calend = ifelse(id &lt;= growth_duration, NA, surf_lag_calend)\n  ) |&gt; \n  filter(! product %in% c(\"TOTAL\", \"total\")) |&gt; \n  rename(\n    \"gr_duration_calend\" = \"growth_duration\", \n    \"month_plan_calend\" = \"month.y\", \n    \"month_harv_calend\" = \"month.x\"\n  ) |&gt; \n  select(- period.x, - pct.x, -pct.y, -period.y)\n\nrm(calendar2)\n\n\n\n\n2.3.3 Saving the raw data\nWe can save the raw data:\n\nsave(data_P_TOTAL, file = \"../data/output/minagri/data_P_TOTAL.rda\")\nsave(data_S_TOTAL, file = \"../data/output/minagri/data_S_TOTAL.rda\")\nsave(data_SR_TOTAL, file = \"../data/output/minagri/data_SR_TOTAL.rda\")\nsave(data_Px_TOTAL, file = \"../data/output/minagri/data_Px_TOTAL.rda\")",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Agricultural Data</span>"
    ]
  },
  {
    "objectID": "data-agriculture.html#aggregation-of-agricultural-datasets",
    "href": "data-agriculture.html#aggregation-of-agricultural-datasets",
    "title": "2  Agricultural Data",
    "section": "2.4 Aggregation of Agricultural Datasets",
    "text": "2.4 Aggregation of Agricultural Datasets\nWe can load the datasets obtained previously:\n\nload(\"../data/output/minagri/data_P_TOTAL.rda\")\nload(\"../data/output/minagri/data_S_TOTAL.rda\")\nload(\"../data/output/minagri/data_SR_TOTAL.rda\")\nload(\"../data/output/minagri/data_Px_TOTAL.rda\")\n\n\n2.4.1 Harmonization\nHarmonizing the production, surfaces and prices datasets.\nProduction prices:\n\ndata_Px_TOTAL &lt;- data_Px_TOTAL |&gt; \n  mutate(\n    region = str_replace_all(region, \"á\", \"a\"),\n    region = str_replace_all(region, \"í\", \"i\"),\n    region = str_replace_all(region, \"é\", \"e\"),\n    region = str_replace_all(region, \"ó\", \"o\"),\n    region = str_replace_all(region, \"ú\", \"u\"),\n    region = str_replace_all(region, \"ñ\", \"n\")\n  ) |&gt; \n  mutate(region = toupper(iconv(region, to = \"ASCII//TRANSLIT\"))) |&gt; \n  mutate(\n    region = toupper(region), \n    product = toupper(product), \n    product = ifelse(\n      product == \"FRIJOL GRANO SECO**\",\n      yes = \"FRIJOL GRANO SECO\", \n      no = product\n    )\n  )\n\nHarvested surface:\n\ndata_SR_TOTAL &lt;- data_SR_TOTAL |&gt; \n  mutate(\n    region = str_replace_all(region, \"á\", \"a\"),\n    region = str_replace_all(region, \"í\", \"i\"),\n    region = str_replace_all(region, \"é\", \"e\"),\n    region = str_replace_all(region, \"ó\", \"o\"),\n    region = str_replace_all(region, \"ú\", \"u\"),\n    region = str_replace_all(region, \"ñ\", \"n\")\n  ) |&gt; \n  mutate(region = toupper(iconv(region, to = \"ASCII//TRANSLIT\"))) |&gt; \n  mutate(region = toupper(region), \n         product = toupper(product), \n         product = ifelse(\n           product == \"FRIJOL GRANO SECO**\",\n           yes = \"FRIJOL GRANO SECO\",\n           no = product)\n  )\n\n\n\n2.4.2 Aggregation in a single tibble\n\ndata &lt;- data_P_TOTAL |&gt; \n  mutate(\n    region = str_replace_all(region, \"á\", \"a\"),\n    region = str_replace_all(region, \"í\", \"i\"),\n    region = str_replace_all(region, \"é\", \"e\"),\n    region = str_replace_all(region, \"ó\", \"o\"),\n    region = str_replace_all(region, \"ú\", \"u\"),\n    region = str_replace_all(region, \"ñ\", \"n\")) |&gt; \n  mutate(region = toupper(iconv(region, to = \"ASCII//TRANSLIT\"))\n  ) |&gt; \n  mutate(\n    region = toupper(region), \n    product = toupper(product), \n    month = as.numeric(month)\n  ) |&gt; \n  full_join(\n    data_S_TOTAL |&gt; \n      select(-c(year,month)), \n    by = c(\"region\",\"product\",\"date\")\n  ) |&gt;  \n  filter(! is.na(year)) |&gt; \n  # No consequences on the aggregation because the missing data are not in the\n  # product sample used hereafter. (except for Lima but without production value)\n  full_join(\n    data_Px_TOTAL |&gt; \n      select(-c(year,month)), \n    by = c(\"region\",\"product\",\"date\")\n  ) |&gt;  \n  filter(product %in% c(\n    \"PAPA\", \"CEBADA GRANO\", \"MAÍZ AMARILLO DURO\", \"MAÍZ AMILÁCEO\",\n    \"ARROZ CÁSCARA\", \"SORGO GRANO\", \"YUCA\", \"TRIGO\")\n  ) |&gt; \n  select(\n    region, product, year, month, date, \n    Value_prod, surf_m, surf_lag_calend, Value_prices, \n    campaign, month_campaign, campaign_plain, id\n  ) |&gt; \n\n  full_join(\n    data_SR_TOTAL |&gt; \n      select(- month, - year), \n    by = c(\"region\",\"product\",\"date\")\n  ) |&gt;\n  filter(product %in% c(\n    \"PAPA\", \"CEBADA GRANO\", \"MAÍZ AMARILLO DURO\", \"MAÍZ AMILÁCEO\",\n    \"ARROZ CÁSCARA\", \"SORGO GRANO\", \"YUCA\", \"TRIGO\")\n  ) |&gt;\n  filter(! region %in% c(\n    \"Lima Metropolitana\", \"Callao\", \"LIMA METROPOLITANA\",\n    \"CALLAO\", \"PROMEDIO NACIONAL\")\n  ) |&gt;\n  filter(! is.na(region)) |&gt; \n  filter(! product %in% c(\"SORGO GRANO\", \"CEBADA GRANO\"))\n\nSome values are missing. Let us deal with those.\n\nmissing_values &lt;- readxl::read_excel(\n  path = \"../data/raw/Macro/Datos_INEI1.xlsx\",\n  sheet = \"SELECTED2\",\n  col_types = \"text\"\n) |&gt; \n  mutate(\n    region = str_replace_all(region, \"á\", \"a\"),\n    region = str_replace_all(region, \"í\", \"i\"),\n    region = str_replace_all(region, \"é\", \"e\"),\n    region = str_replace_all(region, \"ó\", \"o\"),\n    region = str_replace_all(region, \"ú\", \"u\"),\n    region = str_replace_all(region, \"ñ\", \"n\")\n  ) |&gt; \n  mutate(\n    region = toupper(iconv(region, to = \"ASCII//TRANSLIT\")),\n    region = toupper(region), \n    product = toupper(product)\n  ) |&gt; \n  select(-Indicador) |&gt; \n  pivot_longer(cols = c(NOV,DEC), names_to = \"name_month\") |&gt; \n  mutate(\n    year = 2008, \n    month = ifelse(name_month == \"NOV\", 11, 12)\n  ) |&gt; \n  select(-name_month)\n\n\ndata_agri &lt;- data |&gt; \n  left_join(\n    missing_values,\n    by = c(\"region\" ,\"product\" ,\"month\",\"year\")\n  ) |&gt; \n  mutate(\n    Value_prod = ifelse(\n      # The price data in Nov. 2008 are the same as that of Oct...\n      year == 2008 & month %in% c(11,12),\n      yes = as.numeric(value), \n      no = Value_prod\n    )\n  ) |&gt; \n  select(-value) |&gt; \n  select(-id) |&gt; \n  relocate(\n    region, product, year, month, date, \n    Value_prod, surf_m,\n    Value_surfR, Value_prices, \n    campaign, campaign_plain, month_campaign\n  ) |&gt; \n  filter(\n    ! is.na(year), \n    ! region %in% c(\n      \"TOTAL NACIONAL\", \"CALLAO\", \"LIMA METROPOLITANA\",\n      \"PROMEDIO NACIONAL\", \"LIMA PROVINCIAS\"\n    )\n  )\nrm(data)\n\nLet us save that table:\n\nsave(data_agri, file = \"../data/output/minagri/data_agri.rda\")",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Agricultural Data</span>"
    ]
  },
  {
    "objectID": "data-agriculture.html#final-aggregation",
    "href": "data-agriculture.html#final-aggregation",
    "title": "2  Agricultural Data",
    "section": "2.5 Final Aggregation",
    "text": "2.5 Final Aggregation\n\nload(\"../data/output/minagri/data_agri.rda\")\n\n\nlength(unique(data_agri$region)) * \n  length(unique(data_agri$product)) *\n  length(unique(data_agri$date))\n\n[1] 25920\n\n\nLet us fill the table if some rows are missing for the triplet region x product x date:\n\ndata_agri &lt;- \n  data_agri |&gt; \n  complete(region, product, date)\n\nAre there any duplicated triplet?\n\ndata_agri |&gt; \n  group_by(region, product, date) |&gt; \n  count() |&gt; \n  filter(n&gt;1)\n\n# A tibble: 0 × 4\n# Groups:   region, product, date [0]\n# ℹ 4 variables: region &lt;chr&gt;, product &lt;chr&gt;, date &lt;date&gt;, n &lt;int&gt;\n\n\n\ndata_total &lt;-  \n  data_agri |&gt; \n  unique() |&gt; \n  filter(! region == \"TOTAL NACIONAL\") |&gt; \n    # Adding the english names of the selected crops\n  mutate(\n    product_eng = case_when(\n      product == \"ARROZ CÁSCARA\"      ~ \"Rice\",\n      product == \"MAÍZ AMARILLO DURO\" ~ \"Dent corn\",\n      product == \"MAÍZ AMILÁCEO\"      ~ \"Amylaceous corn\", \n      product == \"PAPA\"               ~ \"Potato\", \n      product == \"TRIGO\"              ~ \"Wheat\", \n      product == \"YUCA\"               ~ \"Cassava\", \n      TRUE ~ product)\n  ) |&gt; \n  group_by(region, product, year) |&gt; \n  # Computing the share of the annual production harvested at month m \n  mutate(\n    perc_product = ifelse(Value_prod == 0, NA, Value_prod) / \n      sum( ifelse(Value_prod == 0, NA, Value_prod), na.rm = T)\n  ) |&gt; \n  ungroup() |&gt; \n  group_by(region, product, month) |&gt; \n  # Computing the average share of the annual production harvested at month m\n  mutate(perc_product_mean = mean(perc_product, na.rm = T)) |&gt; \n   ungroup()\n\n\ndata_total &lt;- data_total |&gt;\n  arrange(region, product, date) |&gt; \n  group_by(region) |&gt; \n  mutate(region_id = cur_group_id()) |&gt; \n  ungroup() |&gt; \n  # Computing the log of the quantities and prices\n  mutate(\n    ln_prices = log(Value_prices + 1), \n    ln_produc = log(Value_prod + 1)\n  ) |&gt; \n  relocate(region_id, region, product, date, ln_prices, ln_produc)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `ln_produc = log(Value_prod + 1)`.\nCaused by warning in `log()`:\n! NaNs produced\n\n\nLet us determine the start of the harvest season:\n\ndata_total &lt;- \n  data_total |&gt; \n  arrange(region, product_eng, date) |&gt; \n  # Computing the difference between planted and harvested surfaces at time t \n  mutate(diff_plant_harv = surf_m - Value_surfR) |&gt; \n  group_by(region, product_eng) |&gt; \n # Computing the cumulative difference and normalizing the detrended component\n  mutate(exposition = cumsum(replace_na(diff_plant_harv, 0))) |&gt; \n  mutate(exposition_trend = as.vector(\n    mFilter::hpfilter(\n      exposition, freq = 14400, type = \"lambda\", drift = FALSE\n    )$trend)\n  ) |&gt; \n  mutate(exposition_detrended = exposition - exposition_trend) |&gt; \n  mutate(\n    exposition_norm = (exposition_detrended - min(exposition_detrended)) / \n      (max(exposition_detrended) - min(exposition_detrended))\n  ) |&gt; \n  ungroup() \n\nWe add labels to each column:\n\ndata_total &lt;- \n  data_total |&gt; \n  labelled::set_variable_labels(\n    region_id = \"Region numerical ID\",\n    region = \"Name of the region\",\n    product = \"Name of the crop (in Spanish)\",\n    date = \"Date (YYYY-MM-DD)\",\n    ln_prices = \"Product price (log)\",\n    ln_produc = \"Production (log of tons)\",\n    year = \"Year (YYYY)\",\n    month = \"Month (MM)\",\n    Value_prod = \"Production (tons)\",\n    surf_m = \"Planted Surface during the current month (hectares)\",\n    surf_lag_calend = \"Planted Surface laggued by the growth duration computed from the caledars (hectares)\",\n    Value_surfR = \"Harvested Surface (hectares)\",\n    Value_prices = \"Unit Price (Pesos)\",\n    campaign = \"ID of the planting campaing (starting in August)\",\n    campaign_plain = \"Years of the planting campaing (starting in August)\",\n    month_campaign = \"Month of the planting campaing (August = 1)\",\n    product_eng = \"Name of the Product (in English)\",\n    perc_product = \"Share of the annual production harvested at month m\",\n    perc_product_mean = \"Average share of the annual production harvested at month m\",\n    diff_plant_harv = \"Difference between planted and harvested surfaces during month m\",\n    exposition = \"Cumulative difference between planted and harvested surfaces\",\n    exposition_trend = \"Trend of the exposition using HP filter\",\n    exposition_detrended = \"Difference between the exposition and its trend\",\n    exposition_norm = \"Normalisation of the detrended exposition\"\n  )\n\nAnd lastly, let us save the resulting data\n\nsave(data_total, file = \"../data/output/minagri/dataset_agri_2001_2015.rda\")\nwrite.csv(data_total, \"../data/output/minagri/dataset_agri_2001_2015.csv\")",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Agricultural Data</span>"
    ]
  },
  {
    "objectID": "data-agriculture.html#content-of-the-dataset",
    "href": "data-agriculture.html#content-of-the-dataset",
    "title": "2  Agricultural Data",
    "section": "2.6 Content of the dataset",
    "text": "2.6 Content of the dataset\n\n\n\nTable 2.1: Variables in the dataset_agri_2001_2015.rda file\n\n\n\n\n\n\n\n\n\n\nVariable name\nType\nDescription\n\n\n\n\nregion_id\ninteger\nRegion numerical ID\n\n\nregion\ncharacter\nName of the region\n\n\nproduct\ncharacter\nName of the crop (in Spanish)\n\n\ndate\nDate\nDate (YYYY-MM-DD)\n\n\nln_prices\nnumeric\nProduct price (log)\n\n\nln_produc\nnumeric\nProduction (log of tons)\n\n\nyear\nnumeric\nYear (YYYY)\n\n\nmonth\nnumeric\nMonth (MM)\n\n\nValue_prod\nnumeric\nProduction (tons)\n\n\nsurf_m\nnumeric\nPlanted Surface during the current month (hectares)\n\n\nValue_surfR\nnumeric\nHarvested Surface (hectares)\n\n\nValue_prices\nnumeric\nUnit Price (Pesos)\n\n\ncampaign\nnumeric\nID of the planting campaing (starting in August)\n\n\ncampaign_plain\ncharacter\nYears of the planting campaing (starting in August)\n\n\nmonth_campaign\nnumeric\nMonth of the planting campaing (August = 1)\n\n\nsurf_lag_calend\nnumeric\nPlanted Surface laggued by the growth duration computed from the caledars (hectares)\n\n\nproduct_eng\ncharacter\nName of the Product (in English)\n\n\nperc_product\nnumeric\nShare of the annual production harvested at month m\n\n\nperc_product_mean\nnumeric\nAverage share of the annual production harvested at month m\n\n\ndiff_plant_harv\nnumeric\nDifference between planted and harvested surfaces during month m\n\n\nexposition\nnumeric\nCumulative difference between planted and harvested surfaces\n\n\nexposition_trend\nnumeric\nTrend of the exposition using HP filter\n\n\nexposition_detrended\nnumeric\nDifference between the exposition and its trend\n\n\nexposition_norm\nnumeric\nNormalisation of the detrended exposition",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Agricultural Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html",
    "href": "data-macro.html",
    "title": "3  Macroeconomic Data",
    "section": "",
    "text": "3.1 Load Data\nWe have downloaded these series from the the Banco Central De Reserva Del Perú website.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(labelled)\nperu_macro &lt;- \n  readxl::read_excel(\n  path = \"../data/raw/Macro/data_macro_peru.xlsx\",\n  skip = 1,\n  na = \"n.d.\"\n) |&gt; \n  mutate(\n    month = case_when(\n      str_detect(date, \"^Ene\") ~ 1,\n      str_detect(date, \"^Feb\") ~ 2,\n      str_detect(date, \"^Mar\") ~ 3,\n      str_detect(date, \"^Abr\") ~ 4,\n      str_detect(date, \"^May\") ~ 5,\n      str_detect(date, \"^Jun\") ~ 6,\n      str_detect(date, \"^Jul\") ~ 7,\n      str_detect(date, \"^Ago\") ~ 8,\n      str_detect(date, \"^Sep\") ~ 9,\n      str_detect(date, \"^Oct\") ~ 10,\n      str_detect(date, \"^Nov\") ~ 11,\n      str_detect(date, \"^Dic\") ~ 12\n    ),\n    year = as.numeric(str_sub(date, -2)) + 2000\n  ) |&gt; \n  mutate(date = lubridate::ym(str_c(year, month, sep = \"-\")))\nLet us rename the columns :\nperu_macro &lt;- \n  peru_macro |&gt; \n  select(\n    date,\n    yy_raw  = PN01773AM, # GDP\n    rer_raw = PN01259PM, # Real Exchange Rate\n    x_raw   = PN01461BM, # Exports\n    r_raw   = PN07819NM, # Interest rate\n    pi_raw  = PN01270PM, # CPI\n    pia_raw = PN01336PM, # CPI: food\n    ya_raw  = PN01755AM, # Agricultural GDP\n    ind_prod_raw = PN02079AM # Manufacturing production\n  )",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#helper-functions",
    "href": "data-macro.html#helper-functions",
    "title": "3  Macroeconomic Data",
    "section": "3.2 Helper Functions",
    "text": "3.2 Helper Functions\nIn order to address any underlying trends in the GDP data, we express this variable in percentage deviations from the Hodrick-Prescott trend. To that end, we define a helper that returns the estimated trend:\n\n#' Applies HP filter for monthly data and returns the trend component\n#' \n#' @param x vector of monthly observations\n#' @param freq smoothing parameter (\\lambda)\nhp_filter_trend &lt;- function(x, freq = 14400) {\n  res_hp_filter &lt;- mFilter::hpfilter(\n    x,\n    freq = 14400, \n    type = \"lambda\", \n    drift = FALSE\n  )\n  as.vector(res_hp_filter$trend)\n}\n\nFurthermore, to eliminate any seasonal components present in the data, we employ the X13 method developed by the Census Bureau. This method enables us to remove seasonal variations and isolate the underlying patterns and dynamics of the variables\n\n#' Removes the seasonality of a vector of monthly values\n#'\n#' @param x vector of numerical values\n#' @param start_date start date of the values\n#' @param remove_trend should the estimated trend be removed? Default to ``FALSE\nadj_season_X13 &lt;- function(x,\n                           start_date,\n                           remove_trend = FALSE) {\n  x_ts &lt;- ts(x, start = c(year(start_date), month(start_date)), freq = 12)\n  # Seasonal Adjustment with X-13ARIMA-SEATS\n  x_ts_season &lt;- seasonal::seas(\n    x_ts, estimate.maxiter=10000,\n    arima.model = \"(0 1 1)(0 1 1)\",\n    x11 = \"\"\n  )\n  x_ts_season_df &lt;- timetk::tk_tbl(x_ts_season$data) |&gt;\n    mutate(date = lubridate::my(index))\n\n  df_resul &lt;- tibble(\n    date = seq(start_date, last(x_ts_season_df$date), by = \"month\")\n  ) |&gt;\n    left_join(x_ts_season_df, by = \"date\") |&gt;\n    mutate(val = seasonaladj)\n\n  if (remove_trend) {\n    df_resul &lt;- df_resul |&gt;\n      mutate(val = seasonaladj - trend)\n  }\n\n  df_resul |&gt; pull(val)\n}\n\n\n\n\n\n\n\nNote\n\n\n\nThe hp_filter_trend() and the adj_season_X13() functions are defined in the ../weatherperu/R/detrending.R script.",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#data-pre-processing",
    "href": "data-macro.html#data-pre-processing",
    "title": "3  Macroeconomic Data",
    "section": "3.3 Data Pre-Processing",
    "text": "3.3 Data Pre-Processing\nFor GDP:\n\nperu_gdp &lt;- \n  peru_macro |&gt; \n  select(date, yy_raw) |&gt; \n  filter(date &gt;= \"2003-01-01\") |&gt; \n  mutate(\n    # Remove seasonality\n    yy_sa  = adj_season_X13(yy_raw, ymd(\"2003-01-01\")),\n    # Extract trend\n    yy_trend = hp_filter_trend(yy_sa, freq = 14400),\n    # Percentage dev. from trend\n    y = 100 * log(yy_raw / yy_trend)\n  )\n\nFor agricultural GDP:\n\nperu_ya &lt;- \n  peru_macro |&gt; \n  select(date, ya_raw) |&gt; \n  filter(date &gt;= \"2003-01-01\") |&gt; \n  mutate(\n    # Remove seasonality\n    ya_sa  = adj_season_X13(ya_raw, ymd(\"2003-01-01\")),\n    # Extract trend\n    ya_trend = hp_filter_trend(ya_sa, freq = 14400),\n    # Percentage dev. from trend\n    ya = 100 * log(ya_raw / ya_trend)\n  )\n\nFor Real Exchange Rate:\n\nperu_rer &lt;- \n  peru_macro |&gt; \n  select(date, rer_raw) |&gt; \n  filter(date &gt;= \"2001-01-01\") |&gt; \n  mutate(\n    # Remove seasonality\n    rer_sa = adj_season_X13(rer_raw, ymd(\"2001-01-01\")),\n    rer = rer_sa / 100\n  ) |&gt; \n  mutate(\n    rer_hp_trend = hp_filter_trend(rer_raw, freq = 14400),\n    rer_hp = rer_raw - rer_hp_trend,\n    rer_trend = hp_filter_trend(rer_sa, freq = 14400),\n    rer_dt_sa = log(rer_sa / rer_trend)\n  ) |&gt; \n  select(-rer_hp_trend, -rer_trend, -rer_sa)\n\nFor Exports:\n\nperu_x &lt;- \n  peru_macro |&gt; \n  select(date, x_raw) |&gt; \n  filter(date &gt;= \"2001-01-01\") |&gt; \n  mutate(\n    # Remove seasonality\n    x = adj_season_X13(100 + x_raw, ymd(\"2000-12-01\")),\n    x = x / 100\n  )\n\nFor the Interest rate:\n\nperu_r &lt;- \n  peru_macro |&gt; \n  select(date, r_raw) |&gt; \n  filter(date &gt;= \"2001-01-01\") |&gt; \n  rename(r = r_raw) |&gt; \n  mutate(\n    r_hp_trend = hp_filter_trend(r, freq = 14400),\n    r_hp = r - r_hp_trend\n  ) |&gt; \n  select(-r_hp_trend)\n\nFor the Consumer Price Index:\n\nperu_cpi &lt;- \n  peru_macro |&gt; \n  select(date, pi_raw) |&gt; \n  mutate(\n    # Remove seasonality\n    pi_sa = adj_season_X13(pi_raw, ymd(\"2000-12-01\")),\n    # Log difference\n    pi = c(NA, 100 * diff(log(pi_sa)))\n  )\n\nFor the Food Consumer Price Index:\n\nperu_cpia &lt;- \n  peru_macro |&gt; \n  select(date, pia_raw) |&gt; \n  mutate(\n    # Remove seasonality\n    pia_sa = adj_season_X13(pia_raw, ymd(\"2000-12-01\")),\n    # Log difference\n    pia = c(NA, 100 * diff(log(pia_sa)))\n  )\n\nManufacturing production\n\nperu_ind_prod &lt;- \n  peru_macro |&gt; \n  select(date, ind_prod_raw) |&gt; \n  filter(date &gt;= \"2001-01-01\") |&gt; \n  mutate(\n    # Remove seasonality\n    ind_prod_sa  = adj_season_X13(ind_prod_raw, ymd(\"2001-01-01\")),\n    # Extract trend\n    ind_prod_trend = hp_filter_trend(ind_prod_sa, freq = 14400),\n    # Percentage dev. from trend\n    ind_prod = 100 * log(ind_prod_raw / ind_prod_trend)\n  )",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#sec-data-macro-commodity",
    "href": "data-macro.html#sec-data-macro-commodity",
    "title": "3  Macroeconomic Data",
    "section": "3.4 International Commodity prices",
    "text": "3.4 International Commodity prices\nFor the international commodity prices, we will not include those in the final macro data, as they are given depending on the crop. We will produce a separate file. We have downloaded these series from the IMF Primary Commodity Price System website.\n\nlibrary(readxl)\nint_prices &lt;- read_excel(\n  path = \"../data/raw/Macro/IMF_DATA.xls\",\n  sheet = \"SELECTED\",\n  col_types = \"text\") |&gt; \n  mutate(date = lubridate::ymd(str_c(YEAR,  \"-\", MONTH, \"-01\"))) |&gt; \n  rename(\n    \"FPI\"           = \"FPI_PFOOD\", \n    \"FERTILIZER\"    = \"FERTILIZER_PFERT\",   \n    \"IndexOIL\"      = \"IndexOIL_POILAPSP\",\n    \"PriceOIL\"      = \"PriceOIL_POILAPSP\", \n    \"CORN\"          = \"CORN_PMAIZMT\", \n    \"ARROZ CÁSCARA\" = \"RICE_PRICENPQ\", \n    \"TRIGO\"         = \"WHEAT_PWHEAMT\"\n  ) |&gt; \n  mutate(CORN2 = CORN, \n         FPI2 = FPI) |&gt; \n  select(-c(MONTH, YEAR, CPI_Peru_IMF, CPI_US_IMF, IndexOIL, PriceOIL)) |&gt; \n  pivot_longer(cols = !date, names_to = \"product\", values_to = \"price_int\") |&gt; \n  mutate(\n    product = case_when(\n      product == \"FPI\"  ~ \"PAPA\", \n      product == \"FPI2\" ~ \"YUCA\", \n      product == \"CORN\" ~ \"MAÍZ AMILÁCEO\", \n      product == \"CORN2\"~ \"MAÍZ AMARILLO DURO\", \n      TRUE ~ product\n    ), \n    product_eng = case_when(\n      product == \"PAPA\" ~ \"Potato\",\n      product == \"YUCA\" ~ \"Cassava\",\n      product == \"ARROZ CÁSCARA\" ~ \"Rice\",\n      product == \"MAÍZ AMARILLO DURO\" ~ \"Dent corn\"\n    ),\n    price_int = as.numeric(price_int)\n  ) |&gt; \n  arrange(product, date) |&gt; \n  group_by(product) |&gt; \n  mutate(price_int_inf = price_int / lag(price_int) - 1) |&gt; \n  ungroup() |&gt; \n  filter(! year(date) == 2000)\n\nLet us add labels to the columns:\n\nint_prices &lt;- \n  int_prices |&gt; \n  labelled::set_variable_labels(\n    date = \"Date\",\n    price_int = \"International commodity price\",\n    price_int_inf = \"Int. commodity inflation rate\"\n  )\n\nAnd save the resulting table:\n\nsave(int_prices, file = \"../data/output/macro/df_int_prices.rda\")",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#merging-the-data",
    "href": "data-macro.html#merging-the-data",
    "title": "3  Macroeconomic Data",
    "section": "3.5 Merging the Data",
    "text": "3.5 Merging the Data\nThe sample period for our analysis covers the time span from January 2003 (2003M1) to December 2015 (2015M12).\n\ndf_macro &lt;- \n  peru_gdp |&gt; \n  select(date, y) |&gt; \n  full_join(\n    peru_ya |&gt; select(date, ya),\n    by = \"date\"\n  ) |&gt; \n  full_join(\n    peru_rer |&gt; select(date, rer, rer_hp, rer_dt_sa),\n    by = \"date\"\n  ) |&gt; \n  full_join(\n    peru_r |&gt; select(date, r, r_hp),\n    by = \"date\"\n  ) |&gt; \n  full_join(\n    peru_x |&gt; select(date, x),\n    by = \"date\"\n  ) |&gt; \n  full_join(\n    peru_cpi |&gt; select(date, pi),\n    by = \"date\"\n  ) |&gt;\n  full_join(\n    peru_cpia |&gt; select(date, pia),\n    by = \"date\"\n  ) |&gt;\n  full_join(\n    peru_ind_prod |&gt; select(date, ind_prod),\n    by = \"date\"\n  ) |&gt; \n  arrange(date) |&gt; \n  filter(date &gt;= ymd(\"2001-01-01\"))\n\nLet us add labels to the columns:\n\ndf_macro &lt;- \n  df_macro |&gt; \n  labelled::set_variable_labels(\n    date = \"Date\",\n    rer_dt_sa = \"Real exchange rate\",\n    rer_hp = \"Real exchange rate\",\n    x  = \"Exports\",\n    pi = \"Inflation rate (pp)\",\n    pia = \"Food inflation rate (pp)\",\n    y = \"GDP (pp)\",\n    ya = \"Agricultural GDP (pp)\",\n    r = \"Interest rate (pp)\",\n    r_hp = \"Interest rate (pp)\",\n    ind_prod = \"Manufacturing Prod. (pp)\"\n  )\n\nAnd lastly, we need to save the data for further use (especially in Chapter 10).\n\nsave(df_macro, file = \"../data/output/macro/df_macro.rda\")",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#content-of-the-dataset",
    "href": "data-macro.html#content-of-the-dataset",
    "title": "3  Macroeconomic Data",
    "section": "3.6 Content of the dataset",
    "text": "3.6 Content of the dataset\n\n\n\nTable 3.1: Variables in the df_macro.rda file\n\n\n\n\n\n\n\n\n\n\nVariable name\nType\nDescription\n\n\n\n\ndate\ndate\nDate of the observation (YYYY-MM-DD)\n\n\ny\nnumeric\nGDP in percentage point, percentage deviation from trend, detrended and seasonally adjusted\n\n\nya\nnumeric\nAgricultural GDP in percentage point, percentage deviation from trend, detrended and seasonally adjusted\n\n\nrer_hp\nnumeric\nReal exchange rate, detrended using HP filter\n\n\nrer_dt_sa\nnumeric\nReal exchange rate, detrended and seasonally adjusted\n\n\nx\nnumeric\nExports, seasonally adjusted\n\n\nr\nnumeric\nInterest rate, in percentage point, detrended\n\n\npi\nnumeric\nInflation rate, in percentage point\n\n\npia\nnumeric\nFood inflation rate, in percentage point, seasonally adjusted\n\n\nind_prod\nnumeric\nManufacturing Production, in percentage point, percentage deviation from trend, detrended and seasonally adjusted\n\n\n\n\n\n\n\n\n\nTable 3.2: Variables in the int_prices.rda file\n\n\n\n\n\n\n\n\n\n\nVariable name\nType\nDescription\n\n\n\n\ndate\ndate\nDate of the observation (YYYY-MM-DD)\n\n\nproduct\ncharacter\nCrop name in Spanish\n\n\nprice_int\nnumeric\nInternational commodity prices\n\n\nprice_int_inf\nnumeric\nGrowth rate of international commodity prices",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-other.html",
    "href": "data-other.html",
    "title": "4  Other Data",
    "section": "",
    "text": "4.1 Natural Regions\nWe calculate the share of each type of natural region using data from the Geo GPS Peru website: https://www.geogpsperu.com/2019/11/mapa-de-regiones-naturales-costa-sierra.html\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(rmapshaper)\nlibrary(raster)\n\nLoading required package: sp\n\nAttaching package: 'raster'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(stars)\n\nLoading required package: abind\n\nlibrary(readxl)\n# install.packages(\"lwgeom\")",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Other Data</span>"
    ]
  },
  {
    "objectID": "data-other.html#sec-other-natural-regions",
    "href": "data-other.html#sec-other-natural-regions",
    "title": "4  Other Data",
    "section": "",
    "text": "4.1.1 Import Data\nWe first import the Peruvian map:\n\nsf::sf_use_s2(FALSE)\n\nSpherical geometry (s2) switched off\n\n# Map of the administrative regions \nmap_peru &lt;- sf::st_read(\"../data/raw/shapefile_peru/departamentos/\", quiet = T)\n\nmap_peru &lt;-\n  rmapshaper::ms_simplify(input = as(map_peru, 'Spatial')) |&gt;\n  st_as_sf()\n\nThen, we load the map with the definition of the natural regions:\n\n# Map of the natural regions \nmap_regiones_naturales &lt;-  sf::st_read(\n  str_c(\"../data/raw/shapefile_peru/regiones_naturales/\",\n        \"region natural_geogpsperu_JuanPabloSuyoPomalia.geojson\"\n  ),\n  quiet = T\n)\n\nAs the file contains a lot of unecessary details, we simplify the polygons:\n\nmap_regiones_naturales &lt;-\n  rmapshaper::ms_simplify(input = as(map_regiones_naturales, 'Spatial')) |&gt;\n  st_as_sf()\n\nLet us plot the map:\n\nggplot(data = map_regiones_naturales) +\n  geom_sf(mapping = aes(fill = Nm_RegNat))\n\n\n\n\n\n\n\nFigure 4.1: Map with the Natual Regions (Source: Geo GPS Peru)\n\n\n\n\n\n\n\n4.1.2 Share of Natural Region in Each Administrative Region\nWe calculate the share of each type of natural region in each administrative region.\n\nnatural_region_dep &lt;- vector(mode = \"list\", length = nrow(map_peru))\n\nLooping over each administrative region:\n\nfor (i in 1:nrow(map_peru)) {\n  natural_region_dep_i &lt;- \n    st_intersection(map_peru[i,],  map_regiones_naturales) |&gt; \n    dplyr::mutate(area_cell_intersect = st_area(geometry)) |&gt; \n    mutate(area_cell_sqkm = units::drop_units(area_cell_intersect)*10^-6) |&gt; \n    mutate(\n      area = sum(area_cell_sqkm),\n      area_pct = area_cell_sqkm/area) |&gt; \n    dplyr::select(region = DEPARTAMEN, natural_reg = Nm_RegNat, area_pct) |&gt; \n    as_tibble() |&gt; \n    dplyr::select(-geometry)\n  \n  natural_region_dep[[i]] &lt;- natural_region_dep_i\n}\n\nBinding the results:\n\nnatural_region_dep &lt;- \n  dplyr::bind_rows(natural_region_dep) |&gt; \n  dplyr::mutate(natural_reg = str_c(\"share_\", str_to_lower(natural_reg))) |&gt; \n  tidyr::pivot_wider(\n    names_from = natural_reg, \n    values_from = area_pct, \n    values_fill = 0\n  )\n\nnatural_region_dep |&gt;\n  dplyr::mutate(test = share_sierra + share_selva + share_costa)\n\n# A tibble: 25 × 5\n   region       share_sierra share_selva share_costa  test\n   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 AMAZONAS            0.169   0.831        0            1\n 2 ANCASH              0.709   0            0.291        1\n 3 APURIMAC            1.00    0.0000420    0            1\n 4 AREQUIPA            0.595   0            0.405        1\n 5 AYACUCHO            0.943   0.0568       0.000172     1\n 6 CAJAMARCA           0.525   0.370        0.105        1\n 7 CALLAO              0       0            1            1\n 8 CUSCO               0.487   0.513        0            1\n 9 HUANCAVELICA        1.00    0.0000275    0.000132     1\n10 HUANUCO             0.555   0.445        0            1\n# ℹ 15 more rows\n\n\nWe add labels to the columns:\n\nnatural_region_dep &lt;- \n  natural_region_dep |&gt; \n  labelled::set_variable_labels(\n    region = \"Name of the region\",\n    share_costa = \"Share of coastal areas in the region\",\n    share_selva = \"Share of forest areas in the region\",\n    share_sierra = \"Share of highland areas in the region\"\n  )\n\nAnd the results can be saved:\n\nsave(natural_region_dep, file =  \"../data/output/natural_region_dep.rda\")\n\nLet us plot the share of each type of natural regions in each administrative region on a map. We first prepare the data.\n\nmap_peru_nat_regions &lt;- NULL\n# nat_level &lt;- \"share_sierra\"\nfor (nat_level in c(\"share_sierra\", \"share_selva\", \"share_costa\")) {\n  map_peru_nat_reg_tmp &lt;- \n    map_peru |&gt; \n    dplyr::left_join(\n      natural_region_dep |&gt; \n        tidyr::pivot_longer(cols = c(share_sierra, share_selva, share_costa),\n                            names_to = \"natural_region\", values_to = \"share_natural\") |&gt; \n        filter(natural_region == !!nat_level),\n      by = c(\"DEPARTAMEN\" = \"region\")\n    )\n  map_peru_nat_regions &lt;- dplyr::bind_rows(map_peru_nat_regions, map_peru_nat_reg_tmp)\n}\n\nThen, we can create the maps.\n\nsource(\"../weatherperu/R/utils.R\")\np &lt;- \n  ggplot(\n    data = map_peru_nat_regions |&gt; \n      mutate(\n        natural_region = factor(\n          natural_region,\n          levels = c(\"share_costa\", \"share_selva\", \"share_sierra\"),\n          labels = c(\"Coast\", \"Forest\", \"Highlands\")\n        )\n      )\n    ) +\n  geom_sf(mapping = aes(fill = share_natural, group = DEPARTAMEN)) +\n  scale_fill_gradient2(\n    \"Share of\\neach natural\\ntype\", \n    low = \"#FFC107\", mid = \"white\", high = \"gray15\",\n    breaks = .25*0:4,\n    labels = scales::percent(.25*0:4)\n  ) +\n  facet_wrap(~natural_region) +\n  theme_map_paper()\n\np\n\n\n\n\n\n\n\nFigure 4.2: Share of each type of natural regions",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Other Data</span>"
    ]
  },
  {
    "objectID": "data-other.html#content-of-the-dataset",
    "href": "data-other.html#content-of-the-dataset",
    "title": "4  Other Data",
    "section": "4.2 Content of the dataset",
    "text": "4.2 Content of the dataset\n\n\n\nTable 4.1: Variables in the natural_region_dep file\n\n\n\n\n\nVariable name\nType\nDescription\n\n\n\n\nregion\ncharacter\nAdministrative Name of the Region\n\n\nshare_sierra\nnumeric\nShare of highlands\n\n\nshare_selva\nnumeric\nShare of forest\n\n\nshare_costa\nnumeric\nShare of coast",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Other Data</span>"
    ]
  },
  {
    "objectID": "data-other.html#content-of-the-dataset-1",
    "href": "data-other.html#content-of-the-dataset-1",
    "title": "4  Other Data",
    "section": "5.1 Content of the dataset",
    "text": "5.1 Content of the dataset\n\n\n\nTable 5.1: Variables in the ONI_temp file\n\n\n\n\n\n\n\n\n\n\nVariable name\nType\nDescription\n\n\n\n\nYear\ncharacter\nYear (YYYY)\n\n\nmonth\ncharacter\nMonth (MM)\n\n\nONI\nnumeric\nOceanic Niño Index\n\n\nelnino\nnumeric\n1 if El-Niño event, 0 otherwise\n\n\nlanina\nnumeric\n1 if La-Niña event, 0 otherwise\n\n\ndate\nnumeric\nDate of the beginning of the month (YYYY-MM-DD)\n\n\nState\nnumeric\nState: \"La Niña\", \"Normal\", or \"El Niño\"\n\n\ndate_start\nnumeric\n1 if current date corresponds to the begining of one of the three states, 0 otherwise\n\n\ndate_end\nnumeric\n1 if current date corresponds to the end of one of the three states, 0 otherwise",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Other Data</span>"
    ]
  },
  {
    "objectID": "data-merge.html",
    "href": "data-merge.html",
    "title": "5  Merging the files",
    "section": "",
    "text": "5.1 Load Intermediate Files\nThe weather data (Chapter 1) can be loaded:\nload(\"../data/output/weather/weather_regions_df.rda\")\nThe agricultural data (Chapter 2):\nload(\"../data/output/minagri/dataset_agri_2001_2015.rda\")\nThe macroeconomic data and commodity prices (Chapter 3):\nload(\"../data/output/macro/df_macro.rda\")\nload(\"../data/output/macro/df_int_prices.rda\")\nThe share of natural regions and the El Niño–Southern Oscillation (Chapter 4):\nload(\"../data/output/natural_region_dep.rda\")\nload(\"../data/output/weather/ONI_temp.rda\")",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Merging the files</span>"
    ]
  },
  {
    "objectID": "data-merge.html#merge-the-files",
    "href": "data-merge.html#merge-the-files",
    "title": "5  Merging the files",
    "section": "5.2 Merge the Files",
    "text": "5.2 Merge the Files\nWe add ENSO data to the weather dataset:\n\nWeather &lt;- weather_regions_df |&gt; \n  # Add ENSO data\n  left_join(\n    ONI_temp |&gt; \n      mutate(\n        Year = as.numeric(Year),\n        month = as.numeric(month)\n      ) |&gt; \n      rename(\n        enso_start = date_start,\n        enso_end = date_end\n      ),\n    by = c(\n      \"year\" = \"Year\",\n      \"month\" = \"month\"\n    )\n  ) |&gt; \n  group_by(IDDPTO, month, State) |&gt; \n  mutate( \n    temp_min_dev_ENSO   = temp_min - mean(temp_min),\n    temp_max_dev_ENSO   = temp_max - mean(temp_max),\n    temp_mean_dev_ENSO  = temp_mean - mean(temp_mean),\n    precip_sum_dev_ENSO = precip_sum - mean(precip_sum),\n    precip_piscop_sum_dev_ENSO = precip_piscop_sum - mean(precip_piscop_sum))|&gt; \n  ungroup() |&gt; \n  labelled::set_variable_labels(\n    temp_min_dev_ENSO   = \"Deviation of Min. Temperature from ENSO Normals\",\n    temp_max_dev_ENSO   = \"Deviation of Max. Temperature from ENSO Normals\",\n    temp_mean_dev_ENSO  = \"Deviation of Mean Temperature from ENSO Normals\",\n    precip_sum_dev_ENSO = \"Deviation of Total Rainfall from ENSO Normals (Chirps)\",\n    precip_piscop_sum_dev_ENSO = \"Deviation of Total Rainfall from ENSO Normals (Piscop)\"\n  )\n\nLet us merge all these datasets in a single one:\n\ndata_total &lt;- \n  data_total |&gt; \n  # Add weather data and ENSO \n  left_join(\n    #weather_regions_df |&gt; \n    Weather |&gt; \n      dplyr::select(-IDDPTO),\n    by = c(\n      \"year\" = \"year\",\n      \"month\" = \"month\",\n      \"region\" = \"DEPARTAMEN\", \n      \"date\" = \"date\"\n    )\n  ) |&gt; \n  # Add macroeconomic data\n  left_join(\n    df_macro |&gt; rename(gdp = y),\n    by = \"date\"\n  ) |&gt; \n  # Add commodity prices data\n  left_join(\n    int_prices,\n    by =  c(\n      \"date\", \"product\", \"product_eng\")\n  ) |&gt; \n  # Add share of each type of region\n  left_join(\n    natural_region_dep,\n    by = \"region\"\n  )\n\nHere are the first rows of that tibble:\n\ndata_total\n\n# A tibble: 25,920 × 111\n   region_id region   product       date       ln_prices ln_produc  year month\n       &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;date&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1         1 AMAZONAS MAÍZ AMILÁCEO 2001-01-01     0          0     2001     1\n 2         1 AMAZONAS MAÍZ AMILÁCEO 2001-02-01     0.536      6.58  2001     2\n 3         1 AMAZONAS MAÍZ AMILÁCEO 2001-03-01    NA          6.26  2001     3\n 4         1 AMAZONAS MAÍZ AMILÁCEO 2001-04-01     0.560      6.26  2001     4\n 5         1 AMAZONAS MAÍZ AMILÁCEO 2001-05-01     0.565      6.04  2001     5\n 6         1 AMAZONAS MAÍZ AMILÁCEO 2001-06-01    NA          7.00  2001     6\n 7         1 AMAZONAS MAÍZ AMILÁCEO 2001-07-01     0.531      7.00  2001     7\n 8         1 AMAZONAS MAÍZ AMILÁCEO 2001-08-01     0.525      8.37  2001     8\n 9         1 AMAZONAS MAÍZ AMILÁCEO 2001-09-01    NA          6.53  2001     9\n10         1 AMAZONAS MAÍZ AMILÁCEO 2001-10-01     0.519      6.53  2001    10\n# ℹ 25,910 more rows\n# ℹ 103 more variables: Value_prod &lt;dbl&gt;, surf_m &lt;dbl&gt;, Value_surfR &lt;dbl&gt;,\n#   Value_prices &lt;dbl&gt;, campaign &lt;dbl&gt;, campaign_plain &lt;chr&gt;,\n#   month_campaign &lt;dbl&gt;, surf_lag_calend &lt;dbl&gt;, product_eng &lt;chr&gt;,\n#   perc_product &lt;dbl&gt;, perc_product_mean &lt;dbl&gt;, diff_plant_harv &lt;dbl&gt;,\n#   exposition &lt;dbl&gt;, exposition_trend &lt;dbl&gt;, exposition_detrended &lt;dbl&gt;,\n#   exposition_norm &lt;dbl&gt;, temp_min &lt;dbl&gt;, temp_max &lt;dbl&gt;, temp_mean &lt;dbl&gt;, …\n\n\nSome descriptive statistics are shown in Chapter 6.\nLastly, the dataset can be saved for later use.\n\nsave(data_total, file = \"../data/output/dataset_2001_2015.rda\")\nwrite_csv(data_total, file = \"../data/output/dataset_2001_2015.csv\")",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Merging the files</span>"
    ]
  },
  {
    "objectID": "data-merge.html#dataset-for-the-local-projections",
    "href": "data-merge.html#dataset-for-the-local-projections",
    "title": "5  Merging the files",
    "section": "5.3 Dataset for the Local Projections",
    "text": "5.3 Dataset for the Local Projections\nNow, let us create the dataset specifically used to estimate the models.\nLet us make sure that the region data are encoded as a factor.\n\ndata_total &lt;- \n  data_total |&gt; \n  mutate(region_id = factor(region_id))\n\nThe crops we focus on:\n\ncrops &lt;- c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\")\n\nThe number of observation in each region, for each crop:\n\ndata_total |&gt; \n  filter(product_eng %in% crops) |&gt; \n  group_by(product_eng, region_id) |&gt; \n  summarise(n = sum(Value_prod &lt;= 0)) |&gt; \n  arrange(desc(n))\n\n`summarise()` has grouped output by 'product_eng'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 96 × 3\n# Groups:   product_eng [4]\n   product_eng region_id     n\n   &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt;\n 1 Potato      15          180\n 2 Potato      21          180\n 3 Potato      23          180\n 4 Potato      24          180\n 5 Rice        3           180\n 6 Rice        8           180\n 7 Rice        10          180\n 8 Rice        17          180\n 9 Rice        22          180\n10 Rice        14          171\n# ℹ 86 more rows\n\n\n\n5.3.1 Definition of the Variable of Interest\n\n\n\n\n\n\nWarning\n\n\n\nWe compute percentage deviation of production from monthly regional average, but we will actually not use those values in the subsequent estimations. In the first version of the analysis we used to do so, but this implied estimating a monthly regional trend. As kindly pointed out by a reviewer, the estimation of the trend should not be performed independently of the estimation. In the current version of this work, we use demeaned values of production and estimate the trend in the regressions.\n\n\nThis section outlines a two-step procedure for expressing agricultural production data at the monthly regional level for a specific crop and month as a percentage deviation from the monthly regional crop-specific average. The procedure involves handling missing values.\n\nStep 1: Handling Missing Values\nIn the first step, we address missing values by linear interpolation. This approach helps us estimate the missing values by considering the neighboring data points.\n\nStep 1.1: Imputing missing values with linear interpolation.\nThe missing values get replaced by linear interpolation. However, if there are more than two consecutive missing values, they are not replaced with interpolated values. Instead, the series for the specific crop in the given region is split based on the locations of the missing values. The split with the highest number of consecutive non-missing values is retained, while the other splits are discarded.\nStep 1.2: Dropping Series with Remaining Missing Values\nAfter imputing missing values using the moving median, we check if any missing values still remain in the dataset. If there are any remaining missing values for a particular series, we choose to exclude that series from further analysis. By doing so, we ensure that the subsequent detrending process is performed only on reliable and complete data.\n\nStep 2: Normalized Agricultural Production\nFor each month ( m ), region ( i ), and crop ( c ), we calculate the average production over the entire period (January 2001 to December 2015): \\[\\overline{y}_{c,i,m} = \\frac{1}{n_{T_c}} \\sum_{t=1}^{T_c} y_{c,i,m,t}^{\\text{raw}}\n  \\] Then, we express agricultural production relative to the average: \\[y_{c,i,m,t} = \\begin{cases}\n  \\frac{y_{c,i,m,t}^{\\text{raw}}}{\\overline{y}_{c,i,m}}, & \\overline{y}_{c,i,m} &gt; 0\\\\\n  0, & \\overline{y}_{c,i,m} = 0\n  \\end{cases}\\] Values of \\(y_{c,i,m,t}&gt;1\\) means that the production for crop \\(c\\) in region \\(i\\) during month \\(m\\) of year \\(t\\) is higher than the average monthly production for that crop and region over the period 2001 to 2015. For example, a value of 1.5 means that the production is 50% higher than average.\nStep 2 (alternative version): Deviation from regional monthly average, in percent (this step is useless in the new version of the analysis: it lead to discard too many observations)\nOnce we have addressed the missing values, we proceed to the second step, which consists in computing the deviation of production from the monthly regional average. First, we compute the average production of each crop \\(c\\) in each region \\(i\\) for calendar month \\(m\\): \\[\\overline{y}_{c,i,m} = \\frac{1}{n_{T_c}} \\sum_{t=1}^{T_c} y_{c,i,m,t}^{raw}\\] Then, we compute the percentage deviation from this average at each date \\(t\\): \\[y_{c,i,m,t} = \\frac{y_{c,i,m,t}^{raw} - \\overline{y}_{c,i,m}}{\\overline{y}_{c,i,m}}\\]\n\nLet us implement this process in R. First, we need to define two functions to handle the missing values:\n\nThe get_index_longest_non_na() function retrieves the indices of the longest consecutive sequence without missing values from a given input vector. It helps us identify the positions of elements in that sequence.\nThe keep_values_longest_non_na() function uses the obtained indices to create a logical vector. Each element of this vector indicates whether the corresponding element in the input vector belongs to the longest consecutive sequence of non-missing values. This allows us to filter the data and retain only the values from the longest consecutive sequence without missing values.\n\nThese two functions combined help us handle missing data in the weather series and ensure that we work with the most complete sequences for each region and crop.\nThe first function:\n\n#' Returns the index of the longest sequence of non NA values in a vector\n#'\n#' @param y vector of numerical values\n#' @export\nget_index_longest_non_na &lt;- function(y) {\n  split_indices &lt;- which(is.na(y))\n  nb_obs &lt;- length(y)\n\n  if (length(split_indices) == 0) {\n    res &lt;- seq_len(nb_obs)\n  } else {\n    idx_beg &lt;- c(1, split_indices)\n    if (idx_beg[length(idx_beg)] != nb_obs) {\n      idx_beg &lt;- c(idx_beg, nb_obs)\n    }\n    lengths &lt;- diff(idx_beg)\n    ind_max &lt;- which.max(lengths)\n    index_beginning &lt;- idx_beg[ind_max]\n    if(!index_beginning == 1 | is.na(y[index_beginning])) {\n      index_beginning &lt;- index_beginning + 1\n    }\n    index_end &lt;- idx_beg[ind_max] + lengths[ind_max]\n    if(is.na(y[index_end])) {\n      index_end &lt;- index_end - 1\n    }\n    res &lt;- seq(index_beginning, index_end)\n  }\n  res\n}\n\nThe second one:\n\n#' Returns a logical vector that identifies the longest sequence of non NA\n#' values within the input vector\n#' \n#' @param y numeric vector\nkeep_values_longest_non_na &lt;- function(y) {\n  ids_to_keep &lt;- get_index_longest_non_na(y)\n  ids &lt;- seq(1, length(y))\n  ids %in% ids_to_keep\n}\n\n\n\n\n\n\n\nNote\n\n\n\nThose two functions are defined in weatherperu/R/utils.R.\n\n\nWe define another function, pct_prod_production(), that takes the data frame of observations as input, as well as a crop name and a region ID. It returns a tibble with the following variables:\n\nproduct_eng: the English name of the crop\nregion_id: the ID of the region\nmonth: month number\ndate: date\ny_new_normalized (our variable of interest in Chapter 7): the production demeaned by the month-specific average for the crop of interest in the region of interest\ny_new: the production (in tons) where missing values were imputed, if possible\ny_dev_pct: the production expressed as the percentage deviation from the monthly-specific average (for the crop of interest, in the region of interest)\ny: same as y_dev_pct but without an estimated month-specific quadratic trend estimated by OLS\nt: month-specific trend.\n\n\n#' Computes the percentage deviation of production from monthly regional average\n#'\n#' @param df data\n#' @param crop_name name of the crop\n#' @param region_id id of the region\n#'\n#' @returns tibble with the product, the region id, the date, the production\n#'  where missing values were imputed (`y_new`), the percentage deviation of\n#'  production from its monthly regional average (`y_dev_pct`), the percentage\n#'  deviation of production from its monthly regional average where a quadratic\n#'  trend has been removed (`y`), the demeaned production (`y_new_normalized`),\n#'  a month-specific trend (`t`)\n#' @export\n#' @importFrom dplyr filter arrange mutate select row_number group_by\n#' @importFrom tidyr nest unnest\n#' @importFrom purrr map\n#' @importFrom imputeTS na_interpolation\n#' @importFrom stats lm predict residuals\npct_prod_production &lt;- function(df,\n                                crop_name,\n                                region_id) {\n  # The current data\n  df_current &lt;-\n    df |&gt;\n    filter(\n      product_eng == !!crop_name,\n      region_id == !!region_id\n    ) |&gt;\n    arrange(date)\n\n  ## Dealing with missing values ----\n  # Look for negative production values\n  df_current &lt;-\n    df_current |&gt;\n    mutate(\n      y_new = ifelse(Value_prod &lt; 0, NA, Value_prod)\n    )\n\n  if (any(is.na(df_current$y_new))) {\n\n    # Replacing NAs by interpolation\n    # If there are more than two contiguous NAs, they are not replaced\n    df_current &lt;-\n      df_current |&gt;\n      mutate(\n        y_new = imputeTS::na_interpolation(y_new, maxgap = 3)\n      )\n\n    # Removing obs at the beginning/end if they are still missing\n    df_current &lt;-\n      df_current |&gt;\n      mutate(\n        row_to_keep = !(is.na(y_new) & row_number() %in% c(1:2, (n()-1):(n())))\n      ) |&gt;\n      filter(row_to_keep) |&gt;\n      select(-row_to_keep)\n\n    # Keeping the longest series of continuous non-NA values\n    df_current &lt;-\n      df_current |&gt;\n      mutate(\n        row_to_keep = keep_values_longest_non_na(y_new)\n      ) |&gt;\n      filter(row_to_keep) |&gt;\n      select(-row_to_keep)\n  }\n\n\n  rle_y_new &lt;- rle(df_current$y_new)\n  check_contiguous_zeros &lt;- rle_y_new$lengths[rle_y_new$values==0]\n\n  # If there are more than 8 contiguous 0, the series is discarded\n  if (any(check_contiguous_zeros &gt; 8)) {\n    resul &lt;- NULL\n  } else {\n    ## Percent deviation from monthly regional average\n    resul &lt;-\n      df_current |&gt;\n      group_by(month) |&gt;\n      arrange(date) |&gt;\n      mutate(\n        y_new_normalized = case_when(\n          mean(y_new) == 0~ 0,\n          TRUE ~ y_new / mean(y_new)\n        ),\n        y_dev_pct = case_when(\n          mean(y_new) == 0 ~ 0,\n          TRUE ~ (y_new - mean(y_new)) / mean(y_new)\n        )\n      ) |&gt;\n      ungroup() |&gt;\n      mutate(t = row_number()) |&gt;\n      ungroup() |&gt;\n      nest(.by = c(product_eng, region_id, month)) |&gt;\n      # distinct OLS per month\n      mutate(\n        ols_fit   = map(data, ~ lm(y_new_normalized ~ -1 + t + I(t^2), data = .x)),\n        resid     = map(ols_fit, residuals),\n        fitted    = map(ols_fit, predict)\n      ) |&gt;\n      unnest(cols = c(data, resid, fitted)) |&gt;\n      group_by(month) |&gt;\n      mutate(\n        y = resid\n      ) |&gt;\n      select(product_eng, region_id, month, date, y_new, y_dev_pct, y_new_normalized, y, t) |&gt;\n      ungroup() |&gt;\n      arrange(date)\n  }\n  resul\n}\n\nFor example, for potatoes in region with id 1:\n\npct_prod_production(\n  df = data_total, \n    crop_name = \"Potato\", \n    region_id = 1\n)\n\n# A tibble: 180 × 9\n   product_eng region_id month date       y_new y_dev_pct y_new_normalized     y\n   &lt;chr&gt;       &lt;fct&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 Potato      1             1 2001-01-01 2360    -0.301             0.699 0.671\n 2 Potato      1             2 2001-02-01 2841    -0.379             0.621 0.565\n 3 Potato      1             3 2001-03-01 4585    -0.151             0.849 0.774\n 4 Potato      1             4 2001-04-01 4585    -0.107             0.893 0.786\n 5 Potato      1             5 2001-05-01 6065     0.162             1.16  1.05 \n 6 Potato      1             6 2001-06-01 3998.   -0.307             0.693 0.581\n 7 Potato      1             7 2001-07-01 3998.   -0.351             0.649 0.520\n 8 Potato      1             8 2001-08-01 5921    -0.0959            0.904 0.749\n 9 Potato      1             9 2001-09-01 4751    -0.250             0.750 0.567\n10 Potato      1            10 2001-10-01 4751    -0.151             0.849 0.649\n# ℹ 170 more rows\n# ℹ 1 more variable: t &lt;int&gt;\n\n\nWe can apply this function to all crops of interest, in each region. Let us define a table that contains all the possible values for the combination of crops and regions:\n\nproduct_and_regions &lt;- \n  data_total |&gt; \n  filter(product_eng %in% crops) |&gt; \n  select(product_eng, region_id) |&gt; \n  unique()\n\nThen we apply the pct_prod_production() function to all these different cases, and store the results in a list named df_pct_pred_production:\n\ndf_pct_pred_production &lt;- vector(mode = \"list\", length = nrow(product_and_regions))\ncli_progress_bar(total = nrow(product_and_regions))\nfor(i in 1:nrow(product_and_regions)){\n  df_pct_pred_production[[i]] &lt;- pct_prod_production(\n    df = data_total, \n    crop_name = product_and_regions$product_eng[i], \n    region_id = product_and_regions$region_id[i]\n  )\n  cli_progress_update(set = i)\n}\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n■■■■■■■                           19% | ETA:  4s\n\n\n■■■■■■■■■■■■■■■■■■■■■■■■          77% | ETA:  1s\n\n\n■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% | ETA:  0s\n\n\nThe elements of the list are all tibbles, with the same column names. We can merge them in a single tibble.\n\ndf_pct_pred_production &lt;- bind_rows(df_pct_pred_production)\n\nWe can have a look at the number of months with 0 values for the agricultural production.\n\ndf_pct_pred_production |&gt; \n  group_by(product_eng, region_id) |&gt; \n  summarise(nb_0 = sum(y_new == 0)) |&gt; \n  arrange(desc(nb_0))\n\n`summarise()` has grouped output by 'product_eng'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 78 × 3\n# Groups:   product_eng [4]\n   product_eng region_id  nb_0\n   &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt;\n 1 Rice        2           108\n 2 Rice        4           100\n 3 Potato      20           71\n 4 Rice        7            68\n 5 Dent corn   3            67\n 6 Dent corn   8            66\n 7 Rice        16           61\n 8 Dent corn   22           59\n 9 Cassava     10           54\n10 Rice        23           52\n# ℹ 68 more rows\n\n\nNow, let us add the other columns to the tibble that contains the percentage deviation production data:\n\ndf &lt;- df_pct_pred_production |&gt; \n  left_join(\n    data_total,\n    join_by(product_eng, region_id, month, date)\n  )\n\nLet us also impute missing values for the weather variables.\n\nweather_variables &lt;- \n  weather_regions_df |&gt; \n  select(where(is.numeric)) |&gt; \n  select(-year, -month) |&gt; \n  colnames()\n\nThe current number of missing values:\n\ndf |&gt; \n  summarise(\n    across(\n      .cols = !!weather_variables,\n      .fns = ~ sum(is.na(.x)),\n      .names = \"{.col}_nb_na\"\n    )\n  ) |&gt; \n  unlist()\n\n                temp_min_nb_na                 temp_max_nb_na \n                             0                              0 \n               temp_mean_nb_na               precip_sum_nb_na \n                             0                              0 \n       precip_piscop_sum_nb_na        perc_gamma_precip_nb_na \n                             0                              0 \nperc_gamma_precip_piscop_nb_na                 gdd_rice_nb_na \n                             0                              0 \n               gdd_maize_nb_na               gdd_potato_nb_na \n                             0                              0 \n             gdd_cassava_nb_na                 hdd_rice_nb_na \n                             0                              0 \n               hdd_maize_nb_na               hdd_potato_nb_na \n                             0                              0 \n             hdd_cassava_nb_na             temp_min_dev_nb_na \n                             0                              0 \n            temp_max_dev_nb_na            temp_mean_dev_nb_na \n                             0                              0 \n          precip_sum_dev_nb_na    precip_piscop_sum_dev_nb_na \n                             0                              0 \n            gdd_rice_dev_nb_na            gdd_maize_dev_nb_na \n                             0                              0 \n          gdd_potato_dev_nb_na          gdd_cassava_dev_nb_na \n                             0                              0 \n            hdd_rice_dev_nb_na            hdd_maize_dev_nb_na \n                             0                              0 \n          hdd_potato_dev_nb_na          hdd_cassava_dev_nb_na \n                             0                              0 \n     cold_surprise_maize_nb_na       cold_surprise_rice_nb_na \n                             0                              0 \n    cold_surprise_potato_nb_na    cold_surprise_cassava_nb_na \n                             0                              0 \n      hot_surprise_maize_nb_na        hot_surprise_rice_nb_na \n                             0                              0 \n     hot_surprise_potato_nb_na     hot_surprise_cassava_nb_na \n                             0                              0 \n      dry_surprise_maize_nb_na        dry_surprise_rice_nb_na \n                             0                              0 \n     dry_surprise_potato_nb_na     dry_surprise_cassava_nb_na \n                             0                              0 \n      wet_surprise_maize_nb_na        wet_surprise_rice_nb_na \n                             0                              0 \n     wet_surprise_potato_nb_na     wet_surprise_cassava_nb_na \n                             0                              0 \n                   spi_1_nb_na                    spi_3_nb_na \n                             0                              0 \n                   spi_6_nb_na                   spi_12_nb_na \n                             0                              0 \n                  spei_1_nb_na                   spei_3_nb_na \n                             0                              0 \n                  spei_6_nb_na                  spei_12_nb_na \n                             0                              0 \n            spi_piscop_1_nb_na             spi_piscop_3_nb_na \n                             0                              0 \n            spi_piscop_6_nb_na            spi_piscop_12_nb_na \n                             0                              0 \n           spei_piscop_1_nb_na            spei_piscop_3_nb_na \n                             0                              0 \n           spei_piscop_6_nb_na           spei_piscop_12_nb_na \n                             0                              0 \n\n\nIn case of missing values, we use linear interpolation to replace them:\n\ndf &lt;- \n  df |&gt; \n  mutate(\n    across(\n      .cols = !!weather_variables,\n      .fns = ~ imputeTS::na_interpolation(.x, maxgap = 3)\n    )\n  )\n\nThe number of remaining missing values:\n\ndf |&gt; \n  summarise(\n    across(\n      .cols = !!weather_variables,\n      .fns = ~ sum(is.na(.x)),\n      .names = \"{.col}_nb_na\"\n    )\n  ) |&gt; \n  unlist()\n\n                temp_min_nb_na                 temp_max_nb_na \n                             0                              0 \n               temp_mean_nb_na               precip_sum_nb_na \n                             0                              0 \n       precip_piscop_sum_nb_na        perc_gamma_precip_nb_na \n                             0                              0 \nperc_gamma_precip_piscop_nb_na                 gdd_rice_nb_na \n                             0                              0 \n               gdd_maize_nb_na               gdd_potato_nb_na \n                             0                              0 \n             gdd_cassava_nb_na                 hdd_rice_nb_na \n                             0                              0 \n               hdd_maize_nb_na               hdd_potato_nb_na \n                             0                              0 \n             hdd_cassava_nb_na             temp_min_dev_nb_na \n                             0                              0 \n            temp_max_dev_nb_na            temp_mean_dev_nb_na \n                             0                              0 \n          precip_sum_dev_nb_na    precip_piscop_sum_dev_nb_na \n                             0                              0 \n            gdd_rice_dev_nb_na            gdd_maize_dev_nb_na \n                             0                              0 \n          gdd_potato_dev_nb_na          gdd_cassava_dev_nb_na \n                             0                              0 \n            hdd_rice_dev_nb_na            hdd_maize_dev_nb_na \n                             0                              0 \n          hdd_potato_dev_nb_na          hdd_cassava_dev_nb_na \n                             0                              0 \n     cold_surprise_maize_nb_na       cold_surprise_rice_nb_na \n                             0                              0 \n    cold_surprise_potato_nb_na    cold_surprise_cassava_nb_na \n                             0                              0 \n      hot_surprise_maize_nb_na        hot_surprise_rice_nb_na \n                             0                              0 \n     hot_surprise_potato_nb_na     hot_surprise_cassava_nb_na \n                             0                              0 \n      dry_surprise_maize_nb_na        dry_surprise_rice_nb_na \n                             0                              0 \n     dry_surprise_potato_nb_na     dry_surprise_cassava_nb_na \n                             0                              0 \n      wet_surprise_maize_nb_na        wet_surprise_rice_nb_na \n                             0                              0 \n     wet_surprise_potato_nb_na     wet_surprise_cassava_nb_na \n                             0                              0 \n                   spi_1_nb_na                    spi_3_nb_na \n                             0                              0 \n                   spi_6_nb_na                   spi_12_nb_na \n                             0                              0 \n                  spei_1_nb_na                   spei_3_nb_na \n                             0                              0 \n                  spei_6_nb_na                  spei_12_nb_na \n                             0                              0 \n            spi_piscop_1_nb_na             spi_piscop_3_nb_na \n                             0                              0 \n            spi_piscop_6_nb_na            spi_piscop_12_nb_na \n                             0                              0 \n           spei_piscop_1_nb_na            spei_piscop_3_nb_na \n                             0                              0 \n           spei_piscop_6_nb_na           spei_piscop_12_nb_na \n                             0                              0 \n\n\n\n\n5.3.2 Saving the file\nThe dataset that can be used to estimate the impact of weather shocks on agricultural production can be saved in the data output folder:\n\n# Add labels to the new columns\ndf &lt;- \n  df |&gt; \n  labelled::set_variable_labels(\n    y_new = \"Monthly Agricultural Production (tons)\",\n    y_dev_pct = \"Agricultural Production (percent deviation from monthly regional values)\",\n  )\n\nsave(df, file = \"../data/output/df_lp.rda\")",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Merging the files</span>"
    ]
  },
  {
    "objectID": "data-desc-stats.html",
    "href": "data-desc-stats.html",
    "title": "6  Descriptive Statistics",
    "section": "",
    "text": "6.1 Dictionary of the Variables",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "data-desc-stats.html#dictionary-of-the-variables",
    "href": "data-desc-stats.html#dictionary-of-the-variables",
    "title": "6  Descriptive Statistics",
    "section": "",
    "text": "Table 6.1: Variables in the df dataset stored in \"../data/output/df_lp.rda\" file\n\n\n\n\n\n\n\n\n\n\nVariable name\nType\nDescription\n\n\n\n\ny_new\nnumeric\nMonthly Agricultural Production (tons)\n\n\ny_dev_pct\nnumeric\nAgricultural Production (percent deviation from monthly regional values)\n\n\nproduct\ncharacter\nName of the crop (in Spanish)\n\n\nproduct_eng\ncharacter\nName of the Product (in English)\n\n\nregion_id\ninteger\nRegion numerical ID\n\n\nregion\ncharacter\nName of the region\n\n\nyear\nnumeric\nYear (YYYY)\n\n\nmonth\nnumeric\nMonth (MM)\n\n\ndate\nDate\nDate (YYYY-MM-DD)\n\n\nln_prices\nnumeric\nProduct price (log)\n\n\nln_produc\nnumeric\nProduction (log of tons)\n\n\nValue_prod\nnumeric\nProduction (tons)\n\n\nsurf_m\nnumeric\nPlanted Surface during the current month (hectares)\n\n\nValue_surfR\nnumeric\nHarvested Surface (hectares)\n\n\nValue_prices\nnumeric\nUnit Price (Pesos)\n\n\ncampaign\nnumeric\nID of the planting campaing (starting in August)\n\n\ncampaign_plain\ncharacter\nYears of the planting campaing (starting in August)\n\n\nmonth_campaign\nnumeric\nMonth of the planting campaing (August = 1)\n\n\nsurf_lag_calend\nnumeric\nPlanted Surface laggued by the growth duration computed from the caledars (hectares)\n\n\nperc_product\nnumeric\nShare of the annual production harvested at month m\n\n\nperc_product_mean\nnumeric\nAverage share of the annual production harvested at month m\n\n\ndiff_plant_harv\nnumeric\nDifference between planted and harvested surfaces during month m\n\n\nexposition\nnumeric\nCumulative difference between planted and harvested surfaces\n\n\nexposition_trend\nnumeric\nTrend of the exposition using HP filter\n\n\nexposition_detrended\nnumeric\nDifference between the exposition and its trend\n\n\nexposition_norm\nnumeric\nNormalisation of the detrended exposition\n\n\ntemp_min\nnumeric\nMonthly average of daily min temperature\n\n\ntemp_max\nnumeric\nMonthly average of daily max temperature\n\n\ntemp_mean\nnumeric\nMonthly average of daily mean temperature\n\n\nprecip_sum, precip_piscop_sum\nnumeric\nMonthly sum of daily rainfall\n\n\nperc_gamma_precip, perc_gamma_precip_piscop\nnumeric\nPercentile of the monthly precipitation (Estimated Gamma Distribution)\n\n\ntemp_min_dev\nnumeric\nDeviation of monthly min temperatures (temp_min) from climate normals (1986 – 2015)\n\n\ntemp_max_dev\nnumeric\nDeviation of monthly max temperatures (temp_max) from climate normals (1986 – 2015)\n\n\ntemp_mean_dev\nnumeric\nDeviation of monthly mean temperatures (temp_mean) from climate normals (1986 – 2015)\n\n\nprecip_sum_dev, precip_piscop_sum_dev\nnumeric\nDeviation of monthly total rainfall (precip_sum) from climate normals (1986 – 2015)\n\n\nspi_1, spi_piscop_1\nnumeric\nSPI Drought Index, Scale = 1\n\n\nspi_3, spi_piscop_3\nnumeric\nSPI Drought Index, Scale = 3\n\n\nspi_6, spi_piscop_6\nnumeric\nSPI Drought Index, Scale = 6\n\n\nspi_12, spi_piscop_12\nnumeric\nSPI Drought Index, Scale = 12\n\n\nspei_1, spei_piscop_1\nnumeric\nSPEI Drought Index, Scale = 1\n\n\nspei_3, spei_piscop_3\nnumeric\nSPEI Drought Index, Scale = 3\n\n\nspei_6, spei_piscop_6\nnumeric\nSPEI Drought Index, Scale = 6\n\n\nspei_12, spei_piscop_12\nnumeric\nSPEI Drought Index, Scale = 12\n\n\nONI\nnumeric\nOceanic Niño Index\n\n\nelnino\nnumeric\n1 if El-Niño event, 0 otherwise\n\n\nlanina\nnumeric\n1 if La-Niña event, 0 otherwise\n\n\nState\nnumeric\nState: \"La Niña\", \"Normal\", or \"El Niño\"\n\n\nenso_start\nnumeric\n1 if current date corresponds to the begining of one of the three states, 0 otherwise\n\n\nenso_end\nnumeric\n1 if current date corresponds to the end of one of the three states, 0 otherwise\n\n\ntemp_min_dev_ENSO\nnumeric\nDeviation of Min. Temperature from ENSO Normals\n\n\ntemp_max_dev_ENSO\nnumeric\nDeviation of Max. Temperature from ENSO Normals\n\n\ntemp_mean_dev_ENSO\nnumeric\nDeviation of Mean Temperature from ENSO Normals\n\n\nprecip_sum_dev_ENSO, precip_piscop_sum_dev_ENSO\nnumeric\nDeviation of Total Rainfall from ENSO Normals\n\n\ngdp\nnumeric\nGDP in percentage point, percentage deviation from trend, detrended and seasonally adjusted\n\n\nya\nnumeric\nAgricultural GDP in percentage point, percentage deviation from trend, detrended and seasonally adjusted\n\n\nrer_hp\nnumeric\nReal exchange rate, detrended using HP filter\n\n\nrer_dt_sa\nnumeric\nReal exchange rate, detrended and seasonally adjusted\n\n\nr\nnumeric\nInterest rate, in percentage point, detrended\n\n\nr_hp\nnumeric\nInterest rate, in percentage point, detrended using HP filter\n\n\npi\nnumeric\nInflation rate, in percentage point\n\n\npia\nnumeric\nFood inflation rate, in percentage point, seasonally adjusted\n\n\nind_prod\nnumeric\nManufacturing Production, in percentage point, percentage deviation from trend, detrended and seasonally adjusted\n\n\nprice_int\nnumeric\nInternational commodity prices\n\n\nprice_int_inf\nnumeric\nGrowth rate of international commodity prices\n\n\nshare_sierra\nnumeric\nShare of highlands\n\n\nshare_selva\nnumeric\nShare of forest\n\n\nshare_costa\nnumeric\nShare of coast",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "data-desc-stats.html#crops",
    "href": "data-desc-stats.html#crops",
    "title": "6  Descriptive Statistics",
    "section": "6.2 Crops",
    "text": "6.2 Crops\nWe focus on the following crops:\n\ncrops &lt;- c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\")\n\nTable 6.2 presents some descriptive statistics about the monthly production of the selected crops, averaged over the region\n\n\nCode\ndf |&gt; \n  group_by(product_eng) |&gt; \n  summarise(\n    Mean = mean(y_new, na.rm = TRUE),\n    Median = median(y_new, na.rm = TRUE),\n    `Standard Deviation` = sd(y_new, na.rm = TRUE),\n    Min = min(y_new, na.rm = TRUE),\n    Max = max(y_new, na.rm = TRUE),\n    `No. regions` = length(unique(region)),\n    `No. obs.` = n()\n  ) |&gt; \n  rename(Culture = product_eng) |&gt; \n  knitr::kable(\n    booktabs = TRUE, \n    format.args = list(big.mark = \",\")\n  )\n\n\n\n\nTable 6.2: Descriptive statistics for monthly production (in tons) per type of crop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCulture\nMean\nMedian\nStandard Deviation\nMin\nMax\nNo. regions\nNo. obs.\n\n\n\n\nCassava\n4,531.272\n1,798.6500\n7,109.64\n0\n57,135.0\n20\n3,600\n\n\nDent corn\n4,290.878\n983.4735\n7,254.97\n0\n74,623.7\n23\n4,140\n\n\nPotato\n16,393.353\n5,048.8500\n29,574.83\n0\n360,070.0\n19\n3,420\n\n\nRice\n13,458.593\n1,654.0000\n28,312.71\n0\n318,706.0\n16\n2,880\n\n\n\n\n\n\n\n\n\n6.2.1 National Production\nFigure 6.1 provides a visual representation of the national production of each time of crop over our time sample, which is the sum of the monthly regional production.\n\n\nCode\nggplot(\n  data = df |&gt; \n    group_by(product_eng, date) |&gt; \n    summarise(\n      nat_prod = sum(y_new),\n      .groups = \"drop\"\n    ) |&gt; \n    mutate(\n      product_eng = factor(\n        product_eng, \n        levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n        labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"),\n      )\n    )\n) +\n  geom_line(\n    mapping = aes(x = date, y = nat_prod),\n    colour = \"#1f78b4\",\n    linewidth = 1\n  ) +\n  facet_wrap(~product_eng, scales = \"free_y\", ncol = 2) +\n  labs(x = NULL, y=  \"Aggregate Production (tons)\") +\n  scale_y_continuous(labels = scales::number_format(big.mark = \",\")) +\n  theme_paper()\n\n\n\n\n\n\n\n\nFigure 6.1: National monthly crop production for selected cultures (in tons)\n\n\n\n\n\n\n\n6.2.2 National Production by Month and Type of Region\nIn Figure 6.2, we document the regional differences and the seasonality by averaging the monthly production over the different types of natural regions.\n\n\nCode\nggplot(\n  data = df |&gt; \n    group_by(product_eng, year, month) |&gt; \n    # Average each month at the national level\n    summarise(\n      prod_nat_costa = sum(y_new * share_costa),\n      prod_nat_selva = sum(y_new * share_selva),\n      prod_nat_sierra = sum(y_new * share_sierra),\n      .groups = \"drop\"\n    ) |&gt; \n    pivot_longer(\n      cols = c(prod_nat_costa, prod_nat_selva, prod_nat_sierra),\n      names_to = \"geo\",\n      values_to = \"monthly_prod_geo\"\n    ) |&gt; \n    # Average in each region type for each calendar month\n    group_by(product_eng, month, geo) |&gt; \n    summarise(\n      monthly_prod_geo = mean(monthly_prod_geo),\n      .groups = \"drop\"\n    ) |&gt; \n    mutate(\n      product_eng = factor(\n        product_eng, \n        levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n        labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"),\n      ),\n      geo = factor(\n        geo,\n        levels = c(\"prod_nat_costa\", \"prod_nat_selva\", \"prod_nat_sierra\"),\n        labels = c(\"Coast\", \"Forest\", \"Highlands\")\n      )\n    ),\n  mapping = aes(\n    x = month, y = monthly_prod_geo, \n    colour = geo, linetype = geo\n  )\n) +\n  geom_line(\n    linewidth = 1\n  ) +\n  facet_wrap(~product_eng, scales = \"free\") +\n  labs(x = NULL, y = \"Average Production (tons)\") +\n  scale_colour_manual(\n    NULL,\n    values = c(\n      \"Coast\" = \"#56B4E9\", \"Forest\" = \"#009E73\", \"Highlands\" = \"#E69F00\"\n    )\n  ) +\n  scale_linetype_discrete(NULL) +\n  scale_x_continuous(breaks= 1:12, labels = month.abb) +\n  scale_y_continuous(labels = scales::number_format(big.mark = \",\")) +\n  theme_paper()\n\n\n\n\n\n\n\n\nFigure 6.2: Crop production by months and natural regions (in tons)\n\n\n\n\n\n\n\n6.2.3 Share of agricultural land\nLet us load the share of agricultural area in the cell, for each cell of the grid (see Section 1.2 for more details on that specific grid):\n\nload(\"../data/output/land/map_peru_grid_agri.rda\")\n\nFigure 6.3 presents the production distribution for each type of crop.\n\n\nCode\nggplot() +\n  geom_sf(\n    data = map_peru_grid_agri |&gt; \n      mutate(share_cropland = percent_cropland / 100), \n    mapping = aes(fill = share_cropland),\n    colour = \"grey\"\n  ) +\n  geom_sf(data = map_peru, fill = NA) +\n  scale_fill_gradient2(\n    \"Share of\\nagricultural\\nland\", \n    low = \"white\", high = \"#61C250\",\n    labels = scales::label_percent()\n  ) +\n  theme_map_paper()\n\n\n\n\n\n\n\n\nFigure 6.3: Regional distribution of crop production by administrative regions\n\n\n\n\n\n\n\n6.2.4 Regions Used in the Local Projection Analysis\nWe need to know, for each region of the map, whether it is included or not in the analysis.\nThe number of regions used in the analysis for each crop:\n\ndf |&gt; \n  group_by(product_eng) |&gt; \n  summarise(nb_regions = length(unique(region_id)))\n\n# A tibble: 4 × 2\n  product_eng nb_regions\n  &lt;chr&gt;            &lt;int&gt;\n1 Cassava             20\n2 Dent corn           23\n3 Potato              19\n4 Rice                16\n\n\nWe can also look at the spatial distribution of these regions. Let us create a map that shows whether each region is included/excludes. On the same map, we would like to have an idea of the relative share in the total production over the period each region represents, for each crop.\n\ndf_plot_map_regions &lt;- \n  map_peru |&gt; \n  left_join(\n    df |&gt; \n      select(product_eng, region) |&gt; unique() |&gt; \n      mutate(used = TRUE) |&gt; \n      pivot_wider(names_from = product_eng, values_from = used),\n    by = c(\"DEPARTAMEN\" = \"region\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us load the dataset obtained at the end of Chapter 5.\n\nload(\"../data/output/dataset_2001_2015.rda\")\n\nWe compute the crop-specific share that each region represents in the annual production.\n\nprod_reg &lt;- \n  data_total |&gt; \n  group_by(region, product_eng) |&gt; \n  summarise(\n    total_production = sum(Value_prod, na.rm = T),\n    .groups = \"drop\"\n  ) |&gt; \n  unique() |&gt; \n  group_by(product_eng) |&gt; \n  mutate(total_culture = sum(total_production)) |&gt; \n  ungroup() |&gt; \n  mutate(share = total_production / total_culture)\nprod_reg\n\n# A tibble: 144 × 5\n   region   product_eng     total_production total_culture   share\n   &lt;chr&gt;    &lt;chr&gt;                      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 AMAZONAS Amylaceous corn           99281.      3976940. 0.0250 \n 2 AMAZONAS Cassava                 1894369.     16268850. 0.116  \n 3 AMAZONAS Dent corn                329168.     17776855. 0.0185 \n 4 AMAZONAS Potato                   925576.     55753747. 0.0166 \n 5 AMAZONAS Rice                    4001141.     38770607. 0.103  \n 6 AMAZONAS Wheat                     13652.      3041972. 0.00449\n 7 ANCASH   Amylaceous corn          183573.      3976940. 0.0462 \n 8 ANCASH   Cassava                   99944.     16268850. 0.00614\n 9 ANCASH   Dent corn               1259909.     17776855. 0.0709 \n10 ANCASH   Potato                  1582314.     55753747. 0.0284 \n# ℹ 134 more rows\n\n\nWe prepare a map dataset for each crop:\n\nmap_production_regions &lt;- NULL\nfor (i in 1:length(crops)) {\n  culture &lt;- as.character(crops[i])\n  map_peru_tmp &lt;- map_peru |&gt; \n    left_join(\n      prod_reg |&gt; \n        filter(product_eng == !!culture), \n      by = c(\"DEPARTAMEN\" = \"region\")\n    ) |&gt; \n    mutate(product_eng = !!culture)\n  map_production_regions &lt;- bind_rows(map_production_regions, map_peru_tmp)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLastly, let us add information on inclusion/exclusion from the analysis:\n\nincluded_regions &lt;- \n  df_plot_map_regions |&gt; \n  pivot_longer(cols = !!crops, values_to = \"included\") |&gt; \n  as_tibble() |&gt; \n  select(IDDPTO, name, included) |&gt; \n  mutate(included = replace_na(included, FALSE)) |&gt; \n  rename(product_eng = name)\n\nFigure 6.4 allows to visualize the relative importance of the discarded series in relation to the overall national agricultural production.\n\n\nCode\nlibrary(ggpattern)\np_regions_use_production &lt;- \n  ggplot(\n    data = map_production_regions |&gt; \n      left_join(included_regions, by = c(\"IDDPTO\", \"product_eng\")) |&gt; \n      mutate(\n        included = factor(\n          included, \n          levels = c(TRUE, FALSE), \n          labels = c(\"Yes\", \"No\")\n        )\n      ) |&gt; \n      mutate(\n        product_eng = factor(\n          product_eng,\n          levels = c(\"Cassava\", \"Dent corn\", \"Potato\", \"Rice\"),\n          labels = c(\"Cassava\", \"Maize\", \"Potato\", \"Rice\")\n        )\n      )\n  ) +\n  geom_sf_pattern(\n    mapping = aes(\n      pattern = included,\n      pattern_type = included, \n      fill = share\n    ),\n    pattern_fill = \"white\",\n    pattern_alpha = .8\n  ) +\n  # scale_pattern_type_discrete(\"Included\", values = c(\"Yes\" = \"none\", \"No\" = \"stripe\")) +\n  scale_pattern_manual(\"Included\", values = c(\"No\" = \"stripe\", \"Yes\" = \"none\")) +\n  scale_pattern_type_manual(\"Included\", values=c(NA, NA)) +\n  scale_fill_gradient2(\n    \"Share\", low = \"#FFC107\", mid = \"white\", high = \"#009E73\",\n    labels = scales::percent_format()\n  ) +\n  facet_wrap(~product_eng) +\n  theme_map_paper()\np_regions_use_production\n\n\n\n\n\n\n\n\nFigure 6.4: Comparison of Discarded Series with National Production",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "data-desc-stats.html#natural-regions",
    "href": "data-desc-stats.html#natural-regions",
    "title": "6  Descriptive Statistics",
    "section": "6.3 Natural regions",
    "text": "6.3 Natural regions\nLet us now focus on the natural regions: Coast, Forest, Highlands.\nFirst, we load the map (see Section 4.1):\n\nmap_regiones_naturales &lt;-  sf::st_read(\n  str_c(\n    \"../data/raw/shapefile_peru/regiones_naturales/\",\n    \"region natural_geogpsperu_JuanPabloSuyoPomalia.geojson\"\n  ),\n  quiet = TRUE\n)\n\nWe prepare a map:\n\nmap_regiones_naturales &lt;-\n  rmapshaper::ms_simplify(input = as(map_regiones_naturales, 'Spatial')) |&gt; \n  sf::st_as_sf() |&gt; \n  mutate(\n    Natural_region = case_when(\n      Nm_RegNat == \"Costa\" ~ \"Coast\",\n      Nm_RegNat == \"Selva\" ~ \"Forest\",\n      Nm_RegNat == \"Sierra\" ~ \"Highlands\"\n    )\n  )\n\nWe use the following colours for each type of region:\n\ncols &lt;- c(\"Coast\" = \"#56B4E9\", \"Forest\" = \"#009E73\", \"Highlands\" = \"#E69F00\")\n\nFigure 6.5 shows these regions, and adds the grid used with the weather data from Chapter 1 as well as the regional boundaries used in the analysis.\n\n\nCode\nggplot(data = map_regiones_naturales) +\n  geom_sf(mapping = aes(fill = Natural_region), lwd = 0) +\n  scale_fill_manual(values = cols, name = \"Natural region\") +\n  geom_sf(data = map_peru, fill = NA) +\n  geom_sf(data = map_peru_grid_agri, fill = NA, lwd = 0.25) +\n  theme_map_paper()\n\n\n\n\n\n\n\n\nFigure 6.5: Natural Regions in Peru",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "data-desc-stats.html#correlations-between-agricultural-production-and-the-weather",
    "href": "data-desc-stats.html#correlations-between-agricultural-production-and-the-weather",
    "title": "6  Descriptive Statistics",
    "section": "6.4 Correlations Between Agricultural Production and the Weather",
    "text": "6.4 Correlations Between Agricultural Production and the Weather\nLet us compute the correlation between the agricultural production and our various weather variables. Recall from Chapter 5 that the agricultural production is defined as the percent deviation from the monthly trend.\nThe name of the weather variables:\n\nname_weather_variables &lt;- \n  weather_regions_df |&gt; \n  select(where(is.numeric)) |&gt; \n  select(-year, -month) |&gt; \n  colnames()\nname_weather_variables\n\n [1] \"temp_min\"                 \"temp_max\"                \n [3] \"temp_mean\"                \"precip_sum\"              \n [5] \"precip_piscop_sum\"        \"perc_gamma_precip\"       \n [7] \"perc_gamma_precip_piscop\" \"gdd_rice\"                \n [9] \"gdd_maize\"                \"gdd_potato\"              \n[11] \"gdd_cassava\"              \"hdd_rice\"                \n[13] \"hdd_maize\"                \"hdd_potato\"              \n[15] \"hdd_cassava\"              \"temp_min_dev\"            \n[17] \"temp_max_dev\"             \"temp_mean_dev\"           \n[19] \"precip_sum_dev\"           \"precip_piscop_sum_dev\"   \n[21] \"gdd_rice_dev\"             \"gdd_maize_dev\"           \n[23] \"gdd_potato_dev\"           \"gdd_cassava_dev\"         \n[25] \"hdd_rice_dev\"             \"hdd_maize_dev\"           \n[27] \"hdd_potato_dev\"           \"hdd_cassava_dev\"         \n[29] \"cold_surprise_maize\"      \"cold_surprise_rice\"      \n[31] \"cold_surprise_potato\"     \"cold_surprise_cassava\"   \n[33] \"hot_surprise_maize\"       \"hot_surprise_rice\"       \n[35] \"hot_surprise_potato\"      \"hot_surprise_cassava\"    \n[37] \"dry_surprise_maize\"       \"dry_surprise_rice\"       \n[39] \"dry_surprise_potato\"      \"dry_surprise_cassava\"    \n[41] \"wet_surprise_maize\"       \"wet_surprise_rice\"       \n[43] \"wet_surprise_potato\"      \"wet_surprise_cassava\"    \n[45] \"spi_1\"                    \"spi_3\"                   \n[47] \"spi_6\"                    \"spi_12\"                  \n[49] \"spei_1\"                   \"spei_3\"                  \n[51] \"spei_6\"                   \"spei_12\"                 \n[53] \"spi_piscop_1\"             \"spi_piscop_3\"            \n[55] \"spi_piscop_6\"             \"spi_piscop_12\"           \n[57] \"spei_piscop_1\"            \"spei_piscop_3\"           \n[59] \"spei_piscop_6\"            \"spei_piscop_12\"          \n\n\n\n\nCode\ndf |&gt;\n  select(product_eng, y_new, !!!syms(name_weather_variables)) |&gt;\n  na.omit() |&gt;\n  nest(.by = product_eng) |&gt; \n  mutate(\n    correlation = map(\n      data, ~cor(.x) |&gt; \n        data.frame() |&gt; \n        as_tibble(rownames = \"var\")\n    )\n  ) |&gt; \n  select(-data) |&gt; \n  unnest(correlation) |&gt; \n  select(Crop = product_eng, Variable = var, y_new) |&gt; \n  pivot_wider(names_from = Crop, values_from = y_new) |&gt; \n  mutate(across(where(is.numeric), ~round(.x, 2))) |&gt; \n  knitr::kable()\n\n\n\n\nTable 6.3: Correlations between the weather and the agricultural production\n\n\n\n\n\n\nVariable\nCassava\nDent corn\nPotato\nRice\n\n\n\n\ny_new\n1.00\n1.00\n1.00\n1.00\n\n\ntemp_min\n0.33\n0.17\n-0.19\n0.15\n\n\ntemp_max\n0.30\n0.08\n-0.24\n0.11\n\n\ntemp_mean\n0.32\n0.14\n-0.22\n0.13\n\n\nprecip_sum\n0.31\n-0.13\n0.05\n-0.17\n\n\nprecip_piscop_sum\n0.36\n-0.08\n0.06\n-0.12\n\n\nperc_gamma_precip\n0.07\n-0.06\n0.03\n-0.07\n\n\nperc_gamma_precip_piscop\n-0.18\n-0.08\n0.01\n-0.17\n\n\ngdd_rice\n0.32\n0.13\n-0.22\n0.14\n\n\ngdd_maize\n0.32\n0.13\n-0.22\n0.14\n\n\ngdd_potato\n0.32\n0.13\n-0.23\n0.14\n\n\ngdd_cassava\n0.32\n0.13\n-0.23\n0.14\n\n\nhdd_rice\n0.17\n0.01\n-0.04\n-0.02\n\n\nhdd_maize\n0.17\n0.01\n-0.04\n-0.02\n\n\nhdd_potato\n0.04\n-0.01\n-0.01\n-0.01\n\n\nhdd_cassava\n0.04\n-0.01\n-0.01\n-0.01\n\n\ntemp_min_dev\n0.03\n0.01\n0.03\n0.01\n\n\ntemp_max_dev\n-0.06\n-0.05\n0.02\n-0.01\n\n\ntemp_mean_dev\n-0.02\n-0.03\n0.03\n0.00\n\n\nprecip_sum_dev\n0.04\n-0.04\n0.00\n-0.02\n\n\nprecip_piscop_sum_dev\n0.05\n-0.04\n0.00\n-0.02\n\n\ngdd_rice_dev\n-0.01\n-0.01\n0.02\n0.01\n\n\ngdd_maize_dev\n-0.01\n-0.01\n0.02\n0.01\n\n\ngdd_potato_dev\n-0.01\n-0.02\n0.02\n0.00\n\n\ngdd_cassava_dev\n-0.01\n-0.02\n0.02\n0.00\n\n\nhdd_rice_dev\n0.08\n0.02\n0.03\n-0.01\n\n\nhdd_maize_dev\n0.08\n0.02\n0.03\n-0.01\n\n\nhdd_potato_dev\n0.03\n0.00\n0.05\n0.00\n\n\nhdd_cassava_dev\n0.03\n0.00\n0.05\n0.00\n\n\ncold_surprise_maize\n0.02\n0.03\n0.01\n0.04\n\n\ncold_surprise_rice\n0.02\n0.03\n0.01\n0.04\n\n\ncold_surprise_potato\n0.02\n0.03\n0.01\n0.04\n\n\ncold_surprise_cassava\n0.02\n0.03\n0.01\n0.04\n\n\nhot_surprise_maize\n0.08\n0.01\n0.00\n0.00\n\n\nhot_surprise_rice\n0.08\n0.01\n0.00\n0.00\n\n\nhot_surprise_potato\n0.04\n0.00\n0.02\n-0.01\n\n\nhot_surprise_cassava\n0.04\n0.00\n0.02\n-0.01\n\n\ndry_surprise_maize\n-0.13\n-0.06\n0.01\n0.20\n\n\ndry_surprise_rice\n-0.13\n-0.06\n0.01\n0.20\n\n\ndry_surprise_potato\n-0.13\n-0.06\n0.01\n0.20\n\n\ndry_surprise_cassava\n-0.13\n-0.06\n0.01\n0.20\n\n\nwet_surprise_maize\n-0.14\n0.05\n-0.01\n0.08\n\n\nwet_surprise_rice\n-0.14\n0.05\n-0.01\n0.08\n\n\nwet_surprise_potato\n-0.14\n0.05\n-0.01\n0.08\n\n\nwet_surprise_cassava\n-0.14\n0.05\n-0.01\n0.08\n\n\nspi_1\n0.05\n-0.04\n0.02\n-0.02\n\n\nspi_3\n0.06\n-0.04\n0.04\n-0.03\n\n\nspi_6\n0.05\n-0.04\n0.06\n-0.03\n\n\nspi_12\n0.06\n-0.05\n0.05\n-0.05\n\n\nspei_1\n0.05\n-0.04\n0.02\n-0.03\n\n\nspei_3\n0.06\n-0.04\n0.04\n-0.02\n\n\nspei_6\n0.05\n-0.04\n0.06\n-0.02\n\n\nspei_12\n0.06\n-0.05\n0.05\n-0.04\n\n\nspi_piscop_1\n0.00\n-0.03\n-0.02\n0.00\n\n\nspi_piscop_3\n0.01\n-0.03\n0.01\n-0.01\n\n\nspi_piscop_6\n0.03\n-0.02\n0.05\n0.00\n\n\nspi_piscop_12\n0.07\n-0.02\n0.06\n0.00\n\n\nspei_piscop_1\n0.01\n-0.03\n-0.01\n-0.01\n\n\nspei_piscop_3\n0.01\n-0.03\n0.01\n-0.01\n\n\nspei_piscop_6\n0.03\n-0.01\n0.05\n0.01\n\n\nspei_piscop_12\n0.06\n-0.01\n0.05\n0.01",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "data-desc-stats.html#summary-statistics-for-the-local-projection-data",
    "href": "data-desc-stats.html#summary-statistics-for-the-local-projection-data",
    "title": "6  Descriptive Statistics",
    "section": "6.5 Summary Statistics for the Local Projection Data",
    "text": "6.5 Summary Statistics for the Local Projection Data\nThe number of observation in each region and crops (ordered by decreasing values):\n\ndf |&gt; \n  count(product_eng, region_id) |&gt; \n  arrange(n)\n\n# A tibble: 78 × 3\n   product_eng region_id     n\n   &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt;\n 1 Cassava     1           180\n 2 Cassava     2           180\n 3 Cassava     3           180\n 4 Cassava     4           180\n 5 Cassava     5           180\n 6 Cassava     6           180\n 7 Cassava     7           180\n 8 Cassava     9           180\n 9 Cassava     10          180\n10 Cassava     11          180\n# ℹ 68 more rows\n\n\nLet us plot the agricultural production series for each crop in each region.\n\nProduction in levelProduction in percentage deviation\n\n\n\n\nCode\nplots_crops_lp &lt;- vector(mode = \"list\", length = length(crops))\nfor (i_crop in 1:length(crops)) {\n  current_crop &lt;- crops[i_crop]\n  # The series in each region for the current crop\n  p_crop_lp &lt;- \n    ggplot(\n    data = df |&gt; filter(product_eng == !!current_crop),\n    mapping = aes(x = date, y = y_new)\n  ) +\n    geom_line() +\n    facet_wrap(~region, scales = \"free_y\") +\n    labs(\n      title = current_crop, x = NULL,\n      y = \"Production (tons)\") +\n    theme_paper()\n  plots_crops_lp[[i_crop]] &lt;- p_crop_lp\n}\nnames(plots_crops_lp) &lt;- crops\n\n\n\nRiceMaize (Dent corn)PotatoCassava\n\n\n\nprint(plots_crops_lp[[\"Rice\"]])\n\n\n\n\n\n\n\nFigure 6.6: Regional Production of Rice (tons)\n\n\n\n\n\n\n\n\nprint(plots_crops_lp[[\"Dent corn\"]])\n\n\n\n\n\n\n\nFigure 6.7: Regional Production of Maize (tons)\n\n\n\n\n\n\n\n\nprint(plots_crops_lp[[\"Potato\"]])\n\n\n\n\n\n\n\nFigure 6.8: Regional Production of Potato (tons)\n\n\n\n\n\n\n\n\nprint(plots_crops_lp[[\"Cassava\"]])\n\n\n\n\n\n\n\nFigure 6.9: Regional Production of Cassava (tons)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplots_crops_lp &lt;- vector(mode = \"list\", length = length(crops))\nfor (i_crop in 1:length(crops)) {\n  current_crop &lt;- crops[i_crop]\n  # The series in each region for the current crop\n  p_crop_lp &lt;- \n    ggplot(\n    data = df |&gt; filter(product_eng == !!current_crop),\n    mapping = aes(x = date, y = y_dev_pct)\n  ) +\n    geom_line() +\n    facet_wrap(~region, scales = \"free_y\") +\n    labs(\n      title = current_crop, x = NULL,\n      y = \"Percent deviation from regional monthly trend\") +\n    theme_paper()\n  plots_crops_lp[[i_crop]] &lt;- p_crop_lp\n}\nnames(plots_crops_lp) &lt;- crops\n\n\n\nRiceMaize (Dent corn)PotatoCassava\n\n\n\nprint(plots_crops_lp[[\"Rice\"]])\n\n\n\n\n\n\n\nFigure 6.10: Regional Production of Rice (Deviation from Monthly Trend)\n\n\n\n\n\n\n\n\nprint(plots_crops_lp[[\"Dent corn\"]])\n\n\n\n\n\n\n\nFigure 6.11: Regional Production of Maize (Deviation from Monthly Trend)\n\n\n\n\n\n\n\n\nprint(plots_crops_lp[[\"Potato\"]])\n\n\n\n\n\n\n\nFigure 6.12: Regional Production of Potato (Deviation from Monthly Trend)\n\n\n\n\n\n\n\n\nprint(plots_crops_lp[[\"Cassava\"]])\n\n\n\n\n\n\n\nFigure 6.13: Regional Production of Cassava (Deviation from Monthly Trend)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(gtsummary)\ndf |&gt;\n  tbl_summary(\n    include = c(\n      \"product_eng\",\n      # Production\n      \"y_new\", \"y_dev_pct\",\n      # Weather\n      \"temp_max\", \"precip_sum\", \"precip_piscop_sum\",\n      \"temp_max_dev\", \"precip_sum_dev\", \"precip_piscop_sum_dev\",\n      # Control\n      \"rer_hp\", \"r_hp\", \"pi\", \"ind_prod\", \"ONI\", \"price_int_inf\",\n      # Type of region\n      \"share_costa\", \"share_selva\", \"share_sierra\"\n    ),\n    by = product_eng,\n    type = all_continuous() ~ \"continuous2\",\n    statistic = list(\n      all_continuous() ~ c(\"{mean} ({sd})\", \"{median} ({p25}, {p75})\"),\n      all_categorical() ~ \"{n} ({p}%)\"),\n    digits = list(\n      all_continuous() ~ 2,\n      all_categorical() ~ 0\n    )\n  ) |&gt; \n  add_overall(col_label = \"Whole sample\") |&gt; \n  modify_header(label ~ \"**Variable**\") |&gt; \n  modify_spanning_header(\n    c(\"stat_1\", \"stat_2\", \"stat_3\", \"stat_4\") ~ \"**Crop**\"\n  ) |&gt; \n  add_stat_label(\n    label = list(\n      all_continuous() ~ c(\"Mean (SD)\", \"Median (IQR)\"),\n      all_categorical() ~ \"n (%)\"\n    )\n  )\n\n\n\n\nTable 6.4: Descriptive Statistics of the Data Used in the Local Projections\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nWhole sample\nCrop\n\n\nCassava, N = 3,600\nDent corn, N = 4,140\nPotato, N = 3,420\nRice, N = 2,880\n\n\n\n\nMonthly Agricultural Production (tons)\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n9,181.11 (20,854.21)\n4,531.27 (7,109.64)\n4,290.88 (7,254.97)\n16,393.35 (29,574.83)\n13,458.59 (28,312.71)\n\n\n    Median (IQR)\n1,934.00 (218.00, 8,840.35)\n1,798.65 (412.75, 5,619.30)\n983.47 (56.06, 5,648.00)\n5,048.85 (904.75, 20,775.86)\n1,654.00 (45.00, 15,162.07)\n\n\nAgricultural Production (percent deviation from monthly regional values)\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n0.00 (0.96)\n0.00 (0.70)\n0.00 (1.02)\n0.00 (0.77)\n0.00 (1.30)\n\n\n    Median (IQR)\n-0.08 (-0.45, 0.22)\n-0.06 (-0.35, 0.20)\n-0.14 (-0.54, 0.25)\n-0.06 (-0.35, 0.21)\n-0.12 (-0.62, 0.23)\n\n\nMax. Temperature\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n23.43 (4.75)\n24.10 (4.84)\n23.53 (4.78)\n21.47 (3.77)\n24.75 (4.92)\n\n\n    Median (IQR)\n21.85 (19.63, 27.42)\n22.67 (19.97, 29.03)\n21.82 (19.64, 27.85)\n20.56 (18.96, 22.97)\n23.97 (20.47, 29.85)\n\n\nTotal Rainfall (Chirps)\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n70.67 (75.27)\n76.17 (80.04)\n71.58 (77.66)\n52.89 (52.89)\n83.61 (83.91)\n\n\n    Median (IQR)\n47.07 (12.23, 107.66)\n51.28 (13.60, 115.54)\n46.61 (11.72, 109.18)\n33.40 (9.95, 84.62)\n62.44 (16.70, 126.40)\n\n\nTotal Rainfall (Piscop)\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n80.58 (97.55)\n88.55 (104.81)\n81.61 (100.96)\n54.62 (61.77)\n99.98 (110.57)\n\n\n    Median (IQR)\n43.64 (7.98, 122.41)\n50.20 (9.06, 135.91)\n42.42 (7.02, 123.12)\n27.60 (5.49, 88.01)\n65.50 (13.10, 150.77)\n\n\nDeviation of Max. Temperature from Normals\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n0.10 (0.60)\n0.09 (0.59)\n0.10 (0.59)\n0.13 (0.60)\n0.07 (0.60)\n\n\n    Median (IQR)\n0.07 (-0.29, 0.48)\n0.06 (-0.30, 0.47)\n0.08 (-0.28, 0.49)\n0.10 (-0.27, 0.52)\n0.04 (-0.32, 0.45)\n\n\nDeviation of Total Rainfall from Normals (Chirps)\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n2.78 (25.00)\n2.84 (26.24)\n2.90 (25.77)\n2.39 (19.76)\n2.97 (27.71)\n\n\n    Median (IQR)\n-0.17 (-5.87, 9.24)\n-0.12 (-6.31, 10.05)\n-0.19 (-5.83, 9.26)\n-0.20 (-4.57, 7.37)\n-0.10 (-6.85, 10.96)\n\n\nDeviation of Total Rainfall from Normals (Piscop)\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n2.60 (32.56)\n2.81 (34.96)\n2.64 (33.23)\n1.90 (22.51)\n3.11 (38.10)\n\n\n    Median (IQR)\n-0.23 (-7.57, 9.90)\n-0.13 (-8.49, 10.60)\n-0.24 (-7.37, 9.77)\n-0.24 (-5.42, 7.46)\n-0.29 (-10.45, 13.34)\n\n\nReal exchange rate\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n0.00 (1.87)\n0.00 (1.87)\n0.00 (1.87)\n0.00 (1.87)\n0.00 (1.87)\n\n\n    Median (IQR)\n0.11 (-1.38, 1.25)\n0.11 (-1.38, 1.25)\n0.11 (-1.38, 1.25)\n0.11 (-1.38, 1.25)\n0.11 (-1.38, 1.25)\n\n\nInterest rate (pp)\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n0.00 (1.28)\n0.00 (1.28)\n0.00 (1.28)\n0.00 (1.28)\n0.00 (1.28)\n\n\n    Median (IQR)\n0.06 (-0.25, 0.36)\n0.06 (-0.25, 0.36)\n0.06 (-0.25, 0.36)\n0.06 (-0.25, 0.36)\n0.06 (-0.25, 0.36)\n\n\nInflation rate (pp)\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n0.22 (0.26)\n0.22 (0.26)\n0.22 (0.26)\n0.22 (0.26)\n0.22 (0.26)\n\n\n    Median (IQR)\n0.24 (0.03, 0.38)\n0.24 (0.03, 0.38)\n0.24 (0.03, 0.38)\n0.24 (0.03, 0.38)\n0.24 (0.03, 0.38)\n\n\nManufacturing Prod. (pp)\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n-0.16 (5.78)\n-0.16 (5.78)\n-0.16 (5.78)\n-0.16 (5.78)\n-0.16 (5.78)\n\n\n    Median (IQR)\n-0.30 (-3.93, 3.99)\n-0.30 (-3.93, 3.99)\n-0.30 (-3.93, 3.99)\n-0.30 (-3.93, 3.99)\n-0.30 (-3.93, 3.99)\n\n\nOceanic Niño Index\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n0.01 (0.80)\n0.01 (0.80)\n0.01 (0.80)\n0.01 (0.80)\n0.01 (0.80)\n\n\n    Median (IQR)\n-0.10 (-0.43, 0.43)\n-0.10 (-0.43, 0.43)\n-0.10 (-0.43, 0.43)\n-0.10 (-0.43, 0.43)\n-0.10 (-0.43, 0.43)\n\n\nInt. commodity inflation rate\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n0.00 (0.05)\n0.00 (0.03)\n0.00 (0.06)\n0.00 (0.03)\n0.01 (0.06)\n\n\n    Median (IQR)\n0.00 (-0.02, 0.03)\n0.00 (-0.02, 0.02)\n0.00 (-0.03, 0.04)\n0.00 (-0.02, 0.02)\n0.00 (-0.02, 0.02)\n\n\nShare of coastal areas in the region\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n0.27 (0.35)\n0.27 (0.36)\n0.27 (0.35)\n0.28 (0.33)\n0.25 (0.35)\n\n\n    Median (IQR)\n0.00 (0.00, 0.42)\n0.00 (0.00, 0.42)\n0.00 (0.00, 0.42)\n0.11 (0.00, 0.42)\n0.00 (0.00, 0.41)\n\n\nShare of forest areas in the region\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n0.33 (0.39)\n0.38 (0.40)\n0.33 (0.40)\n0.19 (0.27)\n0.47 (0.40)\n\n\n    Median (IQR)\n0.06 (0.00, 0.69)\n0.21 (0.00, 0.73)\n0.05 (0.00, 0.69)\n0.00 (0.00, 0.44)\n0.48 (0.00, 0.87)\n\n\nShare of highland areas in the region\n\n\n\n\n\n\n\n\n\n\n\n\n    Mean (SD)\n0.40 (0.31)\n0.36 (0.32)\n0.40 (0.33)\n0.53 (0.29)\n0.29 (0.25)\n\n\n    Median (IQR)\n0.46 (0.07, 0.58)\n0.38 (0.04, 0.56)\n0.46 (0.05, 0.60)\n0.53 (0.31, 0.71)\n0.24 (0.00, 0.53)",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "data-desc-stats.html#growing-season-duration",
    "href": "data-desc-stats.html#growing-season-duration",
    "title": "6  Descriptive Statistics",
    "section": "6.6 Growing Season Duration",
    "text": "6.6 Growing Season Duration\nLet us have a look at the growing season duration for each crop.\nWe need these additional two packages:\n\nlibrary(readxl)\n\nWe load the dataset with agricultural data (see Chapter 2).\n\nload(\"../data/output/minagri/data_S_TOTAL.rda\")\nload(\"../data/output/minagri/data_SR_TOTAL.rda\")\nload(\"../data/output/df_lp.rda\")\n\nLet us focus on the regions kept in the final data for each crop.\n\nlist_of_regions &lt;- \n  df |&gt; \n  select(product, region) |&gt; \n  unique() |&gt; \n  mutate(keep_data = 1)\n\n\ndata_S_TOTAL &lt;- data_S_TOTAL |&gt; \n  mutate(\n    region = str_replace_all(region, \"á\", \"a\"),\n    region = str_replace_all(region, \"í\", \"i\"),\n    region = str_replace_all(region, \"é\", \"e\"),\n    region = str_replace_all(region, \"ó\", \"o\"),\n    region = str_replace_all(region, \"ú\", \"u\"),\n    region = str_replace_all(region, \"ñ\", \"n\")\n  )\n\n\ndata_SR_TOTAL &lt;- data_SR_TOTAL |&gt; \n  mutate(\n    region = str_replace_all(region, \"á\", \"a\"),\n    region = str_replace_all(region, \"í\", \"i\"),\n    region = str_replace_all(region, \"é\", \"e\"),\n    region = str_replace_all(region, \"ó\", \"o\"),\n    region = str_replace_all(region, \"ú\", \"u\"),\n    region = str_replace_all(region, \"ñ\", \"n\")\n  )\n\n\nharv_cum &lt;- data_SR_TOTAL |&gt; \n  select(-date) |&gt; \n  mutate(month = str_c(\"month\", month)) |&gt; \n  pivot_wider(names_from = month, values_from = Value_surfR) |&gt; \n  mutate(\n    cum_sum1  = month1, \n    cum_sum2  = month2 + cum_sum1, \n    cum_sum3  = month3 + cum_sum2, \n    cum_sum4  = month4 + cum_sum3, \n    cum_sum5  = month5 + cum_sum4, \n    cum_sum6  = month6 + cum_sum5, \n    cum_sum7  = month7 + cum_sum6, \n    cum_sum8  = month8 + cum_sum7, \n    cum_sum9  = month9 + cum_sum8, \n    cum_sum10 = month10 + cum_sum9, \n    cum_sum11 = month11 + cum_sum10, \n    cum_sum12 = month12 + cum_sum11\n  ) |&gt; \n  mutate(\n    perc_cum1   = cum_sum1/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum2   = cum_sum2/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum3   = cum_sum3/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum4   = cum_sum4/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum5   = cum_sum5/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum6   = cum_sum6/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum7   = cum_sum7/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum8   = cum_sum8/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum9   = cum_sum9/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum10  = cum_sum10/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum11  = cum_sum11/ifelse(cum_sum12 == 0, 1, cum_sum12),\n    perc_cum12  = cum_sum12/ifelse(cum_sum12 == 0, 1, cum_sum12)\n  ) |&gt;  \n  select(\n    product, region, year, perc_cum1, perc_cum2, perc_cum3, perc_cum4,\n    perc_cum5, perc_cum6, perc_cum7, perc_cum8, perc_cum9, perc_cum10,\n    perc_cum11, perc_cum12\n  ) |&gt; \n  pivot_longer(\n    cols = c(\n      perc_cum1, perc_cum2, perc_cum3, perc_cum4, perc_cum5, perc_cum6,\n      perc_cum7, perc_cum8, perc_cum9, perc_cum10, perc_cum11, perc_cum12\n    ),\n    names_to = \"month\"\n  ) |&gt; \n  rename(perc_cum_harv_obs = value) |&gt; \n  mutate(perc_cum_harv_obs = perc_cum_harv_obs*100) |&gt; \n  mutate(\n    month = case_when(\n      month == \"perc_cum1\"  ~ 1, \n      month == \"perc_cum2\"  ~ 2, \n      month == \"perc_cum3\"  ~ 3, \n      month == \"perc_cum4\"  ~ 4, \n      month == \"perc_cum5\"  ~ 5, \n      month == \"perc_cum6\"  ~ 6, \n      month == \"perc_cum7\"  ~ 7, \n      month == \"perc_cum8\"  ~ 8, \n      month == \"perc_cum9\"  ~ 9, \n      month == \"perc_cum10\" ~ 10, \n      month == \"perc_cum11\" ~ 11, \n      month == \"perc_cum12\" ~ 12, \n    )\n  ) |&gt; \n  mutate(\n    first = ifelse(perc_cum_harv_obs == 100, 1, 0),\n    first = ifelse(first == 1 & lag(first) == 1, -1, first),\n    perc_cum_harv_obs_first100 = ifelse(first == -1,0,perc_cum_harv_obs)\n  ) |&gt; \n  select(-first)\n\nSelecting the total value (December) as an additional month:\n\ntemp &lt;- data_S_TOTAL |&gt; \n  select(\n    region, product, campaign_plain, month_campaign, Value_surf, campaign\n  ) |&gt;\n  filter(month_campaign == 12) |&gt; \n  mutate(month_campaign = 13) |&gt; \n  rename(surf_m = Value_surf)\n\n\nplan_perc &lt;- data_S_TOTAL |&gt; \n  select(\n    region, product, campaign_plain, month_campaign, surf_m, campaign\n  ) |&gt; \n  rbind(temp) %&gt;%\n  .[with(., order(region, product, campaign, month_campaign) ), ] |&gt; \n  select(-campaign) |&gt; \n  mutate(month_campaign = str_c(\"cum_sum\", month_campaign)) |&gt; \n  unique() |&gt; \n  filter(! product == \"TOTAL\") |&gt; \n  pivot_wider(names_from = month_campaign, values_from = surf_m) |&gt; \n  relocate(\n    region, product, campaign_plain, cum_sum1, cum_sum2, cum_sum3,\n    cum_sum4, cum_sum5, cum_sum6, cum_sum7, cum_sum8,\n    cum_sum9, cum_sum10, cum_sum11, cum_sum12, cum_sum13\n  ) |&gt; \n  rename(total = cum_sum13) |&gt; \n  mutate(\n    perc_cum1  = cum_sum1/total*100,\n    perc_cum2  = cum_sum2/total*100,\n    perc_cum3  = cum_sum3/total*100,\n    perc_cum4  = cum_sum4/total*100,\n    perc_cum5  = cum_sum5/total*100,\n    perc_cum6  = cum_sum6/total*100,\n    perc_cum7  = cum_sum7/total*100,\n    perc_cum8  = cum_sum8/total*100,\n    perc_cum9  = cum_sum9/total*100,\n    perc_cum10 = cum_sum10/total*100,\n    perc_cum11 = cum_sum11/total*100, \n    perc_cum12 = cum_sum12/total*100\n    ) |&gt; \n  select(\n    product, region, campaign_plain, perc_cum1, perc_cum2, perc_cum3,\n    perc_cum4, perc_cum5, perc_cum6, perc_cum7, perc_cum8, perc_cum9, \n    perc_cum10, perc_cum11, perc_cum12\n  ) |&gt; \n  pivot_longer(\n    cols = c(\n      perc_cum1, perc_cum2, perc_cum3, perc_cum4, perc_cum5, perc_cum6, \n      perc_cum7, perc_cum8, perc_cum9, perc_cum10, perc_cum11, perc_cum12\n    ), \n    names_to = \"month_campaign\"\n  ) |&gt; \n  rename(perc_plan_obs = value) |&gt; \n  mutate(perc_plan_obs = perc_plan_obs) |&gt; \n  mutate(\n    month_campaign = case_when(\n      month_campaign == \"perc_cum1\"  ~ 1,\n      month_campaign == \"perc_cum2\"  ~ 2,\n      month_campaign == \"perc_cum3\"  ~ 3,\n      month_campaign == \"perc_cum4\"  ~ 4,\n      month_campaign == \"perc_cum5\"  ~ 5,\n      month_campaign == \"perc_cum6\"  ~ 6,\n      month_campaign == \"perc_cum7\"  ~ 7,\n      month_campaign == \"perc_cum8\"  ~ 8,\n      month_campaign == \"perc_cum9\"  ~ 9,\n      month_campaign == \"perc_cum10\" ~ 10,\n      month_campaign == \"perc_cum11\" ~ 11,\n      month_campaign == \"perc_cum12\" ~ 12,\n    )\n  )\n\n\nplan_perc_cum &lt;- data_S_TOTAL |&gt; \n  select(\n    region, product, campaign_plain, month_campaign, Value_surf, campaign\n  ) %&gt;%\n  .[with(., order(region, product, campaign, month_campaign) ), ] |&gt; \n  select(-campaign) |&gt; \n  mutate(month_campaign = str_c(\"cum_sum\", month_campaign)) |&gt; \n  unique() |&gt; \n  filter(! product == \"TOTAL\") |&gt; \n  pivot_wider(\n    names_from = month_campaign, values_from = Value_surf\n  ) |&gt; \n  relocate(\n    region, product, campaign_plain, \n    cum_sum1, cum_sum2, cum_sum3, cum_sum4, cum_sum5, cum_sum6, \n    cum_sum7, cum_sum8, cum_sum9, cum_sum10, cum_sum11, cum_sum12\n  ) |&gt; \n  mutate(\n    perc_cum1   = cum_sum1/cum_sum12, \n    perc_cum2   = cum_sum2/cum_sum12, \n    perc_cum3   = cum_sum3/cum_sum12, \n    perc_cum4   = cum_sum4/cum_sum12,\n    perc_cum5   = cum_sum5/cum_sum12, \n    perc_cum6   = cum_sum6/cum_sum12, \n    perc_cum7   = cum_sum7/cum_sum12, \n    perc_cum8   = cum_sum8/cum_sum12, \n    perc_cum9   = cum_sum9/cum_sum12, \n    perc_cum10  = cum_sum10/cum_sum12, \n    perc_cum11  = cum_sum11/cum_sum12, \n    perc_cum12  = cum_sum12/cum_sum12\n  ) |&gt; \n  select(\n    product, region, campaign_plain, \n    perc_cum1, perc_cum2, perc_cum3, perc_cum4, perc_cum5, perc_cum6, \n    perc_cum7, perc_cum8, perc_cum9, perc_cum10, perc_cum11, perc_cum12\n  ) |&gt; \n  pivot_longer(\n    cols = c(\n      perc_cum1, perc_cum2, perc_cum3, perc_cum4, perc_cum5, perc_cum6, \n      perc_cum7, perc_cum8, perc_cum9, perc_cum10, perc_cum11, perc_cum12\n    ), \n    names_to = \"month_campaign\"\n  ) |&gt; \n  rename(perc_cum_plan_obs = value) |&gt; \n  mutate(perc_cum_plan_obs = perc_cum_plan_obs*100) |&gt; \n  mutate(\n    month_campaign = case_when(\n      month_campaign == \"perc_cum1\"  ~ 1, \n      month_campaign == \"perc_cum2\"  ~ 2, \n      month_campaign == \"perc_cum3\"  ~ 3, \n      month_campaign == \"perc_cum4\"  ~ 4, \n      month_campaign == \"perc_cum5\"  ~ 5, \n      month_campaign == \"perc_cum6\"  ~ 6, \n      month_campaign == \"perc_cum7\"  ~ 7, \n      month_campaign == \"perc_cum8\"  ~ 8, \n      month_campaign == \"perc_cum9\"  ~ 9, \n      month_campaign == \"perc_cum10\" ~ 10, \n      month_campaign == \"perc_cum11\" ~ 11, \n      month_campaign == \"perc_cum12\" ~ 12, \n    )\n  )\n\nWe load the calendars (see Chapter 2).\n\nload(\"../data/output/Calendario agricola/calendar3.rda\")\nload(\"../data/output/Calendario agricola/calendar4.rda\")\ncalendar3 &lt;- calendar3 |&gt; \n  mutate(\n    region = str_replace_all(region, \"á\", \"a\"),\n    region = str_replace_all(region, \"í\", \"i\"),\n    region = str_replace_all(region, \"é\", \"e\"),\n    region = str_replace_all(region, \"ó\", \"o\"),\n    region = str_replace_all(region, \"ú\", \"u\"),\n    region = str_replace_all(region, \"ñ\", \"n\")\n  )|&gt; \n  mutate(\n    region = toupper(region), \n    product = toupper(product)\n  ) |&gt; \n  rename(\"perc_cum_harv_calend\" = \"perc_cum_harv\")\n\ncalendar4 &lt;- calendar4 |&gt; \n  mutate(\n    region = str_replace_all(region, \"á\", \"a\"),\n    region = str_replace_all(region, \"í\", \"i\"),\n    region = str_replace_all(region, \"é\", \"e\"),\n    region = str_replace_all(region, \"ó\", \"o\"),\n    region = str_replace_all(region, \"ú\", \"u\"),\n    region = str_replace_all(region, \"ñ\", \"n\")\n  ) |&gt; \n  mutate(\n    region = toupper(region), \n    product = toupper(product)\n  ) |&gt; \n  rename(\"perc_cum_plan_calend\" = \"perc_cum_plan\")\n\nNow, we can determine the optimal growth duration from the data.\n\noptimal_growth_duration &lt;- data_SR_TOTAL |&gt; \n  mutate(\n    region = toupper(region), \n    product = toupper(product)\n  ) |&gt;\n  filter(product  %in% c(\"PAPA\",\"MAÍZ AMARILLO DURO\", \"ARROZ CÁSCARA\",  \"YUCA\")) |&gt; \n  full_join(list_of_regions) |&gt; \n  filter(keep_data == 1) |&gt; \n  select(-keep_data)\n\nJoining with `by = join_by(region, product)`\n\n\nLooping over months:\n\nfor (ii in 1:12) {\n  optimal_growth_duration &lt;- optimal_growth_duration |&gt; \n    left_join(\n      data_S_TOTAL |&gt; \n        group_by(region, product) |&gt; \n        select(region,product, date, surf_m) |&gt; \n        mutate(!!as.name(paste(\"surf_lag\", ii, sep=\"\")) := lag(surf_m, ii)) |&gt; \n        select(-surf_m) , \n      by = c(\"region\", \"product\", \"date\")\n    ) |&gt; \n    mutate(\n      !!as.name(paste(\"diff\", ii, sep=\"\")) := \n        !!as.name(paste(\"surf_lag\", ii, sep=\"\")) - Value_surfR\n    )\n}\nrm(ii)\n\n\noptimal_growth_duration_results &lt;- optimal_growth_duration |&gt; \n  unique() |&gt; \n  group_by(region, product) |&gt; \n  mutate(sum_totale = sum(Value_surfR, na.rm = T )) |&gt; \n  ungroup() |&gt; \n  filter(! sum_totale == 0) |&gt; \n  select(- sum_totale) |&gt; \n  filter(! year == 2001) |&gt; \n  group_by(region, product) |&gt; \n  transmute(\n    nb_neg1 = sum(diff1 &lt;0, na.rm = T),\n    nb_neg2 = sum(diff2 &lt;0, na.rm = T),\n    nb_neg3 = sum(diff3 &lt;0, na.rm = T),\n    nb_neg4 = sum(diff4 &lt;0, na.rm = T),\n    nb_neg5 = sum(diff5 &lt;0, na.rm = T),\n    nb_neg6 = sum(diff6 &lt;0, na.rm = T),\n    nb_neg7 = sum(diff7 &lt;0, na.rm = T),\n    nb_neg8 = sum(diff8 &lt;0, na.rm = T),\n    diff1 = mean(abs(diff1), na.rm = T), \n    diff2 = mean(abs(diff2), na.rm = T), \n    diff3 = mean(abs(diff3), na.rm = T), \n    diff4 = mean(abs(diff4), na.rm = T), \n    diff5 = mean(abs(diff5), na.rm = T), \n    diff6 = mean(abs(diff6), na.rm = T), \n    diff7 = mean(abs(diff7), na.rm = T), \n    diff8 = mean(abs(diff8), na.rm = T), \n    diff9 = mean(abs(diff9), na.rm = T), \n    diff10 = mean(abs(diff10), na.rm = T), \n    diff11 = mean(abs(diff11), na.rm = T), \n    diff12 = mean(abs(diff12), na.rm = T)\n  ) |&gt; \n  unique() |&gt; \n  mutate(\n    min_lag = min(\n      diff1,diff2,diff3, diff4, diff5, diff6, \n      diff7, diff8, diff9, diff10, diff11, diff12\n    ), \n    max_neg_values = max(\n      nb_neg1, nb_neg2, nb_neg3, nb_neg4, nb_neg5, nb_neg6, nb_neg7, nb_neg8\n    )\n  ) |&gt; \n  ungroup() |&gt; \n  filter(! is.nan(min_lag)) |&gt; \n  mutate(\n    best_lag = case_when(\n      diff1 == min_lag ~ as.numeric(13), \n      diff2 == min_lag ~ as.numeric(14), \n      diff3 == min_lag ~ as.numeric(3), \n      diff4 == min_lag ~ as.numeric(4), \n      diff5 == min_lag ~ as.numeric(5), \n      diff6 == min_lag ~ as.numeric(6), \n      diff7 == min_lag ~ as.numeric(7), \n      diff8 == min_lag ~ as.numeric(8), \n      diff9 == min_lag ~ as.numeric(9), \n      diff10 == min_lag ~ as.numeric(10), \n      diff11 == min_lag ~ as.numeric(11), \n      diff12 == min_lag ~ as.numeric(12), \n    ),\n  worse_neg_values = case_when(\n    nb_neg1 == max_neg_values ~ as.numeric(1),\n    nb_neg2 == max_neg_values ~ as.numeric(2), \n    nb_neg3 == max_neg_values ~ as.numeric(3),\n    nb_neg4 == max_neg_values ~ as.numeric(4),\n    nb_neg5 == max_neg_values ~ as.numeric(5), \n    nb_neg6 == max_neg_values ~ as.numeric(6), \n    nb_neg7 == max_neg_values ~ as.numeric(7), \n    nb_neg8 == max_neg_values ~ as.numeric(8)), \n  bad_result = ifelse(best_lag == worse_neg_values, 1,0)\n  )\n\n\nsummary_opt_growth_duration &lt;- optimal_growth_duration_results |&gt; \n  filter(! best_lag == 1) |&gt; \n  select(region, product, max_neg_values, best_lag) |&gt; \n  group_by(product) |&gt; \n  transmute(\n    mean_best_lag = mean(best_lag),\n    med_best_lag  = median(best_lag), \n    sd_best_lag   = sd(best_lag), \n    min_best_lag  = min(best_lag), \n    max_best_lag  = max(best_lag), \n    nb_regions    = n()\n  ) |&gt; \n  ungroup() |&gt; \n  mutate(\n    product_eng = case_when(\n      product == \"ARROZ CÁSCARA\"      ~ \"Rice\",\n      product == \"MAÍZ AMARILLO DURO\" ~ \"Maize\",\n      product == \"PAPA\"               ~ \"Potato\", \n      product == \"YUCA\"               ~ \"Cassava\", \n      TRUE ~ \"delete\")\n  ) |&gt; \n  unique() |&gt; \n  select(- product) |&gt; \n  relocate(product_eng) |&gt; \n  arrange(product_eng)\n\n\nknitr::kable(summary_opt_growth_duration, digits = 2)\n\n\n\nTable 6.5: Calculated growing season duration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_eng\nmean_best_lag\nmed_best_lag\nsd_best_lag\nmin_best_lag\nmax_best_lag\nnb_regions\n\n\n\n\nCassava\n9.00\n9\n2.96\n3\n14\n20\n\n\nMaize\n5.17\n5\n1.64\n4\n12\n23\n\n\nPotato\n5.79\n6\n1.08\n4\n8\n19\n\n\nRice\n4.44\n4\n0.73\n3\n6\n16",
    "crumbs": [
      "I. Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "local_projections_piscop.html",
    "href": "local_projections_piscop.html",
    "title": "7  The Dynamic Effects of Weather Shocks",
    "section": "",
    "text": "7.1 Linear Local Projections\nIn this section, we focus on estimating the Local Projections (Jordà 2005) to quantify the impact of weather on agricultural production. We use panel data, similar to the approach used in the study by Acevedo et al. (2020), and independently estimate models for each specific crop.\nFor a particular crop denoted as \\(c\\), the model can be expressed as follows: \\[\n\\begin{aligned}\n\\underbrace{y_{c,i,{\\color{wongGold}t+h}}}_{\\text{Production}} = &  {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}T}}} {\\color{wongPurple}{T_{i,{\\color{wongGold}t}}}} + {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}P}}} {\\color{wongPurple}P_{i,{\\color{wongGold}t}}}\\\\\n        &+\\gamma_{c,i,h}\\underbrace{X_{t}}_{\\text{controls}}  + \\underbrace{\\zeta_{c,i,h} \\text{Trend}_{t} \\times \\text{Month}_t + \\eta_{c,i,h} \\text{Trend}^2_{t} \\times \\text{Month}_t}_{\\text{regional monthly trend}} + \\varepsilon_{c,i,t+h}\n\\end{aligned}\n\\tag{7.1}\\]\nHere, \\(i\\) represents the region, \\(t\\) represents the time, and \\(h\\) represents the horizon. The primary focus lies on estimating the coefficients associated with temperature and precipitation for different time horizons \\(\\color{wongGold}h=\\{0,1,...,T_{c}\\}\\)\nNote that we allow a crop regional monthly specific quadratic trend to be estimated.",
    "crumbs": [
      "II. Local Projections",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Dynamic Effects of Weather Shocks</span>"
    ]
  },
  {
    "objectID": "local_projections_piscop.html#sec-lp-linear-piscop",
    "href": "local_projections_piscop.html#sec-lp-linear-piscop",
    "title": "7  The Dynamic Effects of Weather Shocks",
    "section": "",
    "text": "7.1.1 Functions\nTo estimate the models, we develop a function, get_data_lp(), that generates the endogenous variable and the regressors for a specific crop, considering a given time horizon. This function is designed to return a list where each element represents the dataset that will be used for estimating the model corresponding to a specific time horizon.\nWhen we call the get_data_lp() function, we check for missing values in the weather data. If missing values are present for a specific region and crop, we keep only the longest consecutive sequence without missing values. To achieve this, we use the two functions defined previously: get_index_longest_non_na() and keep_values_longest_non_na().\n\n#' Get the data in a table for the local projections, for a specific crop\n#'\n#' @param df original dataset\n#' @param horizons number of horizons\n#' @param y_name name of the exogenous variable\n#' @param group_name name of the group variable\n#' @param crop_name name of the crop to focus on\n#' @param control_names vector of names of the control variables\n#' @param weather_names vector of names of the weather variables\n#' @param add_month_fe should columns with month dummy variables be added?\n#'   Default to `TRUE`\n#' @param share_geo vector of names of the variables that contain the share of\n#'   each type of geographical pattern. By default `NULL`: no share used\n#' @param transition_name name of the variable used to define the transition to\n#'   the two states. By default `NULL`\n#' @param transition_method if transition function, name of the method to use:\n#'   `logistic` or `normal` (default to `NULL`, i.e., no transition)\n#' @param state_names name of the two states in a vector of characters (only if\n#'   `transition_name` is not `NULL`). First period corresponds to mapped values\n#'   of `transition_name` close to 0, which is for large positive values of\n#'   `transition_name`\n#' @param gamma logistic growth rate (default to 3, only used if\n#'   `transition_name` is not `NULL`)\n#' @param other_var_to_keep vector of names of other variables to keep (default\n#'   to `NULL`: no additional vairable kept)\n#' @export\n#' @importFrom dplyr filter select mutate sym group_by across rowwise arrange\n#'   slice lead ends_with\n#' @importFrom fastDummies dummy_cols\n#' @importFrom stringr str_c str_detect\nget_data_lp &lt;- function(df,\n                        horizons,\n                        y_name,\n                        group_name,\n                        crop_name,\n                        control_names,\n                        weather_names,\n                        add_month_fe = TRUE,\n                        share_geo = NULL,\n                        transition_name = NULL,\n                        transition_method = NULL,\n                        state_names = c(\"planted\", \"harvested\"),\n                        gamma = 3,\n                        other_var_to_keep = NULL) {\n\n  if (!is.null(share_geo) & !is.null(transition_name)) {\n    stop(\"You can only use one between share_geo and transition_name\")\n  }\n\n  # Init empty object to return: list of length horizons\n  df_horizons &lt;- vector(mode = \"list\", length = horizons + 1)\n\n  # Keep only the variables needed\n  df_focus &lt;-\n    df |&gt;\n    filter(product_eng == !!crop_name) |&gt;\n    select(\n      !!y_name,\n      !!group_name,\n      date,\n      product_eng,\n      !!!control_names,\n      !!!weather_names,\n      !!!share_geo,\n      !!transition_name,\n      !!other_var_to_keep\n    ) |&gt;\n    mutate(\n      !!group_name := factor(!!sym(group_name))\n    )\n\n  # Month dummy fixed-effects\n  if (add_month_fe) {\n    df_focus &lt;- df_focus |&gt;\n      mutate(\n        month = as.character(lubridate::month(date))\n      ) |&gt;\n      dummy_cols(select_columns = \"month\", remove_first_dummy = FALSE)\n  }\n\n\n  # For each region, only keep the longest sequence of non NA values found in\n  # the weather variables\n  df_focus &lt;-\n    df_focus |&gt;\n    group_by(region_id) |&gt;\n    mutate(\n      across(\n        .cols  = !!weather_names,\n        .fns   = keep_values_longest_non_na,\n        .names = \"{.col}_keep\"\n      )\n    ) |&gt;\n    rowwise() |&gt;\n    mutate(keep_cols = all(across(ends_with(\"_keep\")))) |&gt;\n    ungroup() |&gt;\n    filter(keep_cols) |&gt;\n    select(-keep_cols, -!!paste0(weather_names, \"_keep\"))\n\n  if (!is.null(share_geo)) {\n    # For each geographical type, multiply the weather variables by the share\n    # that the geo. type represents\n    for(share_geo_type in share_geo) {\n      df_focus &lt;-\n        df_focus |&gt;\n        mutate(\n          across(\n            .cols  = !!weather_names,\n            .fns   = ~ .x * !!sym(share_geo_type),\n            .names = str_c(\"{.col}_\", share_geo_type)\n          )\n        )\n    }\n  }\n\n  if (!is.null(transition_name)) {\n\n    state_1_name &lt;- state_names[1]\n    state_2_name &lt;- state_names[2]\n\n    if (transition_method == \"logistic\") {\n      df_focus &lt;-\n        df_focus |&gt;\n        mutate(\n          fz = logist(!!sym(transition_name), gamma = gamma)\n        )\n    } else if (transition_method == \"normal\") {\n      df_focus &lt;-\n        df_focus |&gt;\n        mutate(\n          fz = pnorm(-!!sym(transition_name))\n        )\n    } else {\n      stop(\"transition method must be either \\\"losistic\\\" or \\\"normal\\\"\")\n    }\n    df_focus &lt;- df_focus |&gt;\n      dummy_cols(group_name, remove_first_dummy = FALSE)\n\n    ind_dummies_group &lt;- str_detect(colnames(df_focus), str_c(group_name, \"_\"))\n    dummies_group_name &lt;- colnames(df_focus)[ind_dummies_group]\n\n    if (add_month_fe) {\n      ind_dummies_month &lt;- str_detect(colnames(df_focus), \"^month_\")\n      dummies_month_name &lt;- colnames(df_focus)[ind_dummies_month]\n      dummies_group_name &lt;- c(dummies_group_name, dummies_month_name)\n    }\n\n    df_focus &lt;-\n      df_focus |&gt;\n      mutate(\n        # First regime:\n        across(\n          .cols  = c(!!!control_names, !!!weather_names, !!!dummies_group_name),\n          .fns   = list(\n            state_1_name = ~ (1 - fz) * .x,\n            state_2_name = ~ fz * .x\n          ),\n          .names = \"{fn}_{col}\"\n        )\n      ) |&gt;\n      rename_with(\n        .fn = ~str_replace(string = .x, pattern = \"state_1_name\", replacement = state_1_name),\n        .cols = starts_with(\"state_1_name\")\n      ) |&gt;\n      rename_with(\n        .fn = ~str_replace(string = .x, pattern = \"state_2_name\", replacement = state_2_name),\n        .cols = starts_with(\"state_2_name\")\n      )\n  } else {\n    df_focus &lt;-\n      df_focus |&gt;\n      dummy_cols(group_name, remove_first_dummy = FALSE)\n  }\n\n\n  # Prepare the values for y at t+h\n  for (h in 0:horizons) {\n    df_horizons[[h+1]] &lt;-\n      df_focus |&gt;\n      group_by(!!sym(group_name)) |&gt;\n      arrange(date) |&gt;\n      mutate(time = row_number()) |&gt;\n      mutate(y_lead = dplyr::lead(!!sym(y_name), n = h)) |&gt;\n      slice(1:(n()-h)) |&gt;\n      ungroup()\n  }\n  names(df_horizons) &lt;- 0:horizons\n  df_horizons\n}\n\nFollowing the data preparation step, we proceed to define a function that performs the estimation of models for all time horizons. This function uses the datasets obtained through the get_data_lp() function.\n\n#' Estimate Local Projections\n#'\n#' @param df original dataset\n#' @param horizons number of horizons\n#' @param y_name name of the exogenous variable\n#' @param group_name name of the group variable\n#' @param crop_name name of the crop to focus on\n#' @param control_names vector of names of the control variables\n#' @param weather_names vector of names of the weather variables\n#' @param detrend if `TRUE` a group-wise quadratic temporal effect is estimated\n#'  (group:time + group:I(time^2))\n#' @param add_month_fe should columns with month dummy variables be added?\n#'   Default to `TRUE`\n#' @param add_intercept should an intercept we added to the regressions?\n#'   (default to `FALSE`)\n#' @param share_geo vector of names of the variables that contain the share of\n#'   each type of geographical pattern. By default `NULL`: no share used\n#' @param std type of standard error (`\"NW\"` for Newey-West, `\"Cluster\"`,\n#'   `\"Standard\"` otherwise)\n#' @param transition_name name of the variable used to define the transition to\n#'   the two states. By default `NULL`\n#' @param transition_method if transition function, name of the method to use:\n#'   `logistic` or `normal` (default to `NULL`, i.e., no transition)\n#' @param state_names name of the two states in a vector of characters (only if\n#'   `transition_name` is not `NULL`). First period corresponds to mapped values\n#'   of `transition_name` close to 0, which is for large positive values of\n#'   `transition_name`\n#' @param gamma logistic growth rate (default to 3, only used if\n#'   `transition_name` is not `NULL`)\n#' @param other_var_to_keep vector of names of other variables to keep in the\n#'   returned dataset (default to `NULL`: no additional vairable kept)\n#' @export\n#' @importFrom dplyr mutate sym ungroup summarise across left_join\n#' @importFrom stringr str_c str_detect\n#' @importFrom purrr map map_dbl list_rbind\n#' @importFrom tibble enframe\n#' @importFrom tidyr pivot_longer\n#' @importFrom sandwich NeweyWest\n#' @importFrom stats sd model.matrix nobs residuals lm coef\nestimate_linear_lp &lt;- function(df,\n                              horizons,\n                              y_name,\n                              group_name,\n                              crop_name,\n                              control_names,\n                              weather_names,\n                              detrend = TRUE,\n                              add_month_fe = TRUE,\n                              add_intercept = FALSE,\n                              share_geo = NULL,\n                              transition_name = NULL,\n                              transition_method = NULL,\n                              state_names = c(\"planted\", \"harvested\"),\n                              gamma = 3,\n                              std = c(\"nw\", \"cluster\", \"standard\"),\n                              other_var_to_keep = NULL) {\n\n  # Format the dataset\n  data_lp &lt;-\n    get_data_lp(\n      df = df,\n      horizons = horizons,\n      y_name = y_name,\n      group_name = group_name,\n      crop_name = crop_name,\n      control_names = control_names,\n      weather_names = weather_names,\n      share_geo = share_geo,\n      transition_name = transition_name,\n      transition_method = transition_method,\n      state_names = state_names,\n      gamma = gamma,\n      other_var_to_keep = other_var_to_keep\n    )\n\n  # Recode levels for the groups\n  for(h in 0:horizons){\n    data_lp[[h + 1]] &lt;-\n      data_lp[[h + 1]] |&gt;\n      mutate(\n        !!group_name := as.factor(as.character(!!sym(group_name)))\n      )\n  }\n\n  control_names_full &lt;- control_names\n  weather_names_full &lt;- weather_names\n  ind_names_groups &lt;- str_detect(\n    colnames(data_lp[[1]]), str_c(\"^\", group_name, \"_\")\n  )\n  group_names_full &lt;- colnames(data_lp[[1]])[ind_names_groups]\n\n  if (!is.null(share_geo)) {\n    # Name of the weather variables\n    weather_names_full &lt;- paste(\n      rep(weather_names, each = length(share_geo)),\n      share_geo,\n      sep = \"_\"\n    )\n  }\n\n  if (!is.null(transition_name)) {\n\n    state_1_name &lt;- str_c(state_names[1], \"_\")\n    state_2_name &lt;- str_c(state_names[2], \"_\")\n\n    # Name of the variables\n    weather_names_full &lt;- str_c(\n      rep(\n        c(state_1_name, state_2_name),\n        each = length(weather_names)\n      ),\n      rep(weather_names, 2)\n    )\n    control_names_full &lt;- str_c(\n      rep(\n        c(state_1_name, state_2_name),\n        each = length(control_names)\n      ),\n      rep(control_names, 2)\n    )\n    ind_names_groups &lt;- str_detect(\n      colnames(data_lp[[1]]),\n      str_c(\"(^\", state_1_name, \"|\", state_2_name, \")\", group_name, \"_\")\n    )\n    group_names_full &lt;- colnames(data_lp[[1]])[ind_names_groups]\n  }\n\n  # Observed standard deviations in the data\n  sd_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = sd\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"std_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n  # Observed median value in the data\n  median_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = ~quantile(.x, probs = .5)\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"median_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n  # Observed quantile of order 0.05 value in the data\n  q05_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = ~quantile(.x, probs = .05)\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"q05_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n  # Observed quantile of order 0.95 value in the data\n  q95_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = ~quantile(.x, probs = .95)\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"q95_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n  if (detrend == TRUE) {\n    formula_lp &lt;- paste0(\n      \"y_lead\",\n      \" ~ -1+\",\n      # \" ~ 1+\", # intercept\n      paste(weather_names_full, collapse = \" + \"),\n      \" + \",\n      paste(control_names_full, collapse = \" + \"),\n      \" + \",\n      # ifelse(\n      #   add_intercept,\n      #   # removing last group\n      #   yes = paste(group_names_full[-length(group_names_full)], collapse = \" + \"),\n      #   # keeping last group\n      #   no = paste(group_names_full, collapse = \" + \")\n      # ),\n      # \" + \",\n      # Group-specific trend\n      paste0(\n        c(\n          paste0(group_names_full, \":time:factor(month)\"),\n          paste0(group_names_full, \":I(time^2):factor(month)\")\n        ),\n        collapse = \" + \"\n      )\n      # \" + \",\n      # # Group-specific Calendar month effect\n      # paste0(\n      #   paste0(group_names_full, \":factor(month)\"),\n      #   collapse = \" + \"\n      # )\n    )\n  } else {\n    # Formula for the regressions\n    formula_lp &lt;- paste0(\n      \"y_lead\",\n      \" ~ -1+\",\n      # \" ~ 1+\", # intercept\n      paste(weather_names_full, collapse = \" + \"),\n      \" + \",\n      paste(control_names_full, collapse = \" + \"),\n      \" + \",\n      ifelse(\n        add_intercept,\n        # removing last group\n        yes = paste(group_names_full[-length(group_names_full)], collapse = \" + \"),\n        # keeping last group\n        no = paste(group_names_full, collapse = \" + \")\n      )\n    )\n  }\n\n  if (add_month_fe) {\n    formula_lp &lt;- paste0(\n      formula_lp, \" + \", paste(paste0(\"month_\", 1:11), collapse = \" + \")\n    )\n  }\n\n  empty_res &lt;- vector(mode = \"list\", length = horizons + 1)\n  reg_lp &lt;- empty_res\n  sig_ols &lt;- empty_res\n  log_likelihood &lt;- empty_res\n  mse &lt;- empty_res\n  coefs &lt;- empty_res\n  cl_std &lt;- empty_res\n\n  for (h in 0:horizons) {\n    # Global assignment... otherwise, errors with coeftest()\n    current_data_h &lt;&lt;- data_lp[[h+1]]\n    # Regression\n    reg_h &lt;- lm(formula = formula_lp, data = current_data_h)\n    # Standard error of the residuals\n    sig_ols_h &lt;- sd(reg_h$residuals)\n    # Log likelihood\n    u_h &lt;- reg_h$residuals\n    log_likelihood_h &lt;-\n      sum(log(1 / sqrt(2 * pi * sig_ols_h^2) * exp(-u_h^2 / (2 * sig_ols_h^2))))\n    mse_h &lt;- mean(u_h^2)\n    coefs_h &lt;- enframe(coef(reg_h)) |&gt; mutate(horizon = h)\n    if (std == \"Cluster\") {\n      cl_std_h &lt;- coeftest(\n        reg_h,\n        vcov = vcovCL,\n        cluster = formula(str_c(\"~\", group_name)))[, \"Std. Error\"] |&gt;\n        enframe() |&gt;\n        mutate(horizon = h)\n    } else {\n      cl_std_h &lt;- enframe(sqrt(diag(vcov(reg_h))), value = \"std\") |&gt;\n        mutate(horizon = h)\n    }\n\n    # Store results in lists\n    reg_lp[[h+1]] &lt;- reg_h\n    sig_ols[[h+1]] &lt;- sig_ols_h\n    log_likelihood[[h+1]] &lt;- log_likelihood_h\n    mse[[h+1]] &lt;- mse_h\n    coefs[[h+1]] &lt;- coefs_h\n    cl_std[[h+1]] &lt;- cl_std_h\n  }\n\n  se_df &lt;- list_rbind(cl_std) |&gt; rename(std = value)\n\n  coefs &lt;-\n    coefs |&gt; list_rbind() |&gt;\n    left_join(se_df, by = c(\"horizon\", \"name\")) |&gt;\n    mutate(\n      crop = crop_name,\n      horizon = as.numeric(horizon)\n    ) |&gt;\n    left_join(sd_weather_shock, by = c(\"horizon\", \"name\")) |&gt;\n    left_join(median_weather_shock, by = c(\"horizon\", \"name\")) |&gt;\n    left_join(q05_weather_shock, by = c(\"horizon\", \"name\")) |&gt;\n    left_join(q95_weather_shock, by = c(\"horizon\", \"name\"))\n\n  list(\n    # reg_lp = reg_lp,\n    coefs = coefs,\n    horizons = horizons,\n    log_likelihood = log_likelihood |&gt; list_c(),\n    mse = mse |&gt; list_c(),\n    crop_name = crop_name,\n    data_lp = data_lp\n  )\n}\n\nTo estimate the clustered standard errors, we use the coeftest() function from {lmtest}. The covariance matrix estimation is performed using the vcovCL() from {sandwich}.\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(sandwich)\n\n\n\n\n\n\n\nNote\n\n\n\nThe get_data_lp() function is defined in the R script saved here: /weatherperu/R/format_data.R. The estimation function, estimate_linear_lp(), is defined in the R script saved here: /weatherperu/R/estimations.R.\n\nsource(\"../weatherperu/R/estimations.R\")\n\n\n\n\n\n7.1.2 Estimation\nTo loop over the different crops, we can use the map() function. This function enables us to apply the estimate_linear_lp() function to each crop iteratively, facilitating the estimation process.\n\ncrops &lt;- df$product_eng |&gt; unique()\nweather_variables &lt;- c(\"temp_max_dev\", \"precip_piscop_sum_dev\")\ncontrol_variables &lt;- c(\n  \"rer_hp\", \"r_hp\", \"pi\", \"ind_prod\", \"ONI\", \"price_int_inf\"\n)\n\nThe estimation (this code takes about a minute to run, we load results in this notebook):\n\nresul_lp &lt;- vector(mode = \"list\", length = length(crops))\nfor (i_crop in 1:length(crops)) {\n  resul_lp[[i_crop]] &lt;- estimate_linear_lp(\n    df,\n    horizons = 14,\n    y_name = \"y_new_normalized\",\n    group_name = \"region_id\",\n    detrend = TRUE,\n    add_month_fe = FALSE,\n    add_intercept = FALSE,\n    crop_name = crops[i_crop],\n    control_names = control_variables,\n    weather_names = weather_variables,\n    std = \"Cluster\",\n    other_var_to_keep = \"y_new\"\n  )\n}\nsave(resul_lp, file = \"../R/output/resul_lp_piscop.rda\")\n\n\nload(\"../R/output/resul_lp_piscop.rda\")\n\n\n\n7.1.3 Results\nWe can visualize the Impulse Response Functions (IRFs) by plotting the estimated coefficients associated with the weather variables. These coefficients represent the impact of weather on agricultural production and can provide valuable insights into the dynamics of the system. By plotting the IRFs, we can gain a better understanding of the relationship between weather variables and the response of agricultural production over time.\nThe data for the graphs:\n\ndf_irfs_lp &lt;- map(resul_lp, \"coefs\") |&gt; \n  list_rbind() |&gt; \n  filter(name %in% weather_variables) |&gt; \n  mutate(\n    shock_1_sd = value * std_shock,\n    lower_95 = (value - qnorm(0.975) * std) * std_shock,\n    upper_95 = (value + qnorm(0.975) * std) * std_shock,\n    lower_68 = (value - qnorm(0.84)  * std) * std_shock,\n    upper_68 = (value + qnorm(0.84)  * std) * std_shock\n  ) |&gt; \n  mutate(\n    crop = factor(\n      crop, \n      levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"))\n  ) |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = c(\n        \"temp_max_dev\",\n        \"precip_piscop_sum_dev\"\n      ),\n      labels = c(\n        \"Temp. anomalies\", \n        \"Precip. anomalies\"\n      )\n    )\n  )\n\nFor the confidence intervals:\n\ndf_irfs_lp_ci &lt;- \n  df_irfs_lp |&gt; \n  select(horizon, crop, name, matches(\"^(lower)|^(upper)\", perl = TRUE)) |&gt; \n  pivot_longer(\n    cols = matches(\"^(lower)|^(upper)\", perl = TRUE),\n    names_pattern = \"(.*)_(95|68)$\",\n    names_to = c(\".value\", \"level\")\n  ) |&gt; \n  mutate(level = str_c(level, \"%\"))\n\n\nnb_h &lt;- 14\n\n# Duration of the growing season\ngs_duration_df &lt;- tribble(\n  ~crop, ~tc,\n  \"Rice\", 4,\n  \"Maize\", 5,\n  \"Potato\", 6,\n  \"Cassava\", 9\n)\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs_lp |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(x = horizon, y = shock_1_sd),\n    colour = \"#0072B2\") +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  geom_vline(\n    data = gs_duration_df, \n    mapping = aes(xintercept = tc),\n    colour = \"#D55E00\", linetype = \"dashed\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", \n    axes = \"all\",\n    independent = \"y\", switch = \"y\") +\n  scale_x_continuous(breaks = seq(0, nb_h, by = 2)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"gray10\", \"95%\" = \"gray60\")\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\nFigure 7.1: Agricultural production response to a weather shock\n\n\n\n\n\n\n\n7.1.4 Exporting results\nLet us save the results for later use.\n\nsave(df_irfs_lp, df_irfs_lp_ci, file = \"../R/output/df_irfs_lp_piscop.rda\")\nsave(resul_lp, file = \"../R/output/resul_lp_piscop.rda\")\n\nWe also save the dataset in a CSV format:\n\ndf |&gt; \n  select(\n    date, region, region_id, product, product_eng, \n    y_new, y_new_normalized,\n    temp_max_dev, precip_piscop_sum_dev,\n    rer_hp, r_hp, pi, ind_prod, ONI, price_int_inf\n    ) |&gt; \n  write_csv(\"../data/output/data_lp.csv\")\n\n\n\n\n\nAcevedo, Sebastian, Mico Mrkaic, Natalija Novta, Evgenia Pugacheva, and Petia Topalova. 2020. “The Effects of Weather Shocks on Economic Activity: What Are the Channels of Impact?” Journal of Macroeconomics 65: 103207.\n\n\nJordà, Òscar. 2005. “Estimation and Inference of Impulse Responses by Local Projections.” American Economic Review 95 (1): 161–82. https://doi.org/10.1257/0002828053828518.",
    "crumbs": [
      "II. Local Projections",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Dynamic Effects of Weather Shocks</span>"
    ]
  },
  {
    "objectID": "local_projections_quadratic_piscop.html",
    "href": "local_projections_quadratic_piscop.html",
    "title": "8  Quadratic Terms",
    "section": "",
    "text": "8.1 Linear Local Projections\nIn this section, we focus on estimating the Local Projections (Jordà 2005) to quantify the impact of weather on agricultural production. We use panel data, similar to the approach used in the study by Acevedo et al. (2020), and independently estimate models for each specific crop.\nFor a particular crop denoted as \\(c\\), the model can be expressed as follows: \\[\n\\begin{aligned}\n\\underbrace{y_{c,i,{\\color{wongGold}t+h}}}_{\\text{Production}} = & {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}T}}} {\\color{wongPurple}{T_{i,{\\color{wongGold}t}}}} +\n{\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}T^2}}} {\\color{wongPurple}{T^2_{i,{\\color{wongGold}t}}}} +\n{\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}P}}} {\\color{wongPurple}P_{i,{\\color{wongGold}t}}} +\n{\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}P^2}}} {\\color{wongPurple}P^2_{i,{\\color{wongGold}t}}}\\\\\n        &+\\gamma_{c,i,h}\\underbrace{X_{t}}_{\\text{controls}}  + \\underbrace{\\zeta_{c,i,h} \\text{Trend}_{t}\\times \\text{Month}_t + \\eta_{c,i,h} \\text{Trend}^2_{t} \\times \\text{Month}_t}_{\\text{regional monthly trend}}  +\\varepsilon_{c,i,t+h}\n\\end{aligned}\n\\tag{8.1}\\]\nHere, \\(i\\) represents the region, \\(t\\) represents the time, and \\(h\\) represents the horizon. The primary focus lies on estimating the coefficients associated with temperature and precipitation for different time horizons \\(\\color{wongGold}h=\\{0,1,...,T_{c}\\}\\)\nThe squared weather variables are added to the dataset.\ndf &lt;- \n  df |&gt; \n  mutate(\n    temp_max_dev_sq = temp_max_dev^2,\n    precip_piscop_sum_dev_sq = precip_piscop_sum_dev^2\n  )",
    "crumbs": [
      "II. Local Projections",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quadratic Terms</span>"
    ]
  },
  {
    "objectID": "local_projections_quadratic_piscop.html#sec-lp-quadratic-linear",
    "href": "local_projections_quadratic_piscop.html#sec-lp-quadratic-linear",
    "title": "8  Quadratic Terms",
    "section": "",
    "text": "8.1.1 Estimation\nTo loop over the different crops, we can use the map() function. This function enables us to apply the estimate_linear_lp() function to each crop iteratively, facilitating the estimation process.\n\ncrops &lt;- df$product_eng |&gt; unique()\nweather_variables &lt;- c(\n  \"temp_max_dev\", \"precip_piscop_sum_dev\",\n  \"temp_max_dev_sq\", \"precip_piscop_sum_dev_sq\"\n)\ncontrol_variables &lt;- c(\"rer_hp\", \"r_hp\", \"pi\", \"ind_prod\", \"ONI\", \"price_int_inf\")\n\nThe estimation (this code takes about a minute to run, we load results in this notebook):\n\nlibrary(lmtest)\nlibrary(sandwich)\nresul_lp_quad &lt;- map(\n  crops, ~ estimate_linear_lp(\n    df = df,\n    horizons = 14,\n    y_name = \"y_new_normalized\",\n    group_name = \"region_id\",\n    detrend = TRUE,\n    add_month_fe = FALSE,\n    add_intercept = FALSE,\n    crop_name = .,\n    control_names = control_variables,\n    weather_names = weather_variables,\n    std = \"Cluster\",\n    other_var_to_keep = \"y_new\"\n  )\n)\nsave(resul_lp_quad, file = \"../R/output/resul_lp_quad.rda\")\n\n\nload(\"../R/output/resul_lp_quad.rda\")\n\n\n\n8.1.2 Results\nWe can visualize the Impulse Response Functions (IRFs) by plotting the estimated coefficients associated with the weather variables. These coefficients represent the impact of weather on agricultural production and can provide valuable insights into the dynamics of the system. By plotting the IRFs, we can gain a better understanding of the relationship between weather variables and the response of agricultural production over time.\nThe data for the graphs:\n\ndf_irfs_lp_quad &lt;- \n  map(resul_lp_quad, \"coefs\") |&gt; \n  list_rbind() |&gt; \n  filter(name %in% weather_variables) |&gt; \n  mutate(\n    order = ifelse(str_detect(name, \"_sq$\"), 2, 1),\n    name = str_remove(name, \"_sq$\")\n  ) |&gt; \n  pivot_wider(\n    names_from = order, \n    values_from = c(value, std, std_shock, median_shock, q05_shock, q95_shock), \n    names_prefix = 'order_') |&gt; \n  rename(d = std_shock_order_1) |&gt; \n  mutate(\n    ir = value_order_1 * d + value_order_2 * d^2,\n    # IC 95%\n    ir_lower_95 = (value_order_1 - qnorm(0.975) * std_order_1) * d + \n      (value_order_2 - qnorm(0.975) * std_order_2) * d^2,\n    ir_upper_95 = (value_order_1 + qnorm(0.975) * std_order_1) * d +\n      (value_order_2 + qnorm(0.975) * std_order_2) * d^2,\n    # IC 68%\n    ir_lower_68 = (value_order_1 - qnorm(0.84) * std_order_1) * d + \n      (value_order_2 - qnorm(0.84) * std_order_2) * d^2,\n    ir_upper_68 = (value_order_1 + qnorm(0.84) * std_order_1) * d +\n      (value_order_2 + qnorm(0.84) * std_order_2) * d^2\n  ) |&gt; \n  mutate(\n    crop = factor(\n      crop, \n      levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"))\n  ) |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = c(\n        \"temp_max_dev\",\n        \"precip_piscop_sum_dev\"\n      ),\n      labels = c(\n        \"Temp. anomalies\", \n        \"Precip. anomalies\"\n      )\n    )\n  )\n\nFor the confidence intervals:\n\ndf_irfs_lp_quad_ci &lt;- \n  df_irfs_lp_quad |&gt; \n  select(horizon, crop, name, matches(\"^(ir_lower)|^(ir_upper)\", perl = TRUE)) |&gt; \n  pivot_longer(\n    cols = matches(\"^(ir_lower)|^(ir_upper)\", perl = TRUE),\n    names_pattern = \"(.*)_(95|68)$\",\n    names_to = c(\".value\", \"level\")\n  ) |&gt; \n  mutate(level = str_c(level, \"%\"))\n\n\nnb_h &lt;- 14\n\n# Duration of the growing season\ngs_duration_df &lt;- tribble(\n  ~crop, ~tc,\n  \"Rice\", 4,\n  \"Maize\", 5,\n  \"Potato\", 6,\n  \"Cassava\", 9\n)\n\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_quad_ci,\n    mapping = aes(\n      x = horizon,\n      ymin = ir_lower, ymax = ir_upper, fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs_lp_quad,\n    mapping = aes(x = horizon, y = ir),\n    colour = \"#0072B2\") +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  geom_vline(\n    data = gs_duration_df, \n    mapping = aes(xintercept = tc),\n    colour = \"#D55E00\", linetype = \"dashed\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", \n    axes = \"all\",\n    independent = \"y\", switch = \"y\") +\n  scale_x_continuous(breaks = seq(0, nb_h, by = 2)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"gray10\", \"95%\" = \"gray60\")\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\nFigure 8.1: Agricultural production response to a weather shock\n\n\n\n\n\n\n8.1.2.1 Positive vs Negative Shocks\nLet us now observe the response to a positive vs. negative weather shock with a magnitude of 2 standard deviations.\nLet us define a function, get_quad_response() to get the IRFs depending on the sign and magnitue of the shock.\n\n#' @param scale_shock number of SD to use\n#' @param sign 1 for positive shock (default), -1 for negative shock\nget_quad_response &lt;- function(scale_shock, sign = 1) {\n  df_irfs_lp_quad &lt;- \n    map(resul_lp_quad, \"coefs\") |&gt; \n    list_rbind() |&gt; \n    filter(name %in% weather_variables) |&gt; \n    mutate(\n      order = ifelse(str_detect(name, \"_sq$\"), 2, 1),\n      name = str_remove(name, \"_sq$\")\n    ) |&gt; \n    pivot_wider(\n      names_from = order, \n      values_from = c(value, std, std_shock, median_shock, q05_shock, q95_shock), \n      names_prefix = 'order_') |&gt; \n    rename(d = std_shock_order_1) |&gt; \n    mutate(d = !!sign * !!scale_shock * d) |&gt; \n    mutate(\n      ir = value_order_1 * d + value_order_2 * d^2,\n      # IC 95%\n      ir_lower_95 = (value_order_1 - qnorm(0.975) * std_order_1) * d + \n        (value_order_2 - qnorm(0.975) * std_order_2) * d^2,\n      ir_upper_95 = (value_order_1 + qnorm(0.975) * std_order_1) * d +\n        (value_order_2 + qnorm(0.975) * std_order_2) * d^2,\n      # IC 68%\n      ir_lower_68 = (value_order_1 - qnorm(0.84) * std_order_1) * d + \n        (value_order_2 - qnorm(0.84) * std_order_2) * d^2,\n      ir_upper_68 = (value_order_1 + qnorm(0.84) * std_order_1) * d +\n        (value_order_2 + qnorm(0.84) * std_order_2) * d^2\n    ) |&gt; \n    mutate(\n      crop = factor(\n        crop, \n        levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n        labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"))\n    ) |&gt; \n    mutate(\n      name = factor(\n        name,\n        levels = c(\n          \"temp_max_dev\",\n          \"precip_piscop_sum_dev\"\n        ),\n        labels = c(\n          \"Temp. anomalies\", \n          \"Precip. anomalies\"\n        )\n      )\n    )\n  \n  df_irfs_lp_quad_ci &lt;- \n    df_irfs_lp_quad |&gt; \n    select(horizon, crop, name, matches(\"^(ir_lower)|^(ir_upper)\", perl = TRUE)) |&gt; \n    pivot_longer(\n      cols = matches(\"^(ir_lower)|^(ir_upper)\", perl = TRUE),\n      names_pattern = \"(.*)_(95|68)$\",\n      names_to = c(\".value\", \"level\")\n    ) |&gt; \n    mutate(level = str_c(level, \"%\"))\n  \n  list(df_irfs_lp_quad = df_irfs_lp_quad, df_irfs_lp_quad_ci = df_irfs_lp_quad_ci)\n}\n\n\n\nCode\ntbl_irfs_pos &lt;- get_quad_response(scale_shock = 1, sign = 1)\ntbl_irfs_neg &lt;- get_quad_response(scale_shock = 1, sign = -1)\n\ntbl_irfs &lt;- \n  tbl_irfs_pos$df_irfs_lp_quad |&gt; mutate(sign = \"positive\") |&gt; \n  bind_rows(\n    tbl_irfs_neg$df_irfs_lp_quad |&gt; mutate(sign = \"negative\")\n  )\ntbl_irfs_ci &lt;- \n  tbl_irfs_pos$df_irfs_lp_quad_ci |&gt; mutate(sign = \"positive\") |&gt; \n  bind_rows(\n    tbl_irfs_neg$df_irfs_lp_quad_ci |&gt; mutate(sign = \"negative\")\n  )\n\n\nggplot() +\n  geom_ribbon(\n    data = tbl_irfs_ci |&gt; filter(sign == \"positive\"),\n    mapping = aes(\n      x = horizon,\n      ymin = ir_lower, ymax = ir_upper, fill = level),\n    alpha = .2\n  ) +\n  geom_ribbon(\n    data = tbl_irfs_ci |&gt; filter(sign == \"negative\"),\n    mapping = aes(\n      x = horizon,\n      ymin = ir_lower, ymax = ir_upper, fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    data = tbl_irfs,\n    mapping = aes(x = horizon, y = ir, colour = sign, linetype = sign),\n    linewidth = 1.2\n  ) +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  geom_vline(\n    data = gs_duration_df, \n    mapping = aes(xintercept = tc),\n    colour = \"#D55E00\", linetype = \"dashed\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", \n    axes = \"all\",\n    independent = \"y\", switch = \"y\") +\n  scale_x_continuous(breaks = seq(0, nb_h, by = 2)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"gray10\", \"95%\" = \"gray60\")\n  ) +\n  scale_colour_manual(\n    \"Shock\", \n    values = c(\"negative\" = \"#56B4E9\", \"positive\" = \"#009E73\"),\n    labels = c(\"negative\" = \"Lower than usual\", \"positive\" = \"Higher than usual\")) +\n  scale_linetype_discrete(\n    \"Shock\",\n    labels = c(\"negative\" = \"Lower than usual\", \"positive\" = \"Higher than usual\")\n  ) +\n  theme_paper() +\n  theme(\n    strip.placement = \"outside\", \n    legend.direction = \"horizontal\", \n    legend.position = \"bottom\",\n    legend.box = \"horizontal\"\n  )\n\n\n\n\n\n\n\n\nFigure 8.2: Agricultural production response to a positive and to a negative 1SD weather shock.\n\n\n\n\n\n\n\n\n\nAcevedo, Sebastian, Mico Mrkaic, Natalija Novta, Evgenia Pugacheva, and Petia Topalova. 2020. “The Effects of Weather Shocks on Economic Activity: What Are the Channels of Impact?” Journal of Macroeconomics 65: 103207.\n\n\nJordà, Òscar. 2005. “Estimation and Inference of Impulse Responses by Local Projections.” American Economic Review 95 (1): 161–82. https://doi.org/10.1257/0002828053828518.",
    "crumbs": [
      "II. Local Projections",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quadratic Terms</span>"
    ]
  },
  {
    "objectID": "local_projections_seasonal_piscop.html",
    "href": "local_projections_seasonal_piscop.html",
    "title": "9  Time-varying exposure to weather shocks",
    "section": "",
    "text": "9.1 Definition of the Transition State Variable\nLet us create a table with the duration of the growing season for each crop:\n# Duration of the growing season\ngs_duration_df &lt;- tribble(\n  ~product_eng, ~tc,\n  \"Rice\", 4,\n  \"Dent corn\", 5,\n  \"Potato\", 6,\n  \"Cassava\", 9\n)\nWe focus on the following crops:\ncrops &lt;- c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\")\nWe then define a function that allows us to create multiple lags. This function is defined thanks to Pablo Cánovas.\n#' Add lagged columns in a tibble for a variable\n#' Source: https://typethepipe.com/vizs-and-tips/how-to-create-multiple-lags-in-r/\n#' \n#' @param df tibble\n#' @param var name of the variable to get lag of\n#' @param lags vector of desired lags\ncalculate_lags &lt;- function(df, var, lags) {\n  map_lag &lt;- lags |&gt; map(~ partial(dplyr::lag, n = .x))\n  df |&gt; \n    mutate(\n      across(\n        .cols = {{var}},\n        .fns = map_lag,\n        .names = \"{.col}_lag{lags}\"\n      )\n    )\n}\nThen, we apply the remaining steps of the creation of the planting area index.\ndf_2 &lt;- vector(mode = \"list\", length = length(crops))\n# cli_progress_bar(total = length(crops))\nfor(i_crop in 1:length(crops)) {\n  crop &lt;- crops[i_crop]\n  df_tmp &lt;- \n    df |&gt; \n    filter(product_eng == crop)\n  \n  # Duration\n  tc &lt;- \n    gs_duration_df |&gt; \n    filter(product_eng == crop) |&gt; \n    pull(tc)\n  \n  df_tmp &lt;- \n    df_tmp |&gt; \n    group_by(region_id) |&gt; \n    calculate_lags(surf_m, 1:tc) |&gt; \n    calculate_lags(Value_surfR, 1:tc) |&gt; \n    rowwise() |&gt; \n    mutate(\n      sum_surf_m = sum(\n        across(\n          c(matches(\"^surf_m_lag\"), surf_m)\n        ),\n        na.rm = TRUE\n      ),\n      sum_Value_surfR = sum(\n        across(\n          c(matches(\"^Value_surfR_lag\"), Value_surfR)\n        ), \n        na.rm = TRUE\n      ),\n    ) |&gt; \n    group_by(region_id) |&gt; \n    mutate(flow_y_tmp = sum_surf_m - sum_Value_surfR) |&gt; \n    mutate(flow_y = lag(flow_y_tmp)) |&gt; \n    mutate(flow_y = ifelse(row_number() == 1, flow_y_tmp, flow_y)) |&gt;\n    select(\n      -matches(\"^surf_m_lag\"), \n      -matches(\"^Value_surfR_lag\"),\n      -flow_y_tmp, -sum_surf_m, -sum_Value_surfR\n    ) |&gt; \n    ungroup()\n  \n  # Detrend with HP filter\n  df_tmp &lt;- \n    df_tmp |&gt; \n    group_by(region_id) |&gt; \n    mutate(\n      flow_y_cycle = as.vector(\n        mFilter::hpfilter(\n          flow_y, freq = 14400, type = \"lambda\", drift = FALSE)$cycle\n        )\n    ) |&gt; \n    mutate(\n      # normalizing to have a unit variant\n      flow_y_pot = flow_y_cycle / sd(flow_y_cycle)\n    )\n  \n  df_2[[i_crop]] &lt;- df_tmp\n  # cli_progress_update(set = i_crop)\n}\ndf &lt;- df_2 |&gt; bind_rows()",
    "crumbs": [
      "II. Local Projections",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time-varying exposure to weather shocks</span>"
    ]
  },
  {
    "objectID": "local_projections_seasonal_piscop.html#estimation",
    "href": "local_projections_seasonal_piscop.html#estimation",
    "title": "9  Time-varying exposure to weather shocks",
    "section": "9.2 Estimation",
    "text": "9.2 Estimation\nNow, let us turn to the estimation. We consider the following weather variables:\n\nweather_variables &lt;- c(\"temp_max_dev\", \"precip_piscop_sum_dev\")\n\nAnd add the following control variables:\n\ncontrol_variables &lt;- c(\"rer_hp\", \"r_hp\", \"pi\", \"ind_prod\", \"ONI\", \"price_int_inf\")\n\nWe can call the estimate_linear_lp() function to estimate the models at the different horizons. The estimation takes about a minute to run, we load results in this notebook.\n\nresul_lp_regime &lt;- \n  map(\n    crops, ~ estimate_linear_lp(\n      df = df,\n      horizons = 14,\n      y_name = \"y_new_normalized\",\n      group_name = \"region_id\",\n      detrend = TRUE,\n      add_month_fe = FALSE,\n      add_intercept = FALSE,\n      crop_name = .x,\n      control_names = control_variables,\n      weather_names = weather_variables,\n      share_geo = NULL,\n      transition_name = \"flow_y_pot\",\n      transition_method = \"normal\",\n      state_names = c(\"planted\", \"harvested\"),\n      gamma = NULL,\n      std = \"Cluster\"\n    )\n  )\nnames(resul_lp_regime) &lt;- crops\n\nsave(resul_lp_regime, file = \"../R/output/resul_lp_regime.rda\")\n\n\nload(\"../R/output/resul_lp_regime.rda\")",
    "crumbs": [
      "II. Local Projections",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time-varying exposure to weather shocks</span>"
    ]
  },
  {
    "objectID": "local_projections_seasonal_piscop.html#results",
    "href": "local_projections_seasonal_piscop.html#results",
    "title": "9  Time-varying exposure to weather shocks",
    "section": "9.3 Results",
    "text": "9.3 Results\nWe can visualize the Impulse Response Functions (IRFs) by plotting the estimated coefficients associated with the weather variables. The theme_paper() function has been defined when we loaded the utils.R script. This allows us to modify the theme of graphs made with {ggplot2}.\nThe name of the weather variables in the results:\n\nweather_variables_season &lt;-str_c(\n  rep(c(\"planted\", \"harvested\"), each = 2),\n  rep(weather_variables, 2),\n  sep = \"_\"\n)\nweather_variables_season\n\n[1] \"planted_temp_max_dev\"            \"planted_precip_piscop_sum_dev\"  \n[3] \"harvested_temp_max_dev\"          \"harvested_precip_piscop_sum_dev\"\n\n\nLet us shape the results so that they can be fed to a the ggplot() data argument.\n\ndf_irfs_lp &lt;- \n  map(resul_lp_regime, \"coefs\") |&gt; \n  list_rbind() |&gt; \n  filter(name %in% weather_variables_season) |&gt; \n  rowwise() |&gt; \n  mutate(\n    regime = case_when(\n      str_detect(name, \"^planted\") ~ \"Growing season\",\n      str_detect(name, \"^harvested\") ~ \"Harvesting season\",\n      TRUE ~ \"Error\"\n    ),\n    weather = str_split(name, \"(^planted_|^harvested_)\")[[1]][2]\n  ) |&gt; \n  ungroup() |&gt; \n  mutate(\n    shock_1_sd = value * std_shock,\n    lower_95 = (value - qnorm(0.975) * std) * std_shock,\n    upper_95 = (value + qnorm(0.975) * std) * std_shock,\n    lower_68 = (value - qnorm(0.84)  * std) * std_shock,\n    upper_68 = (value + qnorm(0.84)  * std) * std_shock\n  ) |&gt; \n  mutate(\n    crop = factor(\n      crop, \n      levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"))\n  ) |&gt; \n  mutate(\n    weather = factor(\n      weather,\n      levels = c(\n        \"temp_max_dev\",\n        \"precip_piscop_sum_dev\"\n      ),\n      labels = c(\n        \"Temp. anomalies\", \n        \"Precip. anomalies\"\n      )\n    ),\n    regime = factor(\n      regime,\n      levels = c(\"Growing season\", \"Harvesting season\")\n    )\n  )\n\nWe can define another table with the confidence intervals ready to be plotted as ribbons. Note that we only plot 68% confidence interval later on, to avoid overloading the graphs:\n\ndf_irfs_lp_ci &lt;- \n  df_irfs_lp |&gt; \n  select(horizon, crop, weather, regime, matches(\"^(lower)|^(upper)\", perl = TRUE)) |&gt; \n  pivot_longer(\n    cols = matches(\"^(lower)|^(upper)\", perl = TRUE),\n    names_pattern = \"(.*)_(95|68)$\",\n    names_to = c(\".value\", \"level\")\n  ) |&gt; \n  mutate(level = str_c(level, \"%\"))\n\n\nnb_h &lt;- 14\n\n# Duration of the growing season\ngs_duration_df &lt;- tribble(\n  ~crop, ~tc,\n  \"Rice\", 4,\n  \"Maize\", 5,\n  \"Potato\", 6,\n  \"Cassava\", 9\n)\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci |&gt; filter(level == \"68%\"),\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = regime\n    ),\n    alpha = .4\n  ) +\n  geom_line(\n    data = df_irfs_lp,\n    mapping = aes(x = horizon, y = shock_1_sd, colour = regime)) +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  geom_vline(\n      data = gs_duration_df, \n      mapping = aes(xintercept = tc),\n      colour = \"#D55E00\", linetype = \"dashed\") +\n  ggh4x::facet_grid2(\n    weather~crop, \n    scales = \"free_y\", independent = \"y\", switch = \"y\", axes = \"all\") +\n  scale_x_continuous(breaks = seq(0, nb_h, by = 2)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    NULL,\n    values = c(\n      \"Growing season\" = \"#949698\",\n      \"Harvesting season\" = \"#009E73\"\n    )\n  ) +\n  scale_colour_manual(\n    NULL,\n    values = c(\n      \"Growing season\" = \"#949698\",\n      \"Harvesting season\" = \"#009E73\"\n    )\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\nFigure 9.1: Agricultural production response to a weather shock contrasting for growing vs. harvesting season\n\n\n\n\n\n\n9.3.1 Comparing results with the Linear Local Projections\nLet us load the results obtained in Section 16.1.4 for the linear local projection. First, let us save the current results in different objects.\n\ndf_irfs_lp_season &lt;- df_irfs_lp\ndf_irfs_lp_ci_season &lt;- df_irfs_lp_ci\n\nNow we can load the results from the linear local projections.\n\nload(\"../R/output/df_irfs_lp_piscop.rda\")\n\nWe focus on the growing season.\n\ndf_irfs_lp_season_comparison &lt;- \n  df_irfs_lp |&gt; \n  mutate(regime = \"Linear\") |&gt; \n  rename(weather = name) |&gt; \n  bind_rows(\n    df_irfs_lp_season |&gt; \n      filter(regime == \"Growing season\") |&gt; \n      mutate(regime = \"State-dependent (growing season)\")\n  ) |&gt; \n  filter(horizon &lt;= 9)\n\ndf_irfs_lp_ci_season_comparison &lt;- \n  df_irfs_lp_ci |&gt; \n  mutate(regime = \"Linear\") |&gt; \n  rename(weather = name) |&gt; \n  bind_rows(\n    df_irfs_lp_ci_season |&gt; \n      filter(regime == \"Growing season\") |&gt; \n      mutate(regime = \"State-dependent (growing season)\")\n  ) |&gt; \n  filter(horizon &lt;= 9)\n\nThen, we can compare the IRFs obtained for the growing season with the ones obtained in the linear case, without states.\n\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci_season_comparison |&gt; \n      filter(level == \"68%\"),\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = regime\n    ),\n    alpha = .4\n  ) +\n  geom_line(\n    data = df_irfs_lp_season_comparison,\n    mapping = aes(x = horizon, y = shock_1_sd, colour = regime)) +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  geom_vline(\n    data = gs_duration_df, \n    mapping = aes(xintercept = tc),\n    colour = \"#D55E00\", linetype = \"dashed\") +\n  ggh4x::facet_grid2(\n    weather~crop, \n    scales = \"free_y\", independent = \"y\", axes = \"all\", switch = \"y\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 9, by = 2)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    NULL,\n    values = c(\n      \"Linear\" = \"#0072B2\",\n      \"State-dependent (growing season)\" = \"#E69F00\"\n    )\n  ) +\n  scale_colour_manual(\n    NULL,\n    values = c(\n      \"Linear\" = \"#0072B2\",\n      \"State-dependent (growing season)\" = \"#E69F00\"\n    )\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\nFigure 9.2: Agricultural production response to a weather shock contrasting for the linear case vs growing season state\n\n\n\n\n\n\n\n\n\nAuerbach, Alan, and Yuriy Gorodnichenko. 2011. “Fiscal Multipliers in Recession and Expansion.” National Bureau of Economic Research. https://doi.org/10.3386/w17447.",
    "crumbs": [
      "II. Local Projections",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time-varying exposure to weather shocks</span>"
    ]
  },
  {
    "objectID": "ag-fluctuations_piscop.html",
    "href": "ag-fluctuations_piscop.html",
    "title": "10  From Regional to Aggregate Fluctuations",
    "section": "",
    "text": "10.1 Synthetic measure of the weather\nThis section provides a detailed description of the methodology employed to create the synthetic measure of the weather.\nWe need to load the results from the estimations made in Chapter 7.\nload(\"../R/output/resul_lp_piscop.rda\")\n# Data used in the local projections\nload(\"../data/output/df_lp.rda\")\nThe average price of each crop in the data, denoted hereafter \\(p_c\\):\naverage_price_crops &lt;- \n  df |&gt; group_by(product_eng) |&gt; \n  summarise(price_crop = mean(Value_prices, na.rm = TRUE))\naverage_price_crops\n\n# A tibble: 4 × 2\n  product_eng price_crop\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Cassava          0.550\n2 Dent corn        0.680\n3 Potato           0.578\n4 Rice             0.671\nThen, we can put the coefficients in a table, for each crop and each time horizon:\nweather_variables &lt;- c(\n  \"temp_max_dev\", \"precip_piscop_sum_dev\"\n  )\ncoefs_weather &lt;- map(\n  .x = resul_lp,\n  .f = ~ .x$coefs |&gt; \n    filter(name %in% weather_variables) |&gt; \n    select(horizon, name, value)\n)\nnames(coefs_weather) &lt;- map_chr(resul_lp, \"crop_name\")\ncoefs_weather &lt;- \n  list_rbind(coefs_weather, names_to = \"crop\")\nThen, we follow a methodology consisting of several key steps.",
    "crumbs": [
      "Macroeconomic Impacts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>From Regional to Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "ag-fluctuations_piscop.html#sec-synthetic-measure-weather-piscop",
    "href": "ag-fluctuations_piscop.html#sec-synthetic-measure-weather-piscop",
    "title": "10  From Regional to Aggregate Fluctuations",
    "section": "",
    "text": "10.1.1 Step 1: Weather Shock Contribution\nThe first step involves estimating the contribution of weather shocks at each time period (\\(t\\)) for the chosen crop (\\(c\\)) and time horizon (\\(h\\)). This contribution is determined by considering the weather variables, temperature (\\(T_{i,t}\\)) and precipitation (\\(P_{i,t}\\)), and their respective coefficients (\\(\\beta_{c,h}^{T}\\) as well as \\(\\beta_{c,h}^{P}\\)). The weather shock contribution (\\(\\Gamma_{c,i,t,h}\\)) is obtained by multiplying these coefficients with the corresponding weather variables and summing them up:\n\\[\\Gamma_{c,i,t,h} = \\beta_{c,h}^{T} T_{i,t-h} + \\beta_{c,h}^{P} P_{i,t-h} \\tag{10.1}\\]\nThis first step is performed, using two user-defined functions: weather_contrib_crop_h() which computes the contribution of the weather for a single crop and time horizon, and with contrib_weather_crop() which uses the former to compute the contribution of the weather for a single crop, for all horizons considered. We will consider 9 horizons:\n\nnb_periods_wcal &lt;- 9\ncoefs_weather &lt;- coefs_weather |&gt;\n  filter(horizon &lt;= nb_periods_wcal)\n\n\n#' Computes the contribution of the weather for a single crop and time horizon\n#'\n#' @param lp_crop LP for a specific crop\n#' @param h horizon (single value)\n#' @param weather_variables vector of names of the weather variables\n#' @param ic_conf confidence interval used to determine whether a shock has a\n#'   significant effect (default to .95)\nweather_contrib_crop_h &lt;- function(lp_crop,\n                                   h,\n                                   weather_variables,\n                                   ic_conf = .95) {\n  # The data\n  data_lp &lt;- lp_crop$data_lp\n  data_lp &lt;- data_lp[[which(names(data_lp) == h)]] |&gt; \n    select(region_id, date, product_eng, !!weather_variables) |&gt; \n    ungroup()\n  # The coefficients\n  lp_coefs &lt;- \n    lp_crop$coefs |&gt; \n    filter(horizon == !!h, name %in% !!weather_variables) |&gt; \n    mutate(\n      value_lb = value - qnorm(1 - ((1 - ic_conf) / 2)) * std,\n      value_ub = value + qnorm(1 - ((1 - ic_conf) / 2)) * std\n    )\n  \n  # Keeping the values\n  lp_coefs_value &lt;- lp_coefs$value\n  # The lower and upper bounds\n  lp_coefs_value_lb &lt;- lp_coefs$value_lb\n  lp_coefs_value_ub &lt;- lp_coefs$value_ub\n  \n  data_lp |&gt; \n    nest(.by = c(date, region_id)) |&gt; \n    mutate(\n      contribution = map(\n        .x = data,\n        .f = ~ as.matrix(.x[, weather_variables]) %*% lp_coefs_value |&gt; \n          sum()\n      ),\n      contribution_lb = map(\n        .x = data,\n        .f = ~ as.matrix(.x[, weather_variables]) %*% lp_coefs_value_lb |&gt; \n          sum()\n      ),\n      contribution_ub = map(\n        .x = data,\n        .f = ~ as.matrix(.x[, weather_variables]) %*% lp_coefs_value_ub |&gt; \n          sum()\n      )\n    ) |&gt; \n    unnest(cols = c(contribution, contribution_lb, contribution_ub)) |&gt; \n    select(-data) |&gt; \n    mutate(\n      significant = (contribution_lb &gt; 0 & contribution_ub &gt; 0) | (contribution_lb &lt; 0 & contribution_ub &lt; 0),\n      significant = as.numeric(significant)\n    ) |&gt; \n    mutate(date = date + lubridate::period(str_c(!!h, \" month\")))\n}\n\n\n#' Computes the contribution of the weather for a single crop, for all horizons\n#'\n#' @param lp_crop LP for a specific crop\n#' @param weather_variables vector of names of the weather variables\n#' @param horizons vector of horizons. If `NULL`, uses all horizons in lp_crop\ncontrib_weather_crop &lt;- function(lp_crop,\n                                 weather_variables,\n                                 horizons = NULL) {\n  if (is.null(horizons)) horizons &lt;- unique(lp_crop$coefs$horizon)\n  map(\n    .x = horizons,\n    .f = ~weather_contrib_crop_h(\n      lp_crop = lp_crop, \n      h = .x, \n      weather_variables = weather_variables\n    ) |&gt; \n      mutate(horizon = .x)\n  ) |&gt; \n    list_rbind() |&gt; \n    # group_by(date) |&gt; \n    # summarise(value = sum(contribution)) |&gt; \n    mutate(crop = lp_crop$crop_name) |&gt; \n    mutate(contribution_signif = contribution * significant)\n}\n\nLet us compute the \\(\\Gamma_{c,i,t,h}\\) for each crop, region, date and horizon:\n\nweather_measure_crop &lt;- \n  map(\n    .x = resul_lp,\n    .f = ~contrib_weather_crop(\n      lp_crop = .x, \n      weather_variables = weather_variables,\n      horizons = 0:nb_periods_wcal\n    )\n  ) |&gt; \n  list_rbind()\n\nLet us add the monthly regional selling prices of the crops (df$Value_prices), as well as the average crop-specific prices computed earlier on the whole sample (average_price_crops$price_crop) which will be used as a weight when we will aggregate all crops.\n\nweather_measure_crop &lt;- \n  weather_measure_crop |&gt; \n  left_join(\n    df |&gt; select(product_eng, region_id, date, y_new, Value_prices),\n    by = c(\"date\", \"crop\" = \"product_eng\", \"region_id\")\n  ) |&gt; \n  left_join(\n    average_price_crops,\n    by = c(\"crop\" = \"product_eng\")\n  )\n\nweather_measure_crop\n\n# A tibble: 136,890 × 12\n   date       region_id contribution contribution_lb contribution_ub significant\n   &lt;date&gt;     &lt;fct&gt;            &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1 2001-01-01 1              0.354           0.443            0.265            1\n 2 2001-02-01 1              0.267           0.363            0.171            1\n 3 2001-03-01 1              0.167           0.207            0.128            1\n 4 2001-04-01 1              0.0955          0.138            0.0526           1\n 5 2001-05-01 1              0.110           0.133            0.0874           1\n 6 2001-06-01 1              0.197           0.256            0.138            1\n 7 2001-07-01 1              0.116           0.147            0.0855           1\n 8 2001-08-01 1              0.0466          0.0658           0.0275           1\n 9 2001-09-01 1              0.124           0.136            0.113            1\n10 2001-10-01 1              0.00941         0.00436          0.0145           1\n# ℹ 136,880 more rows\n# ℹ 6 more variables: horizon &lt;int&gt;, crop &lt;chr&gt;, contribution_signif &lt;dbl&gt;,\n#   y_new &lt;dbl&gt;, Value_prices &lt;dbl&gt;, price_crop &lt;dbl&gt;\n\n\n\n\n10.1.2 Step 2: Quantity Weights\nFor each crop and date \\(t\\), we define some weights for the regional observations using the data observed at horizon \\(h=0\\), as the sum of the monthly agricultural production over the regions considered in the analysis. The monthly production is expressed in monetary terms, by multiplying the quantities (y_new, i.e., \\(y^{raw}\\)) by unit price (price_crop, i.e., \\(p_c\\)).\n\\[\\omega_{c,t} = \\sum_{i} y^{\\text{raw}}_{c,t,i} \\times p_{c}\\]\n\nquantity_weight &lt;- \n  weather_measure_crop |&gt; \n  filter(horizon == 0) |&gt; \n  group_by(crop, date, horizon, price_crop) |&gt; \n  summarise(quantity_weight = sum(price_crop * y_new), .groups = \"drop\") |&gt; \n  select(crop, date, quantity_weight)\nquantity_weight\n\n# A tibble: 720 × 3\n   crop    date       quantity_weight\n   &lt;chr&gt;   &lt;date&gt;               &lt;dbl&gt;\n 1 Cassava 2001-01-01          38797.\n 2 Cassava 2001-02-01          39625.\n 3 Cassava 2001-03-01          44893.\n 4 Cassava 2001-04-01          44893.\n 5 Cassava 2001-05-01          41857.\n 6 Cassava 2001-06-01          36333.\n 7 Cassava 2001-07-01          36333.\n 8 Cassava 2001-08-01          39766.\n 9 Cassava 2001-09-01          36525.\n10 Cassava 2001-10-01          36525.\n# ℹ 710 more rows\n\n\n\n\n10.1.3 Step 3: Crop-Specific Weather-Adjusted Agricultural Production\nFor each crop \\(c\\), at each date \\(t\\), we compute the weather-adjusted agricultural production, \\(y_{c,t}^{\\omega}\\), as the sum of the significant crop-specific contributions of the weather to the agricultural production. The crop-specific contribution across regions is first aggregated at the national level, using an average of the crop and region-specific contribution of the weather to the monetary equivalence of the agricultural production.\n\\[y_{c,t}^{\\omega} = \\sum_{h}\\sum_{i} \\frac{\\mathbb{1}_{\\text{signif}_{c,i,t,h}} \\times \\gamma_{c,i,t,h} \\times p_{c} \\times  y^{\\text{raw}}_{c,t,i}}{\\text{card}(I_{c,t})},\\]\nwhere \\(\\text{card(I)}\\) is the number of regions that produce crop \\(c\\) at time \\(t\\). The characteristic function \\(\\mathbb{1}_{\\text{signif}_{c,it,h}}\\) is equal to 1 when the contribution is significantly different from 0 (using the 95% confidence intervals of the coefficients \\(\\beta_{c,h}^{T}\\) and \\(\\beta_{c,h}^{P}\\)), and is equal to 0 otherwise.\n\nweather_adjusted_ag &lt;- \n  weather_measure_crop |&gt; \n  # each group: observations across regions, for a crop x date x horizon\n  group_by(crop, date, horizon, price_crop) |&gt; \n  # weather-adjusted agricultural production at each horizon\n  # y_{c,t,h}^{w}\n  summarise(\n    y_w = sum(price_crop * contribution_signif *  y_new / n(), na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt; \n  group_by(crop, date) |&gt; \n  # weather-adjusted agricultural production summed over horizons\n  # y_{c,t}^{w}\n  summarise(\n    y_w = sum(y_w),\n    .groups = \"drop\"\n  )\n\n\n\n10.1.4 Step 4: Aggregation at the National Level\nThen, we aggregate the crop-specific weather-adjusted agricultural production at the national level using the following formula: \\[y_{t}^{\\omega} = \\frac{\\sum_{c} y_{c,t}^{\\omega}}{\\sum_{c}\\omega_{c,t}},\\]\nwhere \\(\\omega_{c,t}\\) are the quantity weights computed in the second step.\n\nw_df &lt;- \n  weather_adjusted_ag |&gt; \n  left_join(quantity_weight, by = c(\"crop\", \"date\")) |&gt; \n  group_by(date) |&gt; \n  summarise(\n    w = sum(y_w) / sum(quantity_weight),\n    .groups = \"drop\"\n  )\n\n\n\n10.1.5 Step 5: Detrending\nLastly, we express the national weather-adjusted production as a loss, and take it as a deviation from its trend:\n\\[\nW_{t} = -100 \\times (y_{t}^{\\omega} - \\overline{y_{t}^{\\omega}}),\n\\]\nwhere \\(y_{t}^{\\omega}\\) is the trend, obtained with the HP filter.\n\n# Load detrending functions\nsource(\"../weatherperu/R/detrending.R\")\n\nw_df &lt;- w_df |&gt; \n  mutate(\n    w_trend = hp_filter_trend(w, freq = 14400),\n    w_dt = - 100 * (w - w_trend),\n  )",
    "crumbs": [
      "Macroeconomic Impacts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>From Regional to Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "ag-fluctuations_piscop.html#agricultural-production",
    "href": "ag-fluctuations_piscop.html#agricultural-production",
    "title": "10  From Regional to Aggregate Fluctuations",
    "section": "10.2 Agricultural Production",
    "text": "10.2 Agricultural Production\nIn the second step in the creation of the synthetic measure of the weater, we computed quantity weights \\(\\omega_{c,t}\\), which correspond to the agricultural production in month \\(t\\) for crop \\(t\\), expressed in monetary terms. Let us aggregate these values across crops to obtain a monthly agricultural production: \\[y_t^{A} = \\sum_{c}\\omega_{c,t}.\\]\nWe then express these as percentage deviation from their trend computed using the HP filter.\n\nagricultural_output &lt;- \n  quantity_weight |&gt; \n  group_by(date) |&gt; \n  summarise(quantity_weight = sum(quantity_weight), .groups = \"drop\") |&gt; \n  mutate(\n    # Remove seasonality\n    q_sa = adj_season_X13(quantity_weight, ymd(\"2001-01-01\")),\n    # Extract trend\n    q_sa_trend = hp_filter_trend(q_sa, freq = 14400),\n    # Percentage dev. from trend\n    q = 100 * log(q_sa / q_sa_trend)\n  ) |&gt; \n  select(date, q)\n\nNote: this is actually not used.",
    "crumbs": [
      "Macroeconomic Impacts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>From Regional to Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "ag-fluctuations_piscop.html#sec-var-macro-indicators-piscop",
    "href": "ag-fluctuations_piscop.html#sec-var-macro-indicators-piscop",
    "title": "10  From Regional to Aggregate Fluctuations",
    "section": "10.3 Macroeconomic indicators",
    "text": "10.3 Macroeconomic indicators\nThe vector of endogenous variables in the estimation, denoted as \\(Y_t\\), consists of eight variables: \\(W_t\\), \\(RER_t\\), \\(\\pi_t^{a}\\), \\(\\pi_{t}\\) , \\(X_t\\), \\(y^A_t\\), \\(y_t\\), and \\(r_t\\):\n\n\\(W_t\\) represents the aggregate measure of weather-driven agricultural losses defined in Section 10.1, which quantifies the loss in agricultural value added due to weather shocks, expressed as a deviation from its trend. It is the focal variable of interest in this analysis.\n\\(RER_t\\) denotes the Real Exchange Rate (RER), which reflects the relative value of the domestic currency against a basket of foreign currencies.\n\\(\\pi^a_t\\) corresponds to the percentage change of the Food Consumer Price Index (CPIA), which serves as a measure of food inflation.\n\\(\\pi_t\\) corresponds to the percentage change of the Consumer Price Index (CPI), which serves as a measure of inflation.\n\\(X_t\\) denotes Exports.\n\\(y^A_t\\) is the Agricultural production.\n\\(y_t\\) represents the Gross Domestic Product (GDP), which serves as a measure of the overall economic activity and output in the country.\n\\(r_t\\) is the nominal rate.\n\nTo construct the vectors \\(Y_t\\), we use data from the Central Reserve Bank of Perú.\nThe Real Exchange Rate (RER) data are obtained using the token PN01259PM, the Food Consumer Price Index (CPIA) data with token PN01336PM, the Consumer Price Index (CPI) data with token PN01270PM, the Exports data with token PN01461BM, the GDP data with token PN01773AM, the agricultural GDP with token PN01755AM, and the nominal rate with token PN07819NM.\nLet us load the dataset of macroeconomic variables (see Chapter 3 for details on the construction of these variables).\n\nload(\"../data/output/macro/df_macro.rda\")",
    "crumbs": [
      "Macroeconomic Impacts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>From Regional to Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "ag-fluctuations_piscop.html#merging-the-data",
    "href": "ag-fluctuations_piscop.html#merging-the-data",
    "title": "10  From Regional to Aggregate Fluctuations",
    "section": "10.4 Merging the Data",
    "text": "10.4 Merging the Data\nThe sample period for our analysis covers the time span from January 2003 (2003M1) to December 2015 (2015M12). This period provides a comprehensive view of the relationship between weather-driven agricultural losses and the selected economic indicators in Peru.\n\ndf_var &lt;- \n  df_macro |&gt; \n  left_join(\n    w_df,\n    by = \"date\"\n  ) |&gt; \n  mutate(\n    w = w_dt,\n    q = ya\n  )\n  \nvariable_names &lt;- c(\n  \"Agricultural losses\" = \"w\",\n  \"Real exchange rate\" = \"rer_dt_sa\",\n  \"Food inflation rate (pp)\" = \"pia\",\n  \"Inflation rate (pp)\" = \"pi\",\n  \"Exports (pp)\" = \"x\",\n  \"Agricultural output (pp)\" = \"q\",\n  \"GDP (pp)\" = \"y\",\n  \"Interest rate (pp)\" = \"r\"\n)\n\nLet us add labels to the new columns:\n\ndf_var &lt;- \n  df_var |&gt;  \n  labelled::set_variable_labels(\n    w = \"Agricultural losses\"\n  )\n\nLet us divide the all the values (except exports) by 100:\n\ndf_var &lt;- \n  df_var |&gt;  \n  labelled::set_variable_labels(\n    w = \"Agricultural losses\",\n    q = \"Agricultural output (pp)\"\n  ) |&gt; \n  mutate(\n    w = w / 100, \n    rer_dt_sa = rer_dt_sa / 100, \n    pi = pi / 100, \n    pia = pia / 100, \n    # x\n    q = q / 100, \n    y = y / 100, \n    r = r / 100\n  )\n\nFigure Figure 17.1 displays the time series data for the variables included in the vector \\(Y\\).\n\nggplot(\n    data = df_var |&gt; \n      filter(date &gt;= \"2003-01-01\") |&gt; \n      select(date, w, rer_dt_sa, pia, pi, x, q, y, r) |&gt; \n      pivot_longer(cols = -date) |&gt; \n      mutate(\n        name = factor(\n          name,\n          levels = variable_names,\n          labels = names(variable_names)\n        )\n      ),\n    mapping = aes(x = date, y = value)\n  ) +\n  geom_line() +\n  facet_wrap(~name, scales = \"free_y\") +\n  theme_paper() +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\nFigure 10.1: Time Series of Endogenous Variables for Vector Autoregression (VAR) Analysis in Peru (2003-2015)\n\n\n\n\n\nBefore proceeding to the estimation, there is one final step, which involves converting the data into the ts format.\n\nstart_date &lt;- \"2003-01-01\"\ndf_var_ts &lt;-\n  df_var |&gt; \n  filter(date &gt;= start_date) |&gt; \n  select(\n    w, rer_dt_sa, pia, pi, x, q, y, r\n  ) |&gt;\n  ts(start = c(year(start_date), month(start_date)), freq = 12)",
    "crumbs": [
      "Macroeconomic Impacts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>From Regional to Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "ag-fluctuations_piscop.html#estimation",
    "href": "ag-fluctuations_piscop.html#estimation",
    "title": "10  From Regional to Aggregate Fluctuations",
    "section": "10.5 Estimation",
    "text": "10.5 Estimation\nWe estimate a VAR(p) model with a constant term but no trend. Let us look how many lags we should use, using the automatic selection method provided by the VARselect() function from {vars}.\n\ninfo_var_estim &lt;- vars::VARselect(y = df_var_ts, type = \"const\", lag.max = 6)\ninfo_var_estim\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     2      1      1      2 \n\n$criteria\n                   1             2             3             4             5\nAIC(n) -7.720444e+01 -7.741493e+01 -7.702973e+01 -7.669223e+01 -7.650309e+01\nHQ(n)  -7.661734e+01 -7.630596e+01 -7.539890e+01 -7.453952e+01 -7.382852e+01\nSC(n)  -7.575933e+01 -7.468529e+01 -7.301555e+01 -7.139351e+01 -6.991984e+01\nFPE(n)  2.958282e-34  2.412802e-34  3.608431e-34  5.228713e-34  6.678365e-34\n                   6\nAIC(n) -7.654810e+01\nHQ(n)  -7.335167e+01\nSC(n)  -6.868031e+01\nFPE(n)  6.949015e-34\n\n\n\n10.5.1 Results\nLet us estimate a VAR(2) model with a constant term but no trend.\n\nvar_l1 &lt;- vars::VAR(y = df_var_ts, p = 2, type = \"const\")\nsummary(var_l1)\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: w, rer_dt_sa, pia, pi, x, q, y, r \nDeterministic variables: const \nSample size: 154 \nLog Likelihood: 4342.644 \nRoots of the characteristic polynomial:\n0.8901 0.8901 0.8293 0.8293 0.5375 0.5375 0.4283 0.4013 0.4013 0.377 0.377 0.2778 0.2762 0.2762 0.2703 0.2703\nCall:\nvars::VAR(y = df_var_ts, p = 2, type = \"const\")\n\n\nEstimation results for equation w: \n================================== \nw = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nw.l1          1.267638   0.078368  16.175  &lt; 2e-16 ***\nrer_dt_sa.l1  0.068611   0.062152   1.104   0.2716    \npia.l1       -0.059921   0.254887  -0.235   0.8145    \npi.l1         0.365242   0.529096   0.690   0.4912    \nx.l1         -0.006021   0.007015  -0.858   0.3923    \nq.l1          0.034348   0.026556   1.293   0.1980    \ny.l1         -0.151286   0.062832  -2.408   0.0174 *  \nr.l1          0.495312   0.313629   1.579   0.1166    \nw.l2         -0.448714   0.079797  -5.623 1.01e-07 ***\nrer_dt_sa.l2 -0.143152   0.064470  -2.220   0.0280 *  \npia.l2       -0.328037   0.253830  -1.292   0.1984    \npi.l2         0.374000   0.521350   0.717   0.4744    \nx.l2         -0.006520   0.006740  -0.967   0.3351    \nq.l2         -0.037625   0.026388  -1.426   0.1562    \ny.l2          0.012412   0.064949   0.191   0.8487    \nr.l2         -0.607335   0.306618  -1.981   0.0496 *  \nconst         0.016888   0.009983   1.692   0.0930 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.007158 on 137 degrees of freedom\nMultiple R-Squared: 0.9002, Adjusted R-squared: 0.8886 \nF-statistic: 77.25 on 16 and 137 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation rer_dt_sa: \n========================================== \nrer_dt_sa = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nw.l1         -0.128434   0.104940  -1.224   0.2231    \nrer_dt_sa.l1  1.014179   0.083226  12.186   &lt;2e-16 ***\npia.l1        0.429770   0.341311   1.259   0.2101    \npi.l1        -0.950152   0.708494  -1.341   0.1821    \nx.l1          0.013879   0.009394   1.478   0.1418    \nq.l1         -0.016263   0.035560  -0.457   0.6482    \ny.l1          0.092633   0.084136   1.101   0.2728    \nr.l1         -0.383389   0.419970  -0.913   0.3629    \nw.l2          0.219431   0.106853   2.054   0.0419 *  \nrer_dt_sa.l2 -0.188915   0.086329  -2.188   0.0303 *  \npia.l2        0.043780   0.339895   0.129   0.8977    \npi.l2         0.213844   0.698122   0.306   0.7598    \nx.l2         -0.014129   0.009025  -1.566   0.1198    \nq.l2         -0.003499   0.035336  -0.099   0.9213    \ny.l2         -0.005852   0.086971  -0.067   0.9465    \nr.l2          0.429207   0.410582   1.045   0.2977    \nconst        -0.001024   0.013368  -0.077   0.9391    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.009585 on 137 degrees of freedom\nMultiple R-Squared: 0.782,  Adjusted R-squared: 0.7565 \nF-statistic: 30.71 on 16 and 137 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation pia: \n==================================== \npia = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n               Estimate Std. Error t value Pr(&gt;|t|)  \nw.l1          0.0052509  0.0493833   0.106   0.9155  \nrer_dt_sa.l1  0.0580618  0.0391649   1.482   0.1405  \npia.l1        0.1803229  0.1606159   1.123   0.2635  \npi.l1         0.0866057  0.3334069   0.260   0.7954  \nx.l1         -0.0051089  0.0044205  -1.156   0.2498  \nq.l1         -0.0184108  0.0167340  -1.100   0.2732  \ny.l1          0.0069203  0.0395932   0.175   0.8615  \nr.l1         -0.0621598  0.1976318  -0.315   0.7536  \nw.l2         -0.0188948  0.0502835  -0.376   0.7077  \nrer_dt_sa.l2 -0.0074583  0.0406251  -0.184   0.8546  \npia.l2       -0.0159533  0.1599494  -0.100   0.9207  \npi.l2         0.0492539  0.3285258   0.150   0.8810  \nx.l2          0.0024291  0.0042471   0.572   0.5683  \nq.l2          0.0004987  0.0166285   0.030   0.9761  \ny.l2          0.0821621  0.0409271   2.008   0.0467 *\nr.l2          0.0274411  0.1932138   0.142   0.8873  \nconst         0.0065104  0.0062906   1.035   0.3025  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.00451 on 137 degrees of freedom\nMultiple R-Squared: 0.1951, Adjusted R-squared: 0.1011 \nF-statistic: 2.075 on 16 and 137 DF,  p-value: 0.01265 \n\n\nEstimation results for equation pi: \n=================================== \npi = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n               Estimate Std. Error t value Pr(&gt;|t|)  \nw.l1          0.0097926  0.0239830   0.408    0.684  \nrer_dt_sa.l1  0.0455393  0.0190204   2.394    0.018 *\npia.l1       -0.0273362  0.0780032  -0.350    0.727  \npi.l1         0.2660423  0.1619191   1.643    0.103  \nx.l1         -0.0018619  0.0021468  -0.867    0.387  \nq.l1         -0.0122134  0.0081268  -1.503    0.135  \ny.l1          0.0265333  0.0192284   1.380    0.170  \nr.l1         -0.0001969  0.0959799  -0.002    0.998  \nw.l2         -0.0217587  0.0244202  -0.891    0.374  \nrer_dt_sa.l2 -0.0161181  0.0197296  -0.817    0.415  \npia.l2        0.0124399  0.0776795   0.160    0.873  \npi.l2        -0.0370116  0.1595486  -0.232    0.817  \nx.l2          0.0009803  0.0020626   0.475    0.635  \nq.l2         -0.0007480  0.0080756  -0.093    0.926  \ny.l2          0.0362947  0.0198762   1.826    0.070 .\nr.l2         -0.0213335  0.0938343  -0.227    0.820  \nconst         0.0036878  0.0030550   1.207    0.229  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.002191 on 137 degrees of freedom\nMultiple R-Squared:  0.28,  Adjusted R-squared: 0.1959 \nF-statistic:  3.33 on 16 and 137 DF,  p-value: 6.3e-05 \n\n\nEstimation results for equation x: \n================================== \nx = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n             Estimate Std. Error t value Pr(&gt;|t|)    \nw.l1         -0.72984    0.96318  -0.758   0.4499    \nrer_dt_sa.l1 -0.29342    0.76388  -0.384   0.7015    \npia.l1       -1.33995    3.13267  -0.428   0.6695    \npi.l1         5.28399    6.50280   0.813   0.4179    \nx.l1          0.14367    0.08622   1.666   0.0979 .  \nq.l1         -0.46358    0.32638  -1.420   0.1578    \ny.l1          1.06544    0.77223   1.380   0.1699    \nr.l1         -0.59428    3.85463  -0.154   0.8777    \nw.l2         -0.28340    0.98073  -0.289   0.7730    \nrer_dt_sa.l2  0.13202    0.79236   0.167   0.8679    \npia.l2       -1.66568    3.11967  -0.534   0.5943    \npi.l2         4.34184    6.40760   0.678   0.4992    \nx.l2          0.14425    0.08284   1.741   0.0839 .  \nq.l2         -0.10515    0.32432  -0.324   0.7463    \ny.l2          0.24162    0.79825   0.303   0.7626    \nr.l2         -1.29717    3.76846  -0.344   0.7312    \nconst         0.79948    0.12269   6.516 1.27e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.08797 on 137 degrees of freedom\nMultiple R-Squared: 0.2248, Adjusted R-squared: 0.1343 \nF-statistic: 2.484 on 16 and 137 DF,  p-value: 0.002373 \n\n\nEstimation results for equation q: \n================================== \nq = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n              Estimate Std. Error t value Pr(&gt;|t|)  \nw.l1         -0.362386   0.259425  -1.397    0.165  \nrer_dt_sa.l1 -0.178436   0.205745  -0.867    0.387  \npia.l1       -0.407730   0.843764  -0.483    0.630  \npi.l1        -0.986757   1.751487  -0.563    0.574  \nx.l1         -0.004487   0.023222  -0.193    0.847  \nq.l1          0.197841   0.087909   2.251    0.026 *\ny.l1          0.095053   0.207995   0.457    0.648  \nr.l1          0.790868   1.038220   0.762    0.448  \nw.l2          0.060895   0.264154   0.231    0.818  \nrer_dt_sa.l2  0.015055   0.213416   0.071    0.944  \npia.l2       -1.164964   0.840263  -1.386    0.168  \npi.l2         1.311820   1.725846   0.760    0.448  \nx.l2         -0.023495   0.022312  -1.053    0.294  \nq.l2         -0.127431   0.087354  -1.459    0.147  \ny.l2         -0.044173   0.215002  -0.205    0.838  \nr.l2         -0.436027   1.015011  -0.430    0.668  \nconst         0.019718   0.033046   0.597    0.552  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.0237 on 137 degrees of freedom\nMultiple R-Squared: 0.2203, Adjusted R-squared: 0.1292 \nF-statistic: 2.419 on 16 and 137 DF,  p-value: 0.00311 \n\n\nEstimation results for equation y: \n================================== \ny = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nw.l1         -0.003199   0.109057  -0.029  0.97664    \nrer_dt_sa.l1 -0.038732   0.086491  -0.448  0.65499    \npia.l1        0.211133   0.354701   0.595  0.55266    \npi.l1         0.598298   0.736289   0.813  0.41787    \nx.l1         -0.004104   0.009762  -0.420  0.67487    \nq.l1          0.034623   0.036955   0.937  0.35047    \ny.l1          0.415904   0.087437   4.757 4.93e-06 ***\nr.l1          0.324613   0.436446   0.744  0.45829    \nw.l2          0.051638   0.111045   0.465  0.64265    \nrer_dt_sa.l2  0.003376   0.089716   0.038  0.97004    \npia.l2       -0.210002   0.353229  -0.595  0.55315    \npi.l2         0.202028   0.725510   0.278  0.78108    \nx.l2          0.012840   0.009379   1.369  0.17324    \nq.l2          0.010711   0.036722   0.292  0.77096    \ny.l2          0.250580   0.090383   2.772  0.00634 ** \nr.l2         -0.302527   0.426689  -0.709  0.47952    \nconst        -0.012004   0.013892  -0.864  0.38907    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.009961 on 137 degrees of freedom\nMultiple R-Squared: 0.5263, Adjusted R-squared: 0.471 \nF-statistic: 9.513 on 16 and 137 DF,  p-value: 1.514e-15 \n\n\nEstimation results for equation r: \n================================== \nr = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n               Estimate Std. Error t value Pr(&gt;|t|)    \nw.l1          6.421e-03  1.833e-02   0.350   0.7266    \nrer_dt_sa.l1  8.568e-03  1.454e-02   0.589   0.5565    \npia.l1        1.247e-03  5.961e-02   0.021   0.9833    \npi.l1         3.173e-02  1.237e-01   0.256   0.7980    \nx.l1         -1.024e-05  1.641e-03  -0.006   0.9950    \nq.l1          2.224e-03  6.211e-03   0.358   0.7209    \ny.l1          1.710e-02  1.469e-02   1.164   0.2466    \nr.l1          1.452e+00  7.335e-02  19.795  &lt; 2e-16 ***\nw.l2         -4.633e-04  1.866e-02  -0.025   0.9802    \nrer_dt_sa.l2 -8.438e-03  1.508e-02  -0.560   0.5767    \npia.l2       -7.185e-03  5.936e-02  -0.121   0.9038    \npi.l2         1.998e-01  1.219e-01   1.639   0.1035    \nx.l2          5.996e-04  1.576e-03   0.380   0.7042    \nq.l2         -4.738e-03  6.172e-03  -0.768   0.4440    \ny.l2          2.734e-02  1.519e-02   1.800   0.0741 .  \nr.l2         -5.101e-01  7.171e-02  -7.114 5.72e-11 ***\nconst         1.049e-03  2.335e-03   0.449   0.6540    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.001674 on 137 degrees of freedom\nMultiple R-Squared: 0.9819, Adjusted R-squared: 0.9798 \nF-statistic: 463.8 on 16 and 137 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                   w  rer_dt_sa        pia         pi          x          q\nw          5.124e-05 -6.385e-06  1.420e-06 -1.763e-07  2.399e-05  1.197e-05\nrer_dt_sa -6.385e-06  9.187e-05 -3.941e-06 -2.456e-06 -3.212e-05  1.313e-05\npia        1.420e-06 -3.941e-06  2.034e-05  8.375e-06 -2.967e-05  3.334e-06\npi        -1.763e-07 -2.456e-06  8.375e-06  4.798e-06 -2.648e-05  3.513e-06\nx          2.399e-05 -3.212e-05 -2.967e-05 -2.648e-05  7.739e-03 -3.289e-04\nq          1.197e-05  1.313e-05  3.334e-06  3.513e-06 -3.289e-04  5.615e-04\ny         -1.998e-06  6.141e-06  1.200e-06  1.972e-06 -5.098e-05  6.238e-05\nr          4.063e-07  5.816e-07  1.953e-06  8.293e-07 -9.949e-06  1.473e-06\n                   y          r\nw         -1.998e-06  4.063e-07\nrer_dt_sa  6.141e-06  5.816e-07\npia        1.200e-06  1.953e-06\npi         1.972e-06  8.293e-07\nx         -5.098e-05 -9.949e-06\nq          6.238e-05  1.473e-06\ny          9.922e-05  3.801e-06\nr          3.801e-06  2.802e-06\n\nCorrelation matrix of residuals:\n                 w rer_dt_sa      pia       pi        x        q        y\nw          1.00000  -0.09307  0.04399 -0.01124  0.03809  0.07060 -0.02802\nrer_dt_sa -0.09307   1.00000 -0.09116 -0.11698 -0.03809  0.05781  0.06432\npia        0.04399  -0.09116  1.00000  0.84767 -0.07477  0.03119  0.02671\npi        -0.01124  -0.11698  0.84767  1.00000 -0.13741  0.06768  0.09036\nx          0.03809  -0.03809 -0.07477 -0.13741  1.00000 -0.15780 -0.05818\nq          0.07060   0.05781  0.03119  0.06768 -0.15780  1.00000  0.26430\ny         -0.02802   0.06432  0.02671  0.09036 -0.05818  0.26430  1.00000\nr          0.03391   0.03625  0.25864  0.22616 -0.06755  0.03713  0.22796\n                 r\nw          0.03391\nrer_dt_sa  0.03625\npia        0.25864\npi         0.22616\nx         -0.06755\nq          0.03713\ny          0.22796\nr          1.00000\n\n\n\n\n10.5.2 Restricted VAR\nWe impose some restrictions on the VAR:\n\na &lt;- diag(1, 8)\na[lower.tri(a)] &lt;- NA\na\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    1    0    0    0    0    0    0    0\n[2,]   NA    1    0    0    0    0    0    0\n[3,]   NA   NA    1    0    0    0    0    0\n[4,]   NA   NA   NA    1    0    0    0    0\n[5,]   NA   NA   NA   NA    1    0    0    0\n[6,]   NA   NA   NA   NA   NA    1    0    0\n[7,]   NA   NA   NA   NA   NA   NA    1    0\n[8,]   NA   NA   NA   NA   NA   NA   NA    1\n\n\n\nsvar_a &lt;- vars::SVAR(var_l1, Amat = a, max.iter = 500)\n\nWarning in vars::SVAR(var_l1, Amat = a, max.iter = 500): Convergence not\nachieved after 500 iterations. Convergence value: 8.95155527022712e-05 .\n\n\nThe matrix of the estimated coefficients:\n\nsvar_a$A\n\n                   w  rer_dt_sa        pia         pi           x          q\nw         1.00000000 0.00000000 0.00000000 0.00000000 0.000000000 0.00000000\nrer_dt_sa 0.10062174 1.00000000 0.00000000 0.00000000 0.000000000 0.00000000\npia       0.09683520 0.09746244 1.00000000 0.00000000 0.000000000 0.00000000\npi        0.09696978 0.09646253 0.09446819 1.00000000 0.000000000 0.00000000\nx         0.08956979 0.11302010 0.11451975 0.11371938 1.000000000 0.00000000\nq         0.08405673 0.08343476 0.09004483 0.09074844 0.043870980 1.00000000\ny         0.08370154 0.07915368 0.08356944 0.08389639 0.010703005 0.04580249\nr         0.08307618 0.08163940 0.08355461 0.08470402 0.007045767 0.06823200\n                  y r\nw         0.0000000 0\nrer_dt_sa 0.0000000 0\npia       0.0000000 0\npi        0.0000000 0\nx         0.0000000 0\nq         0.0000000 0\ny         1.0000000 0\nr         0.0839859 1\n\n\n\nsave(svar_a, df_var, file = \"../R/output/var_objects_piscop.rda\")\n\n\n\n10.5.3 Impulse Response Functions\nLet us focus on the impulse to the weather aggregate cost equation, with 95% confidence bootstrapped error bands (500 runs).\n\nimpulse_name &lt;- \"w\"\n\n\nnb_runs &lt;- 500\n\nirfs_95 &lt;- vars::irf(\n  svar_a, impulse = impulse_name, boot = TRUE, ci = .95, \n  n.ahead = 20, runs = nb_runs\n)\n\nAnd with 68% confidence bootstrapped error bands (500 runs, again).\n\nirfs_68 &lt;- vars::irf(\n  svar_a, impulse = impulse_name, boot = TRUE, ci = .68, \n  n.ahead = 20, runs = nb_runs\n)\n\nLet us save the results:\n\nsave(irfs_95, file = \"../R/output/irfs_95_piscop.rda\")\nsave(irfs_68, file = \"../R/output/irfs_68_piscop.rda\")\n\n\nlevels &lt;- variable_names\nlabels &lt;- names(variable_names)\n\nNow, we can plot the resulting IRFs. Let us prepare the data for the response:\n\ndf_irfs &lt;- \n  irfs_95$irf[[impulse_name]] |&gt; \n  as_tibble() |&gt; \n  mutate(horizon = row_number()) |&gt; \n  pivot_longer(cols = -horizon) |&gt; \n  mutate(impulse = !!impulse_name)\n\ndf_irfs &lt;- \n  df_irfs |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = !!levels,\n      labels = !!labels\n    )\n  )\n\nThe error bands:\n\ndf_irfs_ci &lt;- \n  map(\n    .x = list(`95` = irfs_95, `68` = irfs_68),\n    .f = function(irf_lvl) {\n      map(\n        .x = list(\n          lower = irf_lvl$Lower[[impulse_name]], \n          mean = irf_lvl$irf[[impulse_name]],\n          upper = irf_lvl$Upper[[impulse_name]]),\n        .f = ~ .x |&gt; \n          as_tibble() |&gt; \n          mutate(horizon = row_number()) |&gt; \n          pivot_longer(cols = -horizon, values_to = \"bound\") |&gt; \n          mutate(impulse = !!impulse_name)\n      ) |&gt; \n        list_rbind(names_to = \"bound_type\")\n    }\n  ) |&gt; \n  list_rbind(names_to = \"level\") |&gt; \n  pivot_wider(names_from = bound_type, values_from = bound)\n\ndf_irfs_ci &lt;- \n  df_irfs_ci |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = !!levels,\n      labels = !!labels),\n    level = factor(level, levels = c(68, 95), labels = c(\"68%\", \"95%\"))\n  )\n\nAnd lastly, the graph.\n\nggplot() +\n  geom_ribbon(\n    data = df_irfs_ci,\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper,\n      fill = level),\n    alpha = .3\n  ) +\n  geom_line(\n    data = df_irfs_ci,\n    mapping = aes(x = horizon, y = mean),\n    colour = \"#0072B2\"\n  ) +\n  geom_hline(yintercept = 0, colour = \"#444444\") +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"#0072B2\", \"95%\" = \"#56B4E9\")) +\n  facet_wrap(~name, scales = \"free_y\", ncol = 4) +\n  theme_paper() +\n  coord_cartesian(xlim = c(1, 20))\n\n\n\n\n\n\n\nFigure 10.2: VAR(2) system response to one standard deviation orthogonal shock to the weather aggregate cost equation\n\n\n\n\n\nThe exact figure replication:\n\n\nCode\nlibrary(tikzDevice)\nlibrary(lemon)\n\nggplot() +\n  geom_ribbon(\n    data = df_irfs_ci |&gt; \n      filter(level == \"68%\") |&gt; \n      mutate(level = str_replace(level, \"\\\\%\", \"\\\\\\\\%\")),\n    mapping = aes(x = horizon,\n                  ymin = lower, ymax = upper,\n                  fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs,\n    mapping = aes(x = horizon, y = value),\n    colour = \"#009E73\"\n  ) +\n  geom_hline(yintercept = 0, colour = \"#444444\") +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68\\\\%\" = \"gray10\", \"95\\\\%\" = \"gray60\"),\n    guide = \"none\") +\n  facet_rep_wrap(~name, scales = \"free_y\", repeat.tick.labels = TRUE, ncol = 4) +\n  theme_paper() +\n  coord_cartesian(xlim = c(1, 20))\n\n\n\n\n\n\n\n\nFigure 10.3: VAR(2) system response to one standard deviation orthogonal shock to the weather aggregate cost equation. The gray bands depict the 68% confidence intervals.",
    "crumbs": [
      "Macroeconomic Impacts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>From Regional to Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "ag-fluctuations_piscop.html#estimation-with-local-projections",
    "href": "ag-fluctuations_piscop.html#estimation-with-local-projections",
    "title": "10  From Regional to Aggregate Fluctuations",
    "section": "10.6 Estimation with Local Projections",
    "text": "10.6 Estimation with Local Projections\nWe can also estimate the propagation of the shock using local projections rather than VAR.\nLet us build the table with the data:\n\ndf_lp &lt;- \n  df_var |&gt; \n  filter(date &gt;= start_date) |&gt; \n  select(\n    w, rer_dt_sa, pia, pi, x, q, y, r\n  )\n\nThe linear impulse response functions can be estimated using the lp_lin() function from {lpirfs}.\n\nlibrary(lpirfs)\n# With 95% conf. int.\nresults_lin_95 &lt;- lp_lin(\n  endog_data     = df_lp,\n  lags_endog_lin = 2,# 2 lags\n  trend          = 0,# no trend\n  shock_type     = 0,# std dev. shock\n  confint        = 1.96,\n  hor            = 20\n)\n# With 68% conf. int.\nresults_lin_68 &lt;- lp_lin(\n  endog_data     = df_lp,\n  lags_endog_lin = 2,# 2 lags\n  trend          = 0,# no trend\n  shock_type     = 0,# std dev. shock\n  confint        = 1,\n  hor            = 20\n)\n\nWe define a small function, get_irfs(), to format the IRFs from the object returned by lp_lin().\n\nget_irfs &lt;- function(resul_lin) {\n  irf_lin_mean &lt;- resul_lin[[\"irf_lin_mean\"]]\n  irf_lin_low &lt;- resul_lin[[\"irf_lin_low\"]]\n  irf_lin_up &lt;- resul_lin[[\"irf_lin_up\"]]\n  specs &lt;- resul_lin$specs\n  \n  irfs_df &lt;- NULL\n  for (rr in 1:(specs$endog)) {\n    for (ss in 1:(specs$endog)) {\n      tbl_lin_mean &lt;- as.matrix(t(irf_lin_mean[, 1:specs$hor, ss]))[, rr]\n      tbl_lin_low &lt;- as.matrix(t(irf_lin_low[, 1:specs$hor, ss]))[, rr]\n      tbl_lin_up &lt;- as.matrix(t(irf_lin_up[, 1:specs$hor, ss]))[, rr]\n      tbl_lin &lt;- tibble(\n        horizon = seq_along(tbl_lin_mean), \n        mean = tbl_lin_mean, \n        low = tbl_lin_low, \n        up = tbl_lin_up,\n        shocked = specs$column_names[ss],\n        on = specs$column_names[rr]\n      )\n      irfs_df &lt;- bind_rows(irfs_df, tbl_lin)\n    }\n  }\n  \n  irfs_df &lt;- \n    irfs_df |&gt;\n    mutate(\n      shocked = factor(shocked, levels = variable_names, labels = names(variable_names)),\n      on = factor(on, levels = variable_names, labels = names(variable_names))\n    )\n  \n  irfs_df\n}\n\nThe formatted IRFs:\n\nirfs_df &lt;- \n  get_irfs(results_lin_95) |&gt; mutate(level = \"95%\") |&gt; \n  bind_rows(\n    get_irfs(results_lin_68) |&gt; mutate(level = \"68%\")\n  )\n\nAnd the figure:\n\n\nCode\nggplot() +\n  geom_ribbon(\n    data = irfs_df |&gt; filter(shocked == \"Agricultural losses\"), \n    mapping = aes(\n      x = horizon,\n      ymin = low, ymax = up,\n      fill = level),\n    alpha = .3\n  ) +\n  geom_line(\n    data = irfs_df |&gt; filter(shocked == \"Agricultural losses\"), ,\n    mapping = aes(x = horizon, y = mean),\n    colour = \"#0072B2\"\n  ) +\n  geom_hline(yintercept = 0, colour = \"#444444\") +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"#0072B2\", \"95%\" = \"#56B4E9\")) +\n  facet_wrap(~on, scales = \"free_y\", ncol = 4) +\n  theme_paper() +\n  coord_cartesian(xlim = c(1, 20)) +\n  scale_y_continuous(labels = scales::label_percent())\n\n\n\n\n\n\n\n\nFigure 10.4: System response to one standard deviation orthogonal shock to the weather aggregate cost equation. Estimations made with local projections.\n\n\n\n\n\nAnd with the 68% confidence interval only:\n\n\nCode\nlibrary(tikzDevice)\nlibrary(lemon)\nggplot(\n    data = irfs_df |&gt; filter(shocked == \"Agricultural losses\") |&gt; \n      filter(level == \"68%\") |&gt; \n      mutate(level = str_replace(level, \"\\\\%\", \"\\\\\\\\%\")) |&gt; \n      mutate(\n        mean = mean * 100,\n        low = low * 100,\n        up = up * 100\n      )\n  ) +\n  geom_ribbon(\n    mapping = aes(x = horizon,ymin = low, ymax = up, fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    mapping = aes(x = horizon, y = mean),\n    colour = \"#009E73\"\n  ) +\n  geom_hline(yintercept = 0, colour = \"#444444\") +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68\\\\%\" = \"gray10\", \"95\\\\%\" = \"gray60\"),\n    guide = \"none\") +\n  facet_rep_wrap(~on, scales = \"free_y\", repeat.tick.labels = TRUE, ncol = 4) +\n  theme_paper() +\n  coord_cartesian(xlim = c(1, 20))\n\n\n\n\n\n\n\n\nFigure 10.5: System response to one standard deviation orthogonal shock to the weather aggregate cost equation. Estimations made with local projections. The gray bands depict the 68% confidence intervals.",
    "crumbs": [
      "Macroeconomic Impacts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>From Regional to Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "ag-fluctuations_piscop.html#var-vs-lp",
    "href": "ag-fluctuations_piscop.html#var-vs-lp",
    "title": "10  From Regional to Aggregate Fluctuations",
    "section": "10.7 VAR vs LP",
    "text": "10.7 VAR vs LP\nLet us graph the IRFs for both the VAR and the LPs on a single Figure.\n\n\nCode\ndf_plot_comparison &lt;- \n  df_irfs_ci |&gt; \n  filter(level == \"68%\") |&gt; \n  mutate(level = str_replace(level, \"\\\\%\", \"\\\\\\\\%\")) |&gt; \n  mutate(estimation = \"VAR(2)\") |&gt; \n  rename(on = name, shocked = impulse) |&gt; \n  bind_rows(\n    irfs_df |&gt; filter(shocked == \"Agricultural losses\") |&gt; \n      filter(level == \"68%\") |&gt; \n      mutate(level = str_replace(level, \"\\\\%\", \"\\\\\\\\%\")) |&gt; \n      mutate(\n        mean = mean * 100,\n        lower = low * 100,\n        upper = up * 100\n      ) |&gt; \n      select(-low, -up) |&gt; \n      mutate(estimation = \"Local Projections\")\n  ) |&gt; \n  mutate(\n    estimation = factor(estimation, levels = c(\"VAR(2)\", \"Local Projections\"))\n  )\n\ncolour_1 &lt;- \"#E69F00\"\ncolour_2 &lt;- \"#009E73\"\n\np_comparison &lt;- ggplot(\n  data = df_plot_comparison\n) +\n  geom_ribbon(\n    mapping = aes(x = horizon,ymin = lower, ymax = upper, fill = estimation),\n    alpha = .4\n  ) +\n  geom_line(\n    mapping = aes(x = horizon, y = mean, colour = estimation, linetype = estimation)\n  ) +\n  geom_hline(yintercept = 0, colour = \"#444444\") +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    NULL,\n    values = c(\"VAR(2)\" = colour_1, \"Local Projections\" = colour_2)\n  ) +\n  scale_colour_manual(\n    NULL,\n    values = c(\"VAR(2)\" = colour_1, \"Local Projections\" = colour_2)\n  ) +\n  scale_linetype_manual(\n    NULL,\n    values = c(\"VAR(2)\" = \"solid\", \"Local Projections\" = \"dashed\")\n  ) +\n  facet_rep_wrap(~on, scales = \"free_y\", repeat.tick.labels = TRUE, ncol = 4) +\n  theme_paper() +\n  coord_cartesian(xlim = c(0, 20))\np_comparison\n\n\n\n\n\n\n\n\nFigure 10.6: System response to one standard deviation orthogonal shock to the weather aggregate cost equation for VAR(2) and Local Projections frameworks. The bands depict the 68% confidence intervals.",
    "crumbs": [
      "Macroeconomic Impacts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>From Regional to Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "robustness-data-merge-quarter.html",
    "href": "robustness-data-merge-quarter.html",
    "title": "11  Merging: quarterly data",
    "section": "",
    "text": "11.1 Load Intermediate Files\nThe (quarterly) weather data (Chapter 1) can be loaded:\nload(\"../data/output/weather/weather_quarter_regions_df.rda\")\nThe agricultural data (Chapter 2):\nload(\"../data/output/minagri/dataset_agri_2001_2015.rda\")\nThe macroeconomic data (Chapter 3):\nload(\"../data/output/macro/df_macro.rda\")\nThe share of natural regions and the El Niño–Southern Oscillation (Chapter 4):\nload(\"../data/output/natural_region_dep.rda\")\nload(\"../data/output/weather/ONI_temp.rda\")",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Merging: quarterly data</span>"
    ]
  },
  {
    "objectID": "robustness-data-merge-quarter.html#merge-the-files",
    "href": "robustness-data-merge-quarter.html#merge-the-files",
    "title": "11  Merging: quarterly data",
    "section": "11.2 Merge the Files",
    "text": "11.2 Merge the Files\nWe add ENSO data to the weather dataset:\n\nWeather &lt;- weather_quarter_regions_df |&gt; \n  # Add ENSO data\n  left_join(\n    ONI_temp |&gt; \n      mutate(quarter = quarter(date)) |&gt; \n      mutate(\n        year = as.numeric(Year), \n        quarter = as.numeric(quarter)) |&gt; \n      group_by(year, quarter) |&gt; \n      summarise(ONI = mean(ONI)), \n    by = c(\n      \"year\" = \"year\",\n      \"quarter\" = \"quarter\"\n    )\n  ) |&gt;\n  group_by(IDDPTO, quarter) |&gt; \n  mutate( \n    temp_min_dev_ENSO   = temp_min - mean(temp_min),\n    temp_max_dev_ENSO   = temp_max - mean(temp_max),\n    temp_mean_dev_ENSO  = temp_mean - mean(temp_mean),\n    precip_sum_dev_ENSO = precip_sum - mean(precip_sum),\n    precip_piscop_sum_dev_ENSO = precip_piscop_sum - mean(precip_piscop_sum))|&gt; \n  ungroup() |&gt; \n  labelled::set_variable_labels(\n    temp_min_dev_ENSO   = \"Deviation of Min. Temperature from ENSO Normals\",\n    temp_max_dev_ENSO   = \"Deviation of Max. Temperature from ENSO Normals\",\n    temp_mean_dev_ENSO  = \"Deviation of Mean Temperature from ENSO Normals\",\n    precip_sum_dev_ENSO = \"Deviation of Total Rainfall from ENSO Normals\",\n    precip_piscop_sum_dev_ENSO = \"Deviation of Total Rainfall from ENSO Normals (Piscop)\"\n  )\n\nFor international prices:\n\nlibrary(readxl)\nint_prices &lt;- read_excel(\n  path = \"../data/raw/Macro/IMF_DATA.xls\",\n  sheet = \"SELECTED\",\n  col_types = \"text\") |&gt; \n  mutate(date = lubridate::ymd(str_c(YEAR,  \"-\", MONTH, \"-01\"))) |&gt; \n  rename(\n    \"FPI\"           = \"FPI_PFOOD\", \n    \"FERTILIZER\"    = \"FERTILIZER_PFERT\",   \n    \"IndexOIL\"      = \"IndexOIL_POILAPSP\",\n    \"PriceOIL\"      = \"PriceOIL_POILAPSP\", \n    \"CORN\"          = \"CORN_PMAIZMT\", \n    \"ARROZ CÁSCARA\" = \"RICE_PRICENPQ\", \n    \"TRIGO\"         = \"WHEAT_PWHEAMT\"\n  ) |&gt; \n  mutate(CORN2 = CORN, \n         FPI2 = FPI) |&gt; \n  select(-c(MONTH, YEAR, CPI_Peru_IMF, CPI_US_IMF, IndexOIL, PriceOIL)) |&gt; \n  pivot_longer(cols = !date, names_to = \"product\", values_to = \"price_int\") |&gt; \n  mutate(\n    product = case_when(\n      product == \"FPI\"  ~ \"PAPA\", \n      product == \"FPI2\" ~ \"YUCA\", \n      product == \"CORN\" ~ \"MAÍZ AMILÁCEO\", \n      product == \"CORN2\"~ \"MAÍZ AMARILLO DURO\", \n      TRUE ~ product\n    ), \n    product_eng = case_when(\n      product == \"PAPA\" ~ \"Potato\",\n      product == \"YUCA\" ~ \"Cassava\",\n      product == \"ARROZ CÁSCARA\" ~ \"Rice\",\n      product == \"MAÍZ AMARILLO DURO\" ~ \"Dent corn\"\n    ),\n    price_int = as.numeric(price_int)\n  ) |&gt; \n  arrange(product, date) |&gt; \n  mutate(\n    quarter = quarter(date), \n    year    = year(date)\n  ) |&gt;  \n  group_by(product, quarter, year) |&gt; \n  mutate(price_int = mean(price_int)) |&gt;  \n  ungroup() |&gt; \n  select(-date) |&gt; \n  unique() |&gt; \n  group_by(product, quarter) |&gt; \n  arrange(year, quarter) |&gt; \n  mutate(int_price_inf = (price_int/lag(price_int) - 1) * 100) |&gt; \n  ungroup() |&gt; \n  arrange(product, year, quarter) |&gt; \n  filter(! year == 2000)\n\nLet us merge all these datasets in a single one:\n\ndata_total &lt;- \n  data_total |&gt; \n  # Add macroeconomic data\n  left_join(\n    df_macro |&gt; rename(gdp = y),\n    by = \"date\"\n  ) |&gt; \n  mutate(quarter = quarter(date)) |&gt; \n  dplyr::select(\n    product_eng, region,region_id, product, quarter, year, Value_prod,\n    rer_hp, r_hp, pi, ind_prod) |&gt; \n  group_by(region, product, quarter, year) |&gt; \n  mutate(\n    Value_prod     = sum(Value_prod),   \n    rer_hp         = mean(rer_hp),\n    r_hp           = mean(r_hp),\n    pi             = mean(pi),\n    ind_prod       = mean(ind_prod)\n  ) |&gt;  \n  ungroup() |&gt; \n  unique() |&gt; \n  # Add commodity prices data\n  left_join(\n    int_prices,\n    by =  c(\n      \"product\", \"product_eng\", \"year\", \"quarter\")\n  ) |&gt; \n  group_by(product, region) |&gt; \n  mutate(\n    int_price_inf = mFilter::hpfilter(\n      int_price_inf, freq = 1600, type = \"lambda\")[[\"cycle\"]]\n  ) |&gt; \n  # Add weather data and ENSO \n  left_join(\n    Weather |&gt; \n      dplyr::select(-IDDPTO),\n    by = c(\n      \"year\" = \"year\",\n      \"quarter\" = \"quarter\",\n      \"region\" = \"DEPARTAMEN\"\n    )\n  ) \n\n\nsave(data_total, file = \"../data/output/dataset_2001_2015-quarter.rda\")\n\nHere are the first rows of that tibble:\n\ndata_total\n\n# A tibble: 8,640 × 46\n# Groups:   product, region [144]\n   product_eng  region region_id product quarter  year Value_prod  rer_hp   r_hp\n   &lt;chr&gt;        &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 Amylaceous … AMAZO…         1 MAÍZ A…       1  2001      1242.  1.71    1.61 \n 2 Amylaceous … AMAZO…         1 MAÍZ A…       2  2001      2038   1.33    5.58 \n 3 Amylaceous … AMAZO…         1 MAÍZ A…       3  2001      6092. -1.24   -0.599\n 4 Amylaceous … AMAZO…         1 MAÍZ A…       4  2001       738  -2.13   -2.64 \n 5 Amylaceous … AMAZO…         1 MAÍZ A…       1  2002      1217  -0.916  -2.98 \n 6 Amylaceous … AMAZO…         1 MAÍZ A…       2  2002      3263  -1.41   -2.36 \n 7 Amylaceous … AMAZO…         1 MAÍZ A…       3  2002      6762   1.36   -0.570\n 8 Amylaceous … AMAZO…         1 MAÍZ A…       4  2002       251   0.0995  0.268\n 9 Amylaceous … AMAZO…         1 MAÍZ A…       1  2003       524  -0.762   0.324\n10 Amylaceous … AMAZO…         1 MAÍZ A…       2  2003      1394   0.0216  0.617\n# ℹ 8,630 more rows\n# ℹ 37 more variables: pi &lt;dbl&gt;, ind_prod &lt;dbl&gt;, price_int &lt;dbl&gt;,\n#   int_price_inf &lt;dbl&gt;, temp_min &lt;dbl&gt;, temp_max &lt;dbl&gt;, temp_mean &lt;dbl&gt;,\n#   precip_sum &lt;dbl&gt;, precip_piscop_sum &lt;dbl&gt;, perc_gamma_precip &lt;dbl&gt;,\n#   perc_gamma_precip_piscop &lt;dbl&gt;, gdd_rice &lt;dbl&gt;, gdd_maize &lt;dbl&gt;,\n#   gdd_potato &lt;dbl&gt;, gdd_cassava &lt;dbl&gt;, hdd_rice &lt;dbl&gt;, hdd_maize &lt;dbl&gt;,\n#   hdd_potato &lt;dbl&gt;, hdd_cassava &lt;dbl&gt;, temp_min_dev &lt;dbl&gt;, …",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Merging: quarterly data</span>"
    ]
  },
  {
    "objectID": "robustness-data-merge-quarter.html#dataset-for-the-local-projections",
    "href": "robustness-data-merge-quarter.html#dataset-for-the-local-projections",
    "title": "11  Merging: quarterly data",
    "section": "11.3 Dataset for the Local Projections",
    "text": "11.3 Dataset for the Local Projections\nNow, let us create the dataset specifically used to estimate the models.\nLet us make sure that the region data are encoded as a factor.\n\ndata_total &lt;- \n  data_total |&gt; \n  mutate(region_id = factor(region_id))\n\nThe crops we focus on:\n\ncrops &lt;- c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\")\n\nThe number of observation in each region, for each crop:\n\ndata_total |&gt; \n  group_by(product_eng, region_id) |&gt; \n  summarise(n = sum(Value_prod &lt;= 0)) |&gt; \n  arrange(desc(n))\n\n`summarise()` has grouped output by 'product_eng'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 144 × 3\n# Groups:   product_eng [6]\n   product_eng     region_id     n\n   &lt;chr&gt;           &lt;fct&gt;     &lt;int&gt;\n 1 Amylaceous corn 15           60\n 2 Amylaceous corn 21           60\n 3 Amylaceous corn 23           60\n 4 Amylaceous corn 24           60\n 5 Potato          15           60\n 6 Potato          21           60\n 7 Potato          23           60\n 8 Potato          24           60\n 9 Rice            3            60\n10 Rice            8            60\n# ℹ 134 more rows\n\n\n\n11.3.1 Definition of the Variable of Interest\n\n\n\n\n\n\nWarning\n\n\n\nWe compute percentage deviation of production from quarterly regional average, but we will actually not use those values in the subsequent estimations. However, we will use the demeanded values, where missing values were, if possible, imputed.\n\n\nThis section outlines a two-step procedure for expressing agricultural production data at the quarterly regional level for a specific crop and quarter as a percentage deviation from the quarterly regional crop-specific average. The procedure involves handling missing values.\n\nStep 1: Handling Missing Values\nIn the first step, we address missing values by linear interpolation. This approach helps us estimate the missing values by considering the neighboring data points.\n\nStep 1.1: Imputing missing values with linear interpolation.\nThe missing values get replaced by linear interpolation. However, if there are more than two consecutive missing values, they are not replaced with interpolated values. Instead, the series for the specific crop in the given region is split based on the locations of the missing values. The split with the highest number of consecutive non-missing values is retained, while the other splits are discarded.\nStep 1.2: Dropping Series with Remaining Missing Values\nAfter imputing missing values using the moving median, we check if any missing values still remain in the dataset. If there are any remaining missing values for a particular series, we choose to exclude that series from further analysis. By doing so, we ensure that the subsequent detrending process is performed only on reliable and complete data.\n\nStep 2: Normalized Agricultural Production\nFor each month ( m ), region ( i ), and crop ( c ), we calculate the average production over the entire period (January 2001 to December 2015): \\[\\overline{y}_{c,i,m} = \\frac{1}{n_{T_c}} \\sum_{t=1}^{T_c} y_{c,i,m,t}^{\\text{raw}}\n  \\] Then, we express agricultural production relative to the average: \\[y_{c,i,m,t} = \\begin{cases}\n  \\frac{y_{c,i,m,t}^{\\text{raw}}}{\\overline{y}_{c,i,m}}, & \\overline{y}_{c,i,m} &gt; 0\\\\\n  0, & \\overline{y}_{c,i,m} = 0\n  \\end{cases}\\] Values of \\(y_{c,i,m,t}&gt;1\\) means that the production for crop \\(c\\) in region \\(i\\) during month \\(m\\) of year \\(t\\) is higher than the average monthly production for that crop and region over the period 2001 to 2015. For example, a value of 1.5 means that the production is 50% higher than average.\nStep 2 (alternative version): Deviation from regional monthly average, in percent (this step is useless in the new version of the analysis: it lead to discard too many observations)\nOnce we have addressed the missing values, we proceed to the second step, which consists in computing the deviation of production from the quarterly regional average. First, we compute the average production of each crop \\(c\\) in each region \\(i\\) for quarter \\(q\\): \\[\\overline{y}_{c,i,q} = \\frac{1}{n_{T_c}} \\sum_{t=1}^{T_c} y_{c,i,q,t}^{raw}\\] Then, we compute the percentage deviation from this average at each date \\(t\\): \\[y_{c,i,q,t} = \\frac{y_{c,i,q,t}^{raw} - \\overline{y}_{c,i,q}}{\\overline{y}_{c,i,q}}\\]\n\nLet us implement this process in R. First, we need to define two functions to handle the missing values:\n\nThe get_index_longest_non_na() function retrieves the indices of the longest consecutive sequence without missing values from a given input vector. It helps us identify the positions of elements in that sequence.\nThe keep_values_longest_non_na() function uses the obtained indices to create a logical vector. Each element of this vector indicates whether the corresponding element in the input vector belongs to the longest consecutive sequence of non-missing values. This allows us to filter the data and retain only the values from the longest consecutive sequence without missing values.\n\nThese two functions combined help us handle missing data in the weather series and ensure that we work with the most complete sequences for each region and crop.\nThe first function:\n\n#' Returns the index of the longest sequence of non NA values in a vector\n#'\n#' @param y vector of numerical values\n#' @export\nget_index_longest_non_na &lt;- function(y) {\n  split_indices &lt;- which(is.na(y))\n  nb_obs &lt;- length(y)\n\n  if (length(split_indices) == 0) {\n    res &lt;- seq_len(nb_obs)\n  } else {\n    idx_beg &lt;- c(1, split_indices)\n    if (idx_beg[length(idx_beg)] != nb_obs) {\n      idx_beg &lt;- c(idx_beg, nb_obs)\n    }\n    lengths &lt;- diff(idx_beg)\n    ind_max &lt;- which.max(lengths)\n    index_beginning &lt;- idx_beg[ind_max]\n    if(!index_beginning == 1 | is.na(y[index_beginning])) {\n      index_beginning &lt;- index_beginning + 1\n    }\n    index_end &lt;- idx_beg[ind_max] + lengths[ind_max]\n    if(is.na(y[index_end])) {\n      index_end &lt;- index_end - 1\n    }\n    res &lt;- seq(index_beginning, index_end)\n  }\n  res\n}\n\nThe second one:\n\n#' Returns a logical vector that identifies the longest sequence of non NA\n#' values within the input vector\n#' \n#' @param y numeric vector\nkeep_values_longest_non_na &lt;- function(y) {\n  ids_to_keep &lt;- get_index_longest_non_na(y)\n  ids &lt;- seq(1, length(y))\n  ids %in% ids_to_keep\n}\n\n\n\n\n\n\n\nNote\n\n\n\nThose two functions are defined in weatherperu/R/utils.R.\n\n\nWe define a function, pct_prod_production(), that takes the data frame of observations as input, as well as a crop name and a region ID. It returns a tibble with the following variables:\n\nproduct_eng: the English name of the crop\nregion_id: the ID of the region\nyear: year\nquarter: quarter number\ny_new_normalized (our variable of interest in Chapter 13): the production demeaned by the month-specific average for the crop of interest in the region of interest\ny_new: the production (in tons) where missing values were imputed, if possible\ny_dev_pct: the production expressed as the percentage deviation from the monthly-specific average (for the crop of interest, in the region of interest)\ny: same as y_dev_pct but without an estimated month-specific quadratic trend estimated by OLS\nt: month-specific trend.\n\n\n#' Computes the percentage deviation of production from quarterly regional average\n#'\n#' @param df data\n#' @param crop_name name of the crop\n#' @param region_id id of the region\n#'\n#' @returns data frame with the product, the region id, the date, the production\n#' with imputed missing values (`y_new`), the production demeaned by the monthly\n#' mean production (`y_new_normalized`), the percentage deviation from monthly\n#' mean production (`y_dev_pct`), the percentage deviation from monthly mean\n#' production minus an estimated quadratic trend (estimated by OLS) (`y`), and,\n#' a trend (`t`)\n#' @export\n#' @importFrom dplyr filter arrange mutate select row_number group_by\n#' @importFrom tidyr nest unnest\n#' @importFrom purrr map\n#' @importFrom imputeTS na_interpolation\n#' @importFrom stats lm predict residuals\npct_prod_production &lt;- function(df,\n                                crop_name,\n                                region_id) {\n  # The current data\n  df_current &lt;-\n    df |&gt;\n    filter(\n      product_eng == !!crop_name,\n      region_id == !!region_id\n    ) |&gt;\n    arrange(year, quarter)\n\n  ## Dealing with missing values ----\n  # Look for negative production values\n  df_current &lt;-\n    df_current |&gt;\n    mutate(\n      y_new = ifelse(Value_prod &lt; 0, NA, Value_prod)\n    )\n\n  if (any(is.na(df_current$y_new))) {\n\n    # Replacing NAs by interpolation\n    # If there are more than two contiguous NAs, they are not replaced\n    df_current &lt;-\n      df_current |&gt;\n      mutate(\n        y_new = imputeTS::na_interpolation(y_new, maxgap = 3)\n      )\n\n    # Removing obs at the beginning/end if they are still missing\n    df_current &lt;-\n      df_current |&gt;\n      mutate(\n        row_to_keep = !(is.na(y_new) & row_number() %in% c(1:2, (n()-1):(n())))\n      ) |&gt;\n      filter(row_to_keep) |&gt;\n      select(-row_to_keep)\n\n    # Keeping the longest series of continuous non-NA values\n    df_current &lt;-\n      df_current |&gt;\n      mutate(\n        row_to_keep = keep_values_longest_non_na(y_new)\n      ) |&gt;\n      filter(row_to_keep) |&gt;\n      select(-row_to_keep)\n  }\n\n  rle_y_new &lt;- rle(df_current$y_new)\n  check_contiguous_zeros &lt;- rle_y_new$lengths[rle_y_new$values==0]\n\n\n  ## Percent deviation from monthly regional average\n  resul &lt;-\n    df_current |&gt;\n    group_by(quarter) |&gt;\n    mutate(\n      y_new_normalized = case_when(\n        mean(y_new) == 0~ 0,\n        TRUE ~ y_new / mean(y_new)\n      ),\n      y_dev_pct = case_when(\n        mean(y_new) == 0 ~ 0,\n        TRUE ~ (y_new - mean(y_new)) / mean(y_new)\n      )\n    ) |&gt;\n    group_by(quarter) |&gt;\n    arrange(year, quarter) |&gt;\n    mutate(t = row_number()) |&gt;\n    ungroup() |&gt;\n    nest(.by = c(product_eng, region_id, quarter)) |&gt;\n    # distinct OLS per quarter\n    mutate(\n      ols_fit   = map(data, ~ lm(y_new_normalized ~ -1 + t + I(t^2), data = .x)),\n      resid     = map(ols_fit, residuals),\n      fitted    = map(ols_fit, predict)\n    ) |&gt;\n    unnest(cols = c(data, resid, fitted)) |&gt;\n    group_by(quarter) |&gt;\n    mutate(\n      y = resid\n    ) |&gt;\n    select(\n      product_eng, region_id, year, quarter,\n      y_new, y_dev_pct, y_new_normalized, y, t\n    ) |&gt;\n    ungroup() |&gt;\n    arrange(year, quarter)\n\n  resul\n}\n\nFor example, for potatoes in region with id 1:\n\npct_prod_production(\n  df = data_total, \n  crop_name = \"Potato\", \n  region_id = 1\n)\n\n# A tibble: 60 × 9\n   product_eng region_id  year quarter  y_new y_dev_pct y_new_normalized     y\n   &lt;chr&gt;       &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 Potato      1          2001       1  9786    -0.267             0.733 0.452\n 2 Potato      1          2001       2 14648.   -0.0919            0.908 0.660\n 3 Potato      1          2001       3 14670.   -0.230             0.770 0.549\n 4 Potato      1          2001       4 10972    -0.167             0.833 0.592\n 5 Potato      1          2002       1 14682.    0.0999            1.10  0.573\n 6 Potato      1          2002       2 15682.   -0.0278            0.972 0.504\n 7 Potato      1          2002       3 14384    -0.245             0.755 0.336\n 8 Potato      1          2002       4 12348    -0.0629            0.937 0.482\n 9 Potato      1          2003       1 12597    -0.0563            0.944 0.205\n10 Potato      1          2003       2 17262     0.0702            1.07  0.411\n# ℹ 50 more rows\n# ℹ 1 more variable: t &lt;int&gt;\n\n\nWe can apply this function to all crops of interest, in each region. Let us define a table that contains all the possible values for the combination of crops and regions:\n\nproduct_and_regions &lt;- \n  data_total |&gt; \n  filter(product_eng %in% crops) |&gt; \n  dplyr::select(product_eng, region_id, region) |&gt; \n  unique()\n\nAdding missing grouping variables: `product`\n\n\nThen we apply the pct_prod_production() function to all these different cases, and store the results in a list named df_pct_pred_production:\n\ndf_pct_pred_production &lt;- vector(mode = \"list\", length = nrow(product_and_regions))\ncli_progress_bar(total = nrow(product_and_regions))\nfor(i in 1:nrow(product_and_regions)){\n  df_pct_pred_production[[i]] &lt;- pct_prod_production(\n    df = data_total, \n    crop_name = product_and_regions$product_eng[i], \n    region_id = product_and_regions$region_id[i]\n  )\n  cli_progress_update(set = i)\n}\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n■■■■■■■■■■■■                      36% | ETA:  2s\n\n\n■■■■■■■■■■■■■■■■■■■■■■■■■■■       88% | ETA:  0s\n\n\n■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% | ETA:  0s\n\n\nThe elements of the list are all tibbles, with the same column names. We can merge them in a single tibble.\n\ndf_pct_pred_production &lt;- bind_rows(df_pct_pred_production)\n\nWe can have a look at the number of quarters with 0 values for the agricultural production.\n\ndf_pct_pred_production |&gt; \n  group_by(product_eng, region_id) |&gt; \n  summarise(nb_0 = sum(y_new == 0)) |&gt; \n  arrange(desc(nb_0))\n\n`summarise()` has grouped output by 'product_eng'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 96 × 3\n# Groups:   product_eng [4]\n   product_eng region_id  nb_0\n   &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt;\n 1 Cassava     8            60\n 2 Cassava     22           60\n 3 Potato      15           60\n 4 Potato      16           60\n 5 Potato      21           60\n 6 Potato      23           60\n 7 Potato      24           60\n 8 Rice        3            60\n 9 Rice        8            60\n10 Rice        10           60\n# ℹ 86 more rows\n\n\nNow, let us add the other columns to the tibble that contains the new data:\n\ndf &lt;- df_pct_pred_production |&gt; \n  left_join(\n    data_total,\n    join_by(product_eng, region_id, year, quarter)\n  )\n\nLet us also impute missing values for the weather variables.\n\nweather_variables &lt;- \n  weather_quarter_regions_df |&gt; \n  select(where(is.numeric)) |&gt; \n  select(-year, -quarter) |&gt; \n  colnames()\n\nThe current number of missing values:\n\ndf |&gt; \n  summarise(\n    across(\n      .cols = !!weather_variables,\n      .fns = ~ sum(is.na(.x)),\n      .names = \"{.col}_nb_na\"\n    )\n  ) |&gt; \n  unlist()\n\n                temp_min_nb_na                 temp_max_nb_na \n                             0                              0 \n               temp_mean_nb_na               precip_sum_nb_na \n                             0                              0 \n       precip_piscop_sum_nb_na        perc_gamma_precip_nb_na \n                             0                              0 \nperc_gamma_precip_piscop_nb_na                 gdd_rice_nb_na \n                             0                              0 \n               gdd_maize_nb_na               gdd_potato_nb_na \n                             0                              0 \n             gdd_cassava_nb_na                 hdd_rice_nb_na \n                             0                              0 \n               hdd_maize_nb_na               hdd_potato_nb_na \n                             0                              0 \n             hdd_cassava_nb_na             temp_min_dev_nb_na \n                             0                              0 \n            temp_max_dev_nb_na            temp_mean_dev_nb_na \n                             0                              0 \n          precip_sum_dev_nb_na    precip_piscop_sum_dev_nb_na \n                             0                              0 \n            gdd_rice_dev_nb_na            gdd_maize_dev_nb_na \n                             0                              0 \n          gdd_potato_dev_nb_na          gdd_cassava_dev_nb_na \n                             0                              0 \n            hdd_rice_dev_nb_na            hdd_maize_dev_nb_na \n                             0                              0 \n          hdd_potato_dev_nb_na          hdd_cassava_dev_nb_na \n                             0                              0 \n\n\nIn case of missing values, we use linear interpolation to replace them:\n\ndf &lt;- \n  df |&gt; \n  mutate(\n    across(\n      .cols = !!weather_variables,\n      .fns = ~ imputeTS::na_interpolation(.x, maxgap = 3)\n    )\n  )\n\nThe number of remaining missing values:\n\ndf |&gt; \n  summarise(\n    across(\n      .cols = !!weather_variables,\n      .fns = ~ sum(is.na(.x)),\n      .names = \"{.col}_nb_na\"\n    )\n  ) |&gt; \n  unlist()\n\n                temp_min_nb_na                 temp_max_nb_na \n                             0                              0 \n               temp_mean_nb_na               precip_sum_nb_na \n                             0                              0 \n       precip_piscop_sum_nb_na        perc_gamma_precip_nb_na \n                             0                              0 \nperc_gamma_precip_piscop_nb_na                 gdd_rice_nb_na \n                             0                              0 \n               gdd_maize_nb_na               gdd_potato_nb_na \n                             0                              0 \n             gdd_cassava_nb_na                 hdd_rice_nb_na \n                             0                              0 \n               hdd_maize_nb_na               hdd_potato_nb_na \n                             0                              0 \n             hdd_cassava_nb_na             temp_min_dev_nb_na \n                             0                              0 \n            temp_max_dev_nb_na            temp_mean_dev_nb_na \n                             0                              0 \n          precip_sum_dev_nb_na    precip_piscop_sum_dev_nb_na \n                             0                              0 \n            gdd_rice_dev_nb_na            gdd_maize_dev_nb_na \n                             0                              0 \n          gdd_potato_dev_nb_na          gdd_cassava_dev_nb_na \n                             0                              0 \n            hdd_rice_dev_nb_na            hdd_maize_dev_nb_na \n                             0                              0 \n          hdd_potato_dev_nb_na          hdd_cassava_dev_nb_na \n                             0                              0 \n\n\nWe add labels to the new columns:\n\ndf &lt;- \n  df |&gt; \n  labelled::set_variable_labels(\n    y_new = \"Quarterly Agricultural Production (tons)\",\n    y_dev_pct = \"Quarterly Agricultural Production (pct. deviation from regional quarterly mean)\"\n  )\n\n\n\n11.3.2 Saving the file\nThe dataset that can be used to estimate the impact of weather shocks on agricultural production can be saved in the data output folder:\n\nsave(df, file = \"../data/output/df_lp_quarter.rda\")",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Merging: quarterly data</span>"
    ]
  },
  {
    "objectID": "robustness-data-merge-annual.html",
    "href": "robustness-data-merge-annual.html",
    "title": "12  Merging: annual data",
    "section": "",
    "text": "12.1 Load Intermediate Files\nThe (annual) weather data (Chapter 1) can be loaded:\nload(\"../data/output/weather/weather_annual_regions_df.rda\")\nThe agricultural data (Chapter 2):\nload(\"../data/output/minagri/dataset_agri_2001_2015.rda\")\nThe macroeconomic data (Chapter 3):\nload(\"../data/output/macro/df_macro.rda\")\nThe share of natural regions and the El Niño–Southern Oscillation (Chapter 4):\nload(\"../data/output/natural_region_dep.rda\")\nload(\"../data/output/weather/ONI_temp.rda\")",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Merging: annual data</span>"
    ]
  },
  {
    "objectID": "robustness-data-merge-annual.html#merge-the-files",
    "href": "robustness-data-merge-annual.html#merge-the-files",
    "title": "12  Merging: annual data",
    "section": "12.2 Merge the Files",
    "text": "12.2 Merge the Files\nWe add ENSO data to the weather dataset:\n\nWeather &lt;- weather_annual_regions_df |&gt; \n  # Add ENSO data\n  left_join(\n    ONI_temp |&gt; \n      mutate(year = as.numeric(Year)) |&gt; \n      group_by(year) |&gt; \n      summarise(ONI = mean(ONI)), \n    by = c(\n      \"year\" = \"year\"\n    )\n  ) |&gt;\n  group_by(IDDPTO) |&gt; \n  mutate( \n    temp_min_dev_ENSO   = temp_min - mean(temp_min),\n    temp_max_dev_ENSO   = temp_max - mean(temp_max),\n    temp_mean_dev_ENSO  = temp_mean - mean(temp_mean),\n    precip_sum_dev_ENSO = precip_sum - mean(precip_sum))|&gt; \n  ungroup() |&gt; \n  labelled::set_variable_labels(\n    temp_min_dev_ENSO   = \"Deviation of Min. Temperature from ENSO Normals\",\n    temp_max_dev_ENSO   = \"Deviation of Max. Temperature from ENSO Normals\",\n    temp_mean_dev_ENSO  = \"Deviation of Mean Temperature from ENSO Normals\",\n    precip_sum_dev_ENSO = \"Deviation of Total Rainfall from ENSO Normals\",\n  )\n\nFor international prices:\n\nlibrary(readxl)\nint_prices &lt;- read_excel(\n  path = \"../data/raw/Macro/IMF_DATA.xls\",\n  sheet = \"SELECTED\",\n  col_types = \"text\") |&gt; \n  mutate(date = lubridate::ymd(str_c(YEAR,  \"-\", MONTH, \"-01\"))) |&gt; \n  rename(\n    \"FPI\"           = \"FPI_PFOOD\", \n    \"FERTILIZER\"    = \"FERTILIZER_PFERT\",   \n    \"IndexOIL\"      = \"IndexOIL_POILAPSP\",\n    \"PriceOIL\"      = \"PriceOIL_POILAPSP\", \n    \"CORN\"          = \"CORN_PMAIZMT\", \n    \"ARROZ CÁSCARA\" = \"RICE_PRICENPQ\", \n    \"TRIGO\"         = \"WHEAT_PWHEAMT\"\n  ) |&gt; \n  mutate(CORN2 = CORN, FPI2 = FPI) |&gt; \n  select(-c(MONTH, YEAR, CPI_Peru_IMF, CPI_US_IMF, IndexOIL, PriceOIL)) |&gt; \n  pivot_longer(cols = !date, names_to = \"product\", values_to = \"price_int\") |&gt; \n  mutate(\n    product = case_when(\n      product == \"FPI\"  ~ \"PAPA\", \n      product == \"FPI2\" ~ \"YUCA\", \n      product == \"CORN\" ~ \"MAÍZ AMILÁCEO\", \n      product == \"CORN2\"~ \"MAÍZ AMARILLO DURO\", \n      TRUE ~ product\n    ), \n    product_eng = case_when(\n      product == \"PAPA\" ~ \"Potato\",\n      product == \"YUCA\" ~ \"Cassava\",\n      product == \"ARROZ CÁSCARA\" ~ \"Rice\",\n      product == \"MAÍZ AMARILLO DURO\" ~ \"Dent corn\"\n    ),\n    price_int = as.numeric(price_int)\n  ) |&gt; \n  arrange(product, date) |&gt; \n  mutate(year = year(date)) |&gt;  \n  group_by(product, year) |&gt; \n  mutate(price_int = mean(price_int)) |&gt;  \n  ungroup() |&gt; \n  select(-date) |&gt; \n  unique() |&gt; \n  group_by(product) |&gt; \n  arrange(year) |&gt; \n  mutate(int_price_inf = (price_int/lag(price_int) - 1) * 100) |&gt; \n  ungroup() |&gt; \n  arrange(product, year) |&gt; \n  filter(! year == 2000)\n\nLet us merge all these datasets in a single one:\n\ndata_total &lt;- \n  data_total |&gt; \n  # Add macroeconomic data\n  left_join(\n    df_macro |&gt; rename(gdp = y),\n    by = \"date\"\n  ) |&gt; \n  dplyr::select(\n    product_eng, region,region_id, product, year, Value_prod,\n    rer_hp, r_hp, pi, ind_prod) |&gt; \n  group_by(region, product, year) |&gt; \n  mutate(\n    Value_prod     = sum(Value_prod),   \n    rer_hp         = mean(rer_hp),\n    r_hp           = mean(r_hp),\n    pi             = mean(pi),\n    ind_prod       = mean(ind_prod)\n  ) |&gt;  \n  ungroup() |&gt; \n  unique() |&gt; \n  # Add commodity prices data\n  left_join(\n    int_prices,\n    by =  c(\n      \"product\", \"product_eng\", \"year\")\n  ) |&gt; \n  group_by(product, region) |&gt; \n  mutate(\n    int_price_inf = mFilter::hpfilter(\n      int_price_inf, freq = 1600, type = \"lambda\")[[\"cycle\"]]\n  ) |&gt; \n  # Add weather data and ENSO \n  left_join(\n    Weather |&gt; \n      dplyr::select(-IDDPTO),\n    by = c(\n      \"year\" = \"year\",\n      \"region\" = \"DEPARTAMEN\"\n    )\n  )\n\n\nsave(data_total, file = \"../data/output/dataset_2001_2015-annual.rda\")\n\nHere are the first rows of that tibble:\n\ndata_total\n\n# A tibble: 2,160 × 45\n# Groups:   product, region [144]\n   product_eng     region   region_id product   year Value_prod   rer_hp    r_hp\n   &lt;chr&gt;           &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 Amylaceous corn AMAZONAS         1 MAÍZ AM…  2001     10110  -0.0828   0.986 \n 2 Amylaceous corn AMAZONAS         1 MAÍZ AM…  2002     11493  -0.218   -1.41  \n 3 Amylaceous corn AMAZONAS         1 MAÍZ AM…  2003      5850   0.444    0.220 \n 4 Amylaceous corn AMAZONAS         1 MAÍZ AM…  2004      7315.  0.00578 -0.109 \n 5 Amylaceous corn AMAZONAS         1 MAÍZ AM…  2005      6126. -0.818   -0.207 \n 6 Amylaceous corn AMAZONAS         1 MAÍZ AM…  2006      4471.  0.00305  0.269 \n 7 Amylaceous corn AMAZONAS         1 MAÍZ AM…  2007      5240.  1.57     0.0279\n 8 Amylaceous corn AMAZONAS         1 MAÍZ AM…  2008      4909. -0.427    1.17  \n 9 Amylaceous corn AMAZONAS         1 MAÍZ AM…  2009      5660. -0.344   -0.456 \n10 Amylaceous corn AMAZONAS         1 MAÍZ AM…  2010      4946. -1.04    -1.26  \n# ℹ 2,150 more rows\n# ℹ 37 more variables: pi &lt;dbl&gt;, ind_prod &lt;dbl&gt;, price_int &lt;dbl&gt;,\n#   int_price_inf &lt;dbl&gt;, temp_min &lt;dbl&gt;, temp_max &lt;dbl&gt;, temp_mean &lt;dbl&gt;,\n#   precip_sum &lt;dbl&gt;, precip_piscop_sum &lt;dbl&gt;, perc_gamma_precip &lt;dbl&gt;,\n#   perc_gamma_precip_piscop &lt;dbl&gt;, gdd_rice &lt;dbl&gt;, gdd_maize &lt;dbl&gt;,\n#   gdd_potato &lt;dbl&gt;, gdd_cassava &lt;dbl&gt;, hdd_rice &lt;dbl&gt;, hdd_maize &lt;dbl&gt;,\n#   hdd_potato &lt;dbl&gt;, hdd_cassava &lt;dbl&gt;, temp_min_dev &lt;dbl&gt;, …",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Merging: annual data</span>"
    ]
  },
  {
    "objectID": "robustness-data-merge-annual.html#dataset-for-the-local-projections",
    "href": "robustness-data-merge-annual.html#dataset-for-the-local-projections",
    "title": "12  Merging: annual data",
    "section": "12.3 Dataset for the Local Projections",
    "text": "12.3 Dataset for the Local Projections\nNow, let us create the dataset specifically used to estimate the models.\nLet us make sure that the region data are encoded as a factor.\n\ndata_total &lt;- \n  data_total |&gt; \n  mutate(region_id = factor(region_id))\n\nThe crops we focus on:\n\ncrops &lt;- c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\")\n\nThe number of observation in each region, for each crop:\n\ndata_total |&gt; \n  group_by(product_eng, region_id) |&gt; \n  summarise(n = sum(Value_prod &lt;= 0)) |&gt; \n  arrange(desc(n))\n\n`summarise()` has grouped output by 'product_eng'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 144 × 3\n# Groups:   product_eng [6]\n   product_eng     region_id     n\n   &lt;chr&gt;           &lt;fct&gt;     &lt;int&gt;\n 1 Amylaceous corn 15           15\n 2 Amylaceous corn 21           15\n 3 Amylaceous corn 23           15\n 4 Amylaceous corn 24           15\n 5 Potato          15           15\n 6 Potato          21           15\n 7 Potato          23           15\n 8 Potato          24           15\n 9 Rice            3            15\n10 Rice            8            15\n# ℹ 134 more rows\n\n\n\n12.3.1 Definition of the Variable of Interest\n\n\n\n\n\n\nWarning\n\n\n\nWe compute percentage deviation of production from the regional average over the period of interest, but we will actually not use those values in the subsequent estimations. We will, however, use the demeaned values computed in this section.\n\n\nThis section outlines a two-step procedure for expressing agricultural production data at the annual regional level for a specific crop as a percentage deviation from the regional crop-specific average over the period of interest. The procedure involves handling missing values.\n\nStep 1: Handling Missing Values\nIn the first step, we address missing values by linear interpolation. This approach helps us estimate the missing values by considering the neighboring data points.\n\nStep 1.1: Imputing missing values with linear interpolation.\nThe missing values get replaced by linear interpolation. However, if there are more than two consecutive missing values, they are not replaced with interpolated values. Instead, the series for the specific crop in the given region is split based on the locations of the missing values. The split with the highest number of consecutive non-missing values is retained, while the other splits are discarded.\nStep 1.2: Dropping Series with Remaining Missing Values\nAfter imputing missing values using the moving median, we check if any missing values still remain in the dataset. If there are any remaining missing values for a particular series, we choose to exclude that series from further analysis. By doing so, we ensure that the subsequent detrending process is performed only on reliable and complete data.\n\nStep 2: Normalized Agricultural Production\nFor each month ( m ), region ( i ), and crop ( c ), we calculate the average production over the entire period (January 2001 to December 2015): \\[\\overline{y}_{c,i,m} = \\frac{1}{n_{T_c}} \\sum_{t=1}^{T_c} y_{c,i,m,t}^{\\text{raw}}\n  \\] Then, we express agricultural production relative to the average: \\[y_{c,i,m,t} = \\begin{cases}\n  \\frac{y_{c,i,m,t}^{\\text{raw}}}{\\overline{y}_{c,i,m}}, & \\overline{y}_{c,i,m} &gt; 0\\\\\n  0, & \\overline{y}_{c,i,m} = 0\n  \\end{cases}\\] Values of \\(y_{c,i,m,t}&gt;1\\) means that the production for crop \\(c\\) in region \\(i\\) during month \\(m\\) of year \\(t\\) is higher than the average monthly production for that crop and region over the period 2001 to 2015. For example, a value of 1.5 means that the production is 50% higher than average.\nStep 2 (alternative version): Deviation from regional monthly average, in percent (this step is useless in the new version of the analysis: it lead to discard too many observations)\nOnce we have addressed the missing values, we proceed to the second step, which consists in computing the deviation of production from the regional average. First, we compute the average production of each crop \\(c\\) in each region \\(i\\): \\[\\overline{y}_{c,i} = \\frac{1}{n_{T_c}} \\sum_{t=1}^{T_c} y_{c,i,t}^{raw}\\] Then, we compute the percentage deviation from this average at each date (i.e., year) \\(t\\): \\[y_{c,i,t} = \\frac{y_{c,i,t}^{raw} - \\overline{y}_{c,i}}{\\overline{y}_{c,i}}\\]\n\nLet us implement this process in R. First, we need to define two functions to handle the missing values:\n\nThe get_index_longest_non_na() function retrieves the indices of the longest consecutive sequence without missing values from a given input vector. It helps us identify the positions of elements in that sequence.\nThe keep_values_longest_non_na() function uses the obtained indices to create a logical vector. Each element of this vector indicates whether the corresponding element in the input vector belongs to the longest consecutive sequence of non-missing values. This allows us to filter the data and retain only the values from the longest consecutive sequence without missing values.\n\nThese two functions combined help us handle missing data in the weather series and ensure that we work with the most complete sequences for each region and crop.\nThe first function:\n\n#' Returns the index of the longest sequence of non NA values in a vector\n#'\n#' @param y vector of numerical values\n#' @export\nget_index_longest_non_na &lt;- function(y) {\n  split_indices &lt;- which(is.na(y))\n  nb_obs &lt;- length(y)\n\n  if (length(split_indices) == 0) {\n    res &lt;- seq_len(nb_obs)\n  } else {\n    idx_beg &lt;- c(1, split_indices)\n    if (idx_beg[length(idx_beg)] != nb_obs) {\n      idx_beg &lt;- c(idx_beg, nb_obs)\n    }\n    lengths &lt;- diff(idx_beg)\n    ind_max &lt;- which.max(lengths)\n    index_beginning &lt;- idx_beg[ind_max]\n    if(!index_beginning == 1 | is.na(y[index_beginning])) {\n      index_beginning &lt;- index_beginning + 1\n    }\n    index_end &lt;- idx_beg[ind_max] + lengths[ind_max]\n    if(is.na(y[index_end])) {\n      index_end &lt;- index_end - 1\n    }\n    res &lt;- seq(index_beginning, index_end)\n  }\n  res\n}\n\nThe second one:\n\n#' Returns a logical vector that identifies the longest sequence of non NA\n#' values within the input vector\n#' \n#' @param y numeric vector\nkeep_values_longest_non_na &lt;- function(y) {\n  ids_to_keep &lt;- get_index_longest_non_na(y)\n  ids &lt;- seq(1, length(y))\n  ids %in% ids_to_keep\n}\n\n\n\n\n\n\n\nNote\n\n\n\nThose two functions are defined in weatherperu/R/utils.R.\n\n\nWe define a function, pct_prod_production(), that takes the data frame of observations as input, as well as a crop name and a region ID. It returns a tibble with the following variables:\n\nproduct_eng: the English name of the crop\nregion_id: the ID of the region\nyear: year\ny_new_normalized (our variable of interest in Chapter 14): the production demeaned by the average for the crop of interest in the region of interest\ny_new: the production (in tons) where missing values were imputed, if possible\ny_dev_pct: the production expressed as the percentage deviation from the average (for the crop of interest, in the region of interest)\ny: same as y_dev_pct but without an estimated quadratic trend estimated by OLS\nt: trend.\n\n\n#' Computes the percentage deviation of production from annual regional average\n#'\n#' @param df data\n#' @param crop_name name of the crop\n#' @param region_id id of the region\n#'\n#' @returns data frame with the product, the region id, the date, the production\n#' with imputed missing values (`y_new`), the production demeaned (`y_new_normalized`), \n#' the percentage deviation from mean production (`y_dev_pct`), the percentage \n#' deviation from mean production minus an estimated quadratic trend (estimated\n#'  by OLS) (`y`), and, a trend (`t`)\n#' @export\n#' @importFrom dplyr filter arrange mutate select row_number group_by\n#' @importFrom tidyr nest unnest\n#' @importFrom purrr map\n#' @importFrom imputeTS na_interpolation\n#' @importFrom stats lm predict residuals\npct_prod_production &lt;- function(df,\n                                crop_name,\n                                region_id) {\n  # The current data\n  df_current &lt;-\n    df |&gt;\n    filter(\n      product_eng == !!crop_name,\n      region_id == !!region_id\n    ) |&gt;\n    arrange(year)\n\n  ## Dealing with missing values ----\n  # Look for negative production values\n  df_current &lt;-\n    df_current |&gt;\n    mutate(\n      y_new = ifelse(Value_prod &lt; 0, NA, Value_prod)\n    )\n\n  if (any(is.na(df_current$y_new))) {\n\n    # Replacing NAs by interpolation\n    # If there are more than two contiguous NAs, they are not replaced\n    df_current &lt;-\n      df_current |&gt;\n      mutate(\n        y_new = imputeTS::na_interpolation(y_new, maxgap = 3)\n      )\n\n    # Removing obs at the beginning/end if they are still missing\n    df_current &lt;-\n      df_current |&gt;\n      mutate(\n        row_to_keep = !(is.na(y_new) & row_number() %in% c(1:2, (n()-1):(n())))\n      ) |&gt;\n      filter(row_to_keep) |&gt;\n      select(-row_to_keep)\n\n    # Keeping the longest series of continuous non-NA values\n    df_current &lt;-\n      df_current |&gt;\n      mutate(\n        row_to_keep = keep_values_longest_non_na(y_new)\n      ) |&gt;\n      filter(row_to_keep) |&gt;\n      select(-row_to_keep)\n  }\n\n  rle_y_new &lt;- rle(df_current$y_new)\n  check_contiguous_zeros &lt;- rle_y_new$lengths[rle_y_new$values==0]\n\n\n  ## Percent deviation from regional average over the period\n  resul &lt;-\n    df_current |&gt;\n    ungroup() |&gt;\n    mutate(\n      y_new_normalized = case_when(\n        mean(y_new) == 0~ 0,\n        TRUE ~ y_new / mean(y_new)\n      ),\n      y_dev_pct = case_when(\n        mean(y_new) == 0 ~ 0,\n        TRUE ~ (y_new - mean(y_new)) / mean(y_new)\n      )\n    ) |&gt;\n    arrange(year) |&gt;\n    mutate(t = row_number()) |&gt;\n    nest(.by = c(product_eng, region_id)) |&gt;\n    mutate(\n      ols_fit   = map(data, ~ lm(y_new_normalized ~ -1 + t + I(t^2), data = .x)),\n      resid     = map(ols_fit, residuals),\n      fitted    = map(ols_fit, predict)\n    ) |&gt;\n    unnest(cols = c(data, resid, fitted)) |&gt;\n    mutate(\n      y = resid\n    ) |&gt;\n    select(\n      product_eng, region_id, year,\n      y_new, y_dev_pct, y_new_normalized, y, t\n    ) |&gt;\n    ungroup() |&gt;\n    arrange(year)\n\n  resul\n}\n\nFor example, for potatoes in region with id 1:\n\npct_prod_production(\n  df = data_total, \n  crop_name = \"Potato\", \n  region_id = 1\n)\n\n# A tibble: 15 × 8\n   product_eng region_id  year  y_new y_dev_pct y_new_normalized        y     t\n   &lt;chr&gt;       &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 Potato      1          2001 50077    -0.188             0.812  0.566       1\n 2 Potato      1          2002 57096    -0.0747            0.925  0.462       2\n 3 Potato      1          2003 57630    -0.0660            0.934  0.281       3\n 4 Potato      1          2004 50169.   -0.187             0.813 -0.00355     4\n 5 Potato      1          2005 47543.   -0.230             0.770 -0.182       5\n 6 Potato      1          2006 53307.   -0.136             0.864 -0.197       6\n 7 Potato      1          2007 74439.    0.206             1.21   0.0643      7\n 8 Potato      1          2008 85019.    0.378             1.38   0.182       8\n 9 Potato      1          2009 80966.    0.312             1.31   0.0899      9\n10 Potato      1          2010 66088.    0.0710            1.07  -0.150      10\n11 Potato      1          2011 51551.   -0.165             0.835 -0.358      11\n12 Potato      1          2012 59051.   -0.0430            0.957 -0.181      12\n13 Potato      1          2013 66423.    0.0765            1.08   0.0217     13\n14 Potato      1          2014 59995.   -0.0277            0.972  0.0278     14\n15 Potato      1          2015 66222.    0.0732            1.07   0.266      15\n\n\nWe can apply this function to all crops of interest, in each region. Let us define a table that contains all the possible values for the combination of crops and regions:\n\nproduct_and_regions &lt;- \n  data_total |&gt; \n  filter(product_eng %in% crops) |&gt; \n  dplyr::select(product_eng, region_id, region) |&gt; \n  unique()\n\nAdding missing grouping variables: `product`\n\n\nThen we apply the pct_prod_production() function to all these different cases, and store the results in a list named df_pct_pred_production:\n\ndf_pct_pred_production &lt;- vector(mode = \"list\", length = nrow(product_and_regions))\ncli_progress_bar(total = nrow(product_and_regions))\nfor(i in 1:nrow(product_and_regions)){\n  df_pct_pred_production[[i]] &lt;- pct_prod_production(\n    df = data_total, \n    crop_name = product_and_regions$product_eng[i], \n    region_id = product_and_regions$region_id[i]\n  )\n  cli_progress_update(set = i)\n}\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nThe elements of the list are all tibbles, with the same column names. We can merge them in a single tibble.\n\ndf_pct_pred_production &lt;- bind_rows(df_pct_pred_production)\n\nWe can have a look at the number regions with 0 values for the agricultural production.\n\ndf_pct_pred_production |&gt; \n  group_by(product_eng, region_id) |&gt; \n  summarise(nb_0 = sum(y_new == 0)) |&gt; \n  arrange(desc(nb_0))\n\n`summarise()` has grouped output by 'product_eng'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 96 × 3\n# Groups:   product_eng [4]\n   product_eng region_id  nb_0\n   &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt;\n 1 Cassava     8            15\n 2 Cassava     22           15\n 3 Potato      15           15\n 4 Potato      16           15\n 5 Potato      21           15\n 6 Potato      23           15\n 7 Potato      24           15\n 8 Rice        3            15\n 9 Rice        8            15\n10 Rice        10           15\n# ℹ 86 more rows\n\n\nNow, let us add the other columns to the tibble that contains the new data:\n\ndf &lt;- df_pct_pred_production |&gt; \n  left_join(\n    data_total,\n    join_by(product_eng, region_id, year)\n  )\n\nLet us also impute missing values for the weather variables.\n\nweather_variables &lt;- \n  weather_annual_regions_df |&gt; \n  select(where(is.numeric)) |&gt; \n  select(-year) |&gt; \n  colnames()\n\nThe current number of missing values:\n\ndf |&gt; \n  summarise(\n    across(\n      .cols = !!weather_variables,\n      .fns = ~ sum(is.na(.x)),\n      .names = \"{.col}_nb_na\"\n    )\n  ) |&gt; \n  unlist()\n\n                temp_min_nb_na                 temp_max_nb_na \n                             0                              0 \n               temp_mean_nb_na               precip_sum_nb_na \n                             0                              0 \n       precip_piscop_sum_nb_na        perc_gamma_precip_nb_na \n                             0                              0 \nperc_gamma_precip_piscop_nb_na                 gdd_rice_nb_na \n                             0                              0 \n               gdd_maize_nb_na               gdd_potato_nb_na \n                             0                              0 \n             gdd_cassava_nb_na                 hdd_rice_nb_na \n                             0                              0 \n               hdd_maize_nb_na               hdd_potato_nb_na \n                             0                              0 \n             hdd_cassava_nb_na             temp_min_dev_nb_na \n                             0                              0 \n            temp_max_dev_nb_na            temp_mean_dev_nb_na \n                             0                              0 \n          precip_sum_dev_nb_na    precip_piscop_sum_dev_nb_na \n                             0                              0 \n            gdd_rice_dev_nb_na            gdd_maize_dev_nb_na \n                             0                              0 \n          gdd_potato_dev_nb_na          gdd_cassava_dev_nb_na \n                             0                              0 \n            hdd_rice_dev_nb_na            hdd_maize_dev_nb_na \n                             0                              0 \n          hdd_potato_dev_nb_na          hdd_cassava_dev_nb_na \n                             0                              0 \n\n\nIn case of missing values, we use linear interpolation to replace them:\n\ndf &lt;- \n  df |&gt; \n  mutate(\n    across(\n      .cols = !!weather_variables,\n      .fns = ~ imputeTS::na_interpolation(.x, maxgap = 3)\n    )\n  )\n\nThe number of remaining missing values:\n\ndf |&gt; \n  summarise(\n    across(\n      .cols = !!weather_variables,\n      .fns = ~ sum(is.na(.x)),\n      .names = \"{.col}_nb_na\"\n    )\n  ) |&gt; \n  unlist()\n\n                temp_min_nb_na                 temp_max_nb_na \n                             0                              0 \n               temp_mean_nb_na               precip_sum_nb_na \n                             0                              0 \n       precip_piscop_sum_nb_na        perc_gamma_precip_nb_na \n                             0                              0 \nperc_gamma_precip_piscop_nb_na                 gdd_rice_nb_na \n                             0                              0 \n               gdd_maize_nb_na               gdd_potato_nb_na \n                             0                              0 \n             gdd_cassava_nb_na                 hdd_rice_nb_na \n                             0                              0 \n               hdd_maize_nb_na               hdd_potato_nb_na \n                             0                              0 \n             hdd_cassava_nb_na             temp_min_dev_nb_na \n                             0                              0 \n            temp_max_dev_nb_na            temp_mean_dev_nb_na \n                             0                              0 \n          precip_sum_dev_nb_na    precip_piscop_sum_dev_nb_na \n                             0                              0 \n            gdd_rice_dev_nb_na            gdd_maize_dev_nb_na \n                             0                              0 \n          gdd_potato_dev_nb_na          gdd_cassava_dev_nb_na \n                             0                              0 \n            hdd_rice_dev_nb_na            hdd_maize_dev_nb_na \n                             0                              0 \n          hdd_potato_dev_nb_na          hdd_cassava_dev_nb_na \n                             0                              0 \n\n\nWe add labels to the new columns:\n\ndf &lt;- \n  df |&gt; \n  labelled::set_variable_labels(\n    y_new = \"Quarterly Agricultural Production (tons)\",\n    y_dev_pct = \"Quarterly Agricultural Production (pct. deviation from regional quarterly mean)\"\n  )\n\n\n\n12.3.2 Saving the file\nThe dataset that can be used to estimate the impact of weather shocks on agricultural production can be saved in the data output folder:\n\nsave(df, file = \"../data/output/df_lp_annual.rda\")",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Merging: annual data</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections-quarter.html",
    "href": "robustness-local_projections-quarter.html",
    "title": "13  Quarterly Agricultural Production (LP)",
    "section": "",
    "text": "13.1 Linear Local Projections\nIn this section, we focus on estimating the Local Projections (Jordà 2005) to quantify the impact of weather on agricultural production. We utilize panel data, similar to the approach used in the study by Acevedo et al. (2020), and independently estimate models for each specific crop.\nFor a particular crop denoted as \\(c\\), the model can be expressed as follows: \\[\n\\begin{aligned}\n\\underbrace{y_{c,i,{\\color{wongGold}t+h}}}_{\\text{Production}} = &  {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}T}}} {\\color{wongPurple}{T_{i,{\\color{wongGold}t}}}} + {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}P}}} {\\color{wongPurple}P_{i,{\\color{wongGold}t}}}\\\\\n        &+\\delta_{c,i,h}\\underbrace{X_{t}}_{\\text{controls}} + \\underbrace{\\zeta_{c,i,h} \\text{Trend}_{t} + \\eta_{c,i,h} \\text{Trend}^2_{t}}_{\\text{regional quarterly trend}}+\\varepsilon_{c,i,t+h}\n\\end{aligned}\n\\tag{13.1}\\]\nHere, \\(i\\) represents the region, \\(t\\) represents the time, and \\(h\\) represents the horizon. The primary focus lies on estimating the coefficients associated with temperature and precipitation for different time horizons \\(\\color{wongGold}h=\\{0,1,...,T_{c}\\}\\)",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarterly Agricultural Production (LP)</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections-quarter.html#sec-lp-linear-quarter",
    "href": "robustness-local_projections-quarter.html#sec-lp-linear-quarter",
    "title": "13  Quarterly Agricultural Production (LP)",
    "section": "",
    "text": "13.1.1 Functions\nTo estimate the models, we develop a function, get_data_lp(), that generates the endogenous variable and the regressors for a specific crop, considering a given time horizon. This function is designed to return a list where each element represents the dataset that will be utilized for estimating the model corresponding to a specific time horizon.\nWhen we call the get_data_lp() function, we check for missing values in the weather data. If missing values are present for a specific region and crop, we keep only the longest consecutive sequence without missing values. To achieve this, we use the two functions defined previously: get_index_longest_non_na() and keep_values_longest_non_na().\n\n#' Get the data in a table for the local projections, for a specific crop\n#'\n#' @param df original dataset\n#' @param horizons number of horizons\n#' @param y_name name of the exogenous variable\n#' @param group_name name of the group variable\n#' @param crop_name name of the crop to focus on\n#' @param control_names vector of names of the control variables\n#' @param weather_names vector of names of the weather variables\n#' @param add_quarter_fe should columns with quarter dummy variables be added?\n#'   Default to `TRUE`\n#' @param share_geo vector of names of the variables that contain the share of\n#'   each type of geographical pattern. By default `NULL`: no share used\n#' @param transition_name name of the variable used to define the transition to\n#'   the two states. By default `NULL`\n#' @param transition_method if transition function, name of the method to use:\n#'   `logistic` or `normal` (default to `NULL`, i.e., no transition)\n#' @param state_names name of the two states in a vector of characters (only if\n#'   `transition_name` is not `NULL`). First period corresponds to mapped values\n#'   of `transition_name` close to 0, which is for large positive values of\n#'   `transition_name`\n#' @param gamma logistic growth rate (default to 3, only used if\n#'   `transition_name` is not `NULL`)\n#' @param other_var_to_keep vector of names of other variables to keep (default\n#'   to `NULL`: no additional vairable kept)\n#' @export\n#' @importFrom dplyr filter select mutate sym group_by across rowwise arrange\n#'   slice lead ends_with\n#' @importFrom fastDummies dummy_cols\n#' @importFrom stringr str_c str_detect\nget_data_lp &lt;- function(df,\n                        horizons,\n                        y_name,\n                        group_name,\n                        crop_name,\n                        control_names,\n                        weather_names,\n                        add_quarter_fe = TRUE,\n                        share_geo = NULL,\n                        transition_name = NULL,\n                        transition_method = NULL,\n                        state_names = c(\"planted\", \"harvested\"),\n                        gamma = 3,\n                        other_var_to_keep = NULL) {\n\n  if (!is.null(share_geo) & !is.null(transition_name)) {\n    stop(\"You can only use one between share_geo and transition_name\")\n  }\n\n  # Init empty object to return: list of length horizons\n  df_horizons &lt;- vector(mode = \"list\", length = horizons + 1)\n\n  # Keep only the variables needed\n  df_focus &lt;-\n    df |&gt;\n    filter(product_eng == !!crop_name) |&gt;\n    select(\n      !!y_name,\n      !!group_name,\n      year,\n      quarter,\n      product_eng,\n      !!!control_names,\n      !!!weather_names,\n      !!!share_geo,\n      !!transition_name,\n      !!other_var_to_keep\n    ) |&gt;\n    mutate(\n      !!group_name := factor(!!sym(group_name))\n    )\n\n  # Quarter dummy fixed-effects\n  if (add_quarter_fe) {\n    df_focus &lt;- df_focus |&gt;\n      # mutate(\n      #   quarter = as.character(lubridate::quarter(date))\n      # ) |&gt;\n      dummy_cols(select_columns = \"quarter\", remove_first_dummy = FALSE)\n  }\n\n\n  # For each region, only keep the longest sequence of non NA values found in\n  # the weather variables\n  df_focus &lt;-\n    df_focus |&gt;\n    group_by(region_id) |&gt;\n    mutate(\n      across(\n        .cols  = !!weather_names,\n        .fns   = keep_values_longest_non_na,\n        .names = \"{.col}_keep\"\n      )\n    ) |&gt;\n    rowwise() |&gt;\n    mutate(keep_cols = all(across(ends_with(\"_keep\")))) |&gt;\n    ungroup() |&gt;\n    filter(keep_cols) |&gt;\n    select(-keep_cols, -!!paste0(weather_names, \"_keep\"))\n\n  if (!is.null(share_geo)) {\n    # For each geographical type, multiply the weather variables by the share\n    # that the geo. type represents\n    for(share_geo_type in share_geo) {\n      df_focus &lt;-\n        df_focus |&gt;\n        mutate(\n          across(\n            .cols  = !!weather_names,\n            .fns   = ~ .x * !!sym(share_geo_type),\n            .names = str_c(\"{.col}_\", share_geo_type)\n          )\n        )\n    }\n  }\n\n  if (!is.null(transition_name)) {\n\n    state_1_name &lt;- state_names[1]\n    state_2_name &lt;- state_names[2]\n\n    if (transition_method == \"logistic\") {\n      df_focus &lt;-\n        df_focus |&gt;\n        mutate(\n          fz = logist(!!sym(transition_name), gamma = gamma)\n        )\n    } else if (transition_method == \"normal\") {\n      df_focus &lt;-\n        df_focus |&gt;\n        mutate(\n          fz = pnorm(-!!sym(transition_name))\n        )\n    } else {\n      stop(\"transition method must be either \\\"losistic\\\" or \\\"normal\\\"\")\n    }\n    df_focus &lt;- df_focus |&gt;\n      dummy_cols(group_name, remove_first_dummy = FALSE)\n\n    ind_dummies_group &lt;- str_detect(colnames(df_focus), str_c(group_name, \"_\"))\n    dummies_group_name &lt;- colnames(df_focus)[ind_dummies_group]\n\n    if (add_quarter_fe) {\n      ind_dummies_quarter &lt;- str_detect(colnames(df_focus), \"^quarter_\")\n      dummies_quarter_name &lt;- colnames(df_focus)[ind_dummies_quarter]\n      dummies_group_name &lt;- c(dummies_group_name, dummies_quarter_name)\n    }\n\n    df_focus &lt;-\n      df_focus |&gt;\n      mutate(\n        # First regime:\n        across(\n          .cols  = c(!!!control_names, !!!weather_names, !!!dummies_group_name),\n          .fns   = list(\n            state_1_name = ~ (1 - fz) * .x,\n            state_2_name = ~ fz * .x\n          ),\n          .names = \"{fn}_{col}\"\n        )\n      ) |&gt;\n      rename_with(\n        .fn = ~str_replace(string = .x, pattern = \"state_1_name\", replacement = state_1_name),\n        .cols = starts_with(\"state_1_name\")\n      ) |&gt;\n      rename_with(\n        .fn = ~str_replace(string = .x, pattern = \"state_2_name\", replacement = state_2_name),\n        .cols = starts_with(\"state_2_name\")\n      )\n  } else {\n    df_focus &lt;-\n      df_focus |&gt;\n      dummy_cols(group_name, remove_first_dummy = FALSE)\n  }\n\n\n  # Prepare the values for y at t+h\n  for (h in 0:horizons) {\n    df_horizons[[h+1]] &lt;-\n      df_focus |&gt;\n      group_by(!!sym(group_name)) |&gt;\n      arrange(year, quarter) |&gt;\n      mutate(time = row_number()) |&gt;\n      mutate(y_lead = dplyr::lead(!!sym(y_name), n = h)) |&gt;\n      slice(1:(n()-h))\n  }\n  names(df_horizons) &lt;- 0:horizons\n  df_horizons\n}\n\nFollowing the data preparation step, we proceed to define a function, estimate_linear_lp() that performs the estimation of models for all time horizons. This function utilizes the datasets obtained through the get_data_lp() function.\n\n#' Estimate Local Projections\n#'\n#' @param df original dataset\n#' @param horizons number of horizons\n#' @param y_name name of the exogenous variable\n#' @param group_name name of the group variable\n#' @param crop_name name of the crop to focus on\n#' @param control_names vector of names of the control variables\n#' @param weather_names vector of names of the weather variables\n#' @param detrend if `TRUE` a group-wise quadratic temporal effect is estimated\n#'  (group:time + group:I(time^2))\n#' @param add_quarter_fe should columns with quarter dummy variables be added?\n#'   Default to `TRUE`\n#' @param add_intercept should an intercept we added to the regressions?\n#'   (default to `FALSE`)\n#' @param share_geo vector of names of the variables that contain the share of\n#'   each type of geographical pattern. By default `NULL`: no share used\n#' @param std type of standard error (`\"NW\"` for Newey-West, `\"Cluster\"`,\n#'   `\"Standard\"` otherwise)\n#' @param transition_name name of the variable used to define the transition to\n#'   the two states. By default `NULL`\n#' @param transition_method if transition function, name of the method to use:\n#'   `logistic` or `normal` (default to `NULL`, i.e., no transition)\n#' @param state_names name of the two states in a vector of characters (only if\n#'   `transition_name` is not `NULL`). First period corresponds to mapped values\n#'   of `transition_name` close to 0, which is for large positive values of\n#'   `transition_name`\n#' @param gamma logistic growth rate (default to 3, only used if\n#'   `transition_name` is not `NULL`)\n#' @param other_var_to_keep vector of names of other variables to keep in the\n#'   returned dataset (default to `NULL`: no additional vairable kept)\n#' @export\n#' @importFrom dplyr mutate sym ungroup summarise across left_join\n#' @importFrom stringr str_c str_detect\n#' @importFrom purrr map map_dbl list_rbind\n#' @importFrom tibble enframe\n#' @importFrom tidyr pivot_longer\n#' @importFrom sandwich NeweyWest\n#' @importFrom stats sd model.matrix nobs residuals lm coef\nestimate_linear_lp &lt;- function(df,\n                              horizons,\n                              y_name,\n                              group_name,\n                              crop_name,\n                              control_names,\n                              weather_names,\n                              detrend = FALSE,\n                              add_quarter_fe = TRUE,\n                              add_intercept = FALSE,\n                              share_geo = NULL,\n                              transition_name = NULL,\n                              transition_method = NULL,\n                              state_names = c(\"planted\", \"harvested\"),\n                              gamma = 3,\n                              std = c(\"nw\", \"cluster\", \"standard\"),\n                              other_var_to_keep = NULL) {\n\n  # Format the dataset\n  data_lp &lt;-\n    get_data_lp(\n      df = df,\n      horizons = horizons,\n      y_name = y_name,\n      group_name = group_name,\n      crop_name = crop_name,\n      control_names = control_names,\n      weather_names = weather_names,\n      share_geo = share_geo,\n      transition_name = transition_name,\n      transition_method = transition_method,\n      state_names = state_names,\n      gamma = gamma,\n      other_var_to_keep = other_var_to_keep\n    )\n\n  # Recode levels for the groups\n  for(h in 0:horizons){\n    data_lp[[h + 1]] &lt;-\n      data_lp[[h + 1]] |&gt;\n      mutate(\n        !!group_name := as.factor(as.character(!!sym(group_name)))\n      )\n  }\n\n  control_names_full &lt;- control_names\n  weather_names_full &lt;- weather_names\n  ind_names_groups &lt;- str_detect(\n    colnames(data_lp[[1]]), str_c(\"^\", group_name, \"_\")\n  )\n  group_names_full &lt;- colnames(data_lp[[1]])[ind_names_groups]\n\n  if (!is.null(share_geo)) {\n    # Name of the weather variables\n    weather_names_full &lt;- paste(\n      rep(weather_names, each = length(share_geo)),\n      share_geo,\n      sep = \"_\"\n    )\n  }\n\n  if (!is.null(transition_name)) {\n\n    state_1_name &lt;- str_c(state_names[1], \"_\")\n    state_2_name &lt;- str_c(state_names[2], \"_\")\n\n    # Name of the variables\n    weather_names_full &lt;- str_c(\n      rep(\n        c(state_1_name, state_2_name),\n        each = length(weather_names)\n      ),\n      rep(weather_names, 2)\n    )\n    control_names_full &lt;- str_c(\n      rep(\n        c(state_1_name, state_2_name),\n        each = length(control_names)\n      ),\n      rep(control_names, 2)\n    )\n    ind_names_groups &lt;- str_detect(\n      colnames(data_lp[[1]]),\n      str_c(\"(^\", state_1_name, \"|\", state_2_name, \")\", group_name, \"_\")\n    )\n    group_names_full &lt;- colnames(data_lp[[1]])[ind_names_groups]\n  }\n\n  # Observed standard deviations in the data\n  sd_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = sd\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"std_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n  # Observed median value in the data\n  median_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = ~quantile(.x, probs = .5)\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"median_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n  # Observed quantile of order 0.05 value in the data\n  q05_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = ~quantile(.x, probs = .05)\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"q05_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n  # Observed quantile of order 0.95 value in the data\n  q95_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = ~quantile(.x, probs = .95)\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"q95_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n\n  if (detrend == TRUE) {\n    # Formula for the regressions\n    formula_lp &lt;- paste0(\n      \"y_lead\",\n      \" ~ -1+\",\n      # \" ~ 1+\", # intercept\n      paste(weather_names_full, collapse = \" + \"),\n      \" + \",\n      paste(control_names_full, collapse = \" + \"),\n      \" + \",\n      paste0(\n        c(\n          paste0(group_names_full, \":time:factor(quarter)\"),\n          paste0(group_names_full, \":I(time^2):factor(quarter)\")\n        ),\n        collapse = \" + \"\n      )\n    )\n  } else {\n    # Formula for the regressions\n    formula_lp &lt;- paste0(\n      \"y_lead\",\n      \" ~ -1+\",\n      # \" ~ 1+\", # intercept\n      paste(weather_names_full, collapse = \" + \"),\n      \" + \",\n      paste(control_names_full, collapse = \" + \"),\n      \" + \",\n      ifelse(\n        add_intercept,\n        # removing last group\n        yes = paste(group_names_full[-length(group_names_full)], collapse = \" + \"),\n        # keeping last group\n        no = paste(group_names_full, collapse = \" + \")\n      )\n    )\n  }\n\n\n\n  if (add_quarter_fe) {\n    formula_lp &lt;- paste0(\n      formula_lp, \" + \", paste(paste0(\"quarter_\", 1:11), collapse = \" + \")\n    )\n  }\n\n  empty_res &lt;- vector(mode = \"list\", length = horizons + 1)\n  reg_lp &lt;- empty_res\n  sig_ols &lt;- empty_res\n  log_likelihood &lt;- empty_res\n  mse &lt;- empty_res\n  coefs &lt;- empty_res\n  cl_std &lt;- empty_res\n\n\n  for (h in 0:horizons) {\n    # Global assignment... otherwise, errors with coeftest()\n    current_data_h &lt;&lt;- data_lp[[h+1]]\n    # Regression\n    reg_h &lt;- lm(formula = formula_lp, data = current_data_h)\n    # Standard error of the residuals\n    sig_ols_h &lt;- sd(reg_h$residuals)\n    # Log likelihood\n    u_h &lt;- reg_h$residuals\n    log_likelihood_h &lt;-\n      sum(log(1 / sqrt(2 * pi * sig_ols_h^2) * exp(-u_h^2 / (2 * sig_ols_h^2))))\n    mse_h &lt;- mean(u_h^2)\n    coefs_h &lt;- enframe(coef(reg_h)) |&gt; mutate(horizon = h)\n    if (std == \"Cluster\") {\n      cl_std_h &lt;- coeftest(\n        reg_h,\n        vcov = vcovCL,\n        cluster = formula(str_c(\"~\", group_name)))[, \"Std. Error\"] |&gt;\n        enframe() |&gt;\n        mutate(horizon = h)\n    } else {\n      cl_std_h &lt;- enframe(sqrt(diag(vcov(reg_h))), value = \"std\") |&gt;\n        mutate(horizon = h)\n    }\n\n    # Store results in lists\n    reg_lp[[h+1]] &lt;- reg_h\n    sig_ols[[h+1]] &lt;- sig_ols_h\n    log_likelihood[[h+1]] &lt;- log_likelihood_h\n    mse[[h+1]] &lt;- mse_h\n    coefs[[h+1]] &lt;- coefs_h\n    cl_std[[h+1]] &lt;- cl_std_h\n  }\n  se_df &lt;- list_rbind(cl_std) |&gt; rename(std = value)\n\n  coefs &lt;-\n    coefs |&gt; list_rbind() |&gt;\n    left_join(se_df, by = c(\"horizon\", \"name\")) |&gt;\n    mutate(\n      crop = crop_name,\n      horizon = as.numeric(horizon)\n    ) |&gt;\n    left_join(sd_weather_shock, by = c(\"horizon\", \"name\")) |&gt;\n    left_join(median_weather_shock, by = c(\"horizon\", \"name\")) |&gt;\n    left_join(q05_weather_shock, by = c(\"horizon\", \"name\")) |&gt;\n    left_join(q95_weather_shock, by = c(\"horizon\", \"name\"))\n\n  list(\n    # reg_lp = reg_lp,\n    coefs = coefs,\n    horizons = horizons,\n    log_likelihood = log_likelihood |&gt; list_c(),\n    mse = mse |&gt; list_c(),\n    crop_name = crop_name,\n    data_lp = data_lp\n  )\n}\n\n\n\n\n\n\n\nNote\n\n\n\nThe get_data_lp() function is defined in the R script saved here: /weatherperu/R/format_data-quarter.R. The estimation function, estimate_linear_lp(), is defined in the R script saved here: /weatherperu/R/estimations-quarter.R.\n\nsource(\"../weatherperu/R/estimations-quarter.R\")\n\n\n\n\n\n13.1.2 Estimation\nTo loop over the different crops, we can utilize the map() function. This function enables us to apply the estimate_linear_lp() function to each crop iteratively, facilitating the estimation process.\n\ncrops &lt;- df$product_eng |&gt; unique()\nweather_variables &lt;- c(\"temp_max_dev\", \"precip_piscop_sum_dev\")\ncontrol_variables &lt;- c(\n  \"rer_hp\", \"r_hp\", \"pi\", \"ind_prod\", \"ONI\", \"int_price_inf\"\n)\n\nThe estimations are run as follows (in this notebook, we load previously estimated results to save compilation time)\n\nresul_lp_quarter &lt;- map(\n  crops, ~ estimate_linear_lp(\n    df = df,\n    horizons = 5,\n    y_name = \"y_new_normalized\",\n    group_name = \"region_id\",\n    detrend = TRUE,\n    add_quarter_fe = FALSE,\n    add_intercept = FALSE,\n    crop_name = .x,\n    control_names = control_variables,\n    weather_names = weather_variables,\n    std = \"Cluster\",\n    other_var_to_keep = \"y_new\"\n  )\n)\nsave(resul_lp_quarter, file = \"../R/output/resul_lp_quarter.rda\")\n\n\nload(\"../R/output/resul_lp_quarter.rda\")\n\n\n\n13.1.3 Results\nWe can visualize the Impulse Response Functions (IRFs) by plotting the estimated coefficients associated with the weather variables. These coefficients represent the impact of weather on agricultural production and can provide valuable insights into the dynamics of the system. By plotting the IRFs, we can gain a better understanding of the relationship between weather variables and the response of agricultural production over time.\nThe data for the graphs:\n\ndf_irfs_lp_quarter &lt;- map(resul_lp_quarter, \"coefs\") |&gt; \n  list_rbind() |&gt; \n  filter(name %in% weather_variables) |&gt; \n  mutate(\n    shock_1_sd = value * std_shock,\n    lower_95 = (value - qnorm(0.975) * std) * std_shock,\n    upper_95 = (value + qnorm(0.975) * std) * std_shock,\n    lower_68 = (value - qnorm(0.84)  * std) * std_shock,\n    upper_68 = (value + qnorm(0.84)  * std) * std_shock\n  ) |&gt; \n  mutate(\n    crop = factor(\n      crop, \n      levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"))\n  ) |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = c(\n        \"temp_max_dev\",\n        \"precip_piscop_sum_dev\"\n      ),\n      labels = c(\n        \"Temp. anomalies\", \n        \"Precip. anomalies\"\n      )\n    )\n  )\n\nFor the confidence intervals:\n\ndf_irfs_lp_ci_quarter &lt;- \n  df_irfs_lp_quarter |&gt; \n  select(horizon, crop, name, matches(\"^(lower)|^(upper)\", perl = TRUE)) |&gt; \n  pivot_longer(\n    cols = matches(\"^(lower)|^(upper)\", perl = TRUE),\n    names_pattern = \"(.*)_(95|68)$\",\n    names_to = c(\".value\", \"level\")\n  ) |&gt; \n  mutate(level = str_c(level, \"%\"))\n\n\n# Duration of the growing season\ngs_duration_df &lt;- tribble(\n  ~crop, ~tc,\n  \"Rice\", 4,\n  \"Maize\", 5,\n  \"Potato\", 6,\n  \"Cassava\", 9\n) |&gt; \n  mutate(tc = tc / 3)\n\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci_quarter,\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs_lp_quarter,\n    mapping = aes(x = horizon, y = shock_1_sd),\n    colour = \"#0072B2\") +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  geom_vline(\n    data = gs_duration_df, \n    mapping = aes(xintercept = tc),\n    colour = \"#D55E00\", linetype = \"dashed\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", axes = \"all\",\n    independent = \"y\", switch = \"y\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"gray10\", \"95%\" = \"gray60\")\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\nFigure 13.1: Agricultural production response to a weather shock\n\n\n\n\n\n\n\n\n\nAcevedo, Sebastian, Mico Mrkaic, Natalija Novta, Evgenia Pugacheva, and Petia Topalova. 2020. “The Effects of Weather Shocks on Economic Activity: What Are the Channels of Impact?” Journal of Macroeconomics 65: 103207.\n\n\nJordà, Òscar. 2005. “Estimation and Inference of Impulse Responses by Local Projections.” American Economic Review 95 (1): 161–82. https://doi.org/10.1257/0002828053828518.",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarterly Agricultural Production (LP)</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections-annual.html",
    "href": "robustness-local_projections-annual.html",
    "title": "14  Annual Agricultural Production (LP)",
    "section": "",
    "text": "14.1 Linear Local Projections\nIn this section, we focus on estimating the Local Projections (Jordà 2005) to quantify the impact of weather on annual agricultural production. We use panel data, similar to the approach used in the study by Acevedo et al. (2020), and independently estimate models for each specific crop.\nFor a particular crop denoted as \\(c\\), the model can be expressed as follows: \\[\n\\begin{aligned}\n\\underbrace{y_{c,i,{\\color{wongGold}t+h}}}_{\\text{Production}} = &  {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}T}}} {\\color{wongPurple}{T_{i,{\\color{wongGold}t}}}} + {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}P}}} {\\color{wongPurple}P_{i,{\\color{wongGold}t}}}\\\\\n        &+\\delta_{c,i,h}\\underbrace{X_{t}}_{\\text{controls}} + \\underbrace{\\zeta_{c,i,h} \\text{Trend}_{t} \\times \\text{Month}_t + \\eta_{c,i,h} \\text{Trend}^2_{t} \\times \\text{Month}_t}_{\\text{regional trend}}+\\varepsilon_{c,i,t+h}\n\\end{aligned}\n\\tag{14.1}\\]\nHere, \\(i\\) represents the region, \\(t\\) represents the time, and \\(h\\) represents the horizon. The primary focus lies on estimating the coefficients associated with temperature and precipitation for different time horizons \\(\\color{wongGold}h=\\{0,1,...,T_{c}\\}\\)",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Annual Agricultural Production (LP)</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections-annual.html#sec-lp-linear-annual",
    "href": "robustness-local_projections-annual.html#sec-lp-linear-annual",
    "title": "14  Annual Agricultural Production (LP)",
    "section": "",
    "text": "14.1.1 Functions\nTo estimate the models, we develop a function, get_data_lp(), that generates the endogenous variable and the regressors for a specific crop, considering a given time horizon. This function is designed to return a list where each element represents the dataset that will be used for estimating the model corresponding to a specific time horizon.\nWhen we call the get_data_lp() function, we check for missing values in the weather data. If missing values are present for a specific region and crop, we keep only the longest consecutive sequence without missing values. To achieve this, we use the two functions defined previously: get_index_longest_non_na() and keep_values_longest_non_na().\n\n#' Get the data in a table for the local projections, for a specific crop\n#'\n#' @param df original dataset\n#' @param horizons number of horizons\n#' @param y_name name of the exogenous variable\n#' @param group_name name of the group variable\n#' @param crop_name name of the crop to focus on\n#' @param control_names vector of names of the control variables\n#' @param weather_names vector of names of the weather variables\n#' @param add_year_fe should columns with annual dummy variables be added?\n#'   Default to `TRUE`\n#' @param share_geo vector of names of the variables that contain the share of\n#'   each type of geographical pattern. By default `NULL`: no share used\n#' @param transition_name name of the variable used to define the transition to\n#'   the two states. By default `NULL`\n#' @param transition_method if transition function, name of the method to use:\n#'   `logistic` or `normal` (default to `NULL`, i.e., no transition)\n#' @param state_names name of the two states in a vector of characters (only if\n#'   `transition_name` is not `NULL`). First period corresponds to mapped values\n#'   of `transition_name` close to 0, which is for large positive values of\n#'   `transition_name`\n#' @param gamma logistic growth rate (default to 3, only used if\n#'   `transition_name` is not `NULL`)\n#' @param other_var_to_keep vector of names of other variables to keep (default\n#'   to `NULL`: no additional vairable kept)\n#' @export\n#' @importFrom dplyr filter select mutate sym group_by across rowwise arrange\n#'   slice lead ends_with\n#' @importFrom fastDummies dummy_cols\n#' @importFrom stringr str_c str_detect\nget_data_lp &lt;- function(df,\n                        horizons,\n                        y_name,\n                        group_name,\n                        crop_name,\n                        control_names,\n                        weather_names,\n                        add_year_fe = TRUE,\n                        share_geo = NULL,\n                        transition_name = NULL,\n                        transition_method = NULL,\n                        state_names = c(\"planted\", \"harvested\"),\n                        gamma = 3,\n                        other_var_to_keep = NULL) {\n\n  if (!is.null(share_geo) & !is.null(transition_name)) {\n    stop(\"You can only use one between share_geo and transition_name\")\n  }\n\n  # Init empty object to return: list of length horizons\n  df_horizons &lt;- vector(mode = \"list\", length = horizons + 1)\n\n  # Keep only the variables needed\n  df_focus &lt;-\n    df |&gt;\n    filter(product_eng == !!crop_name) |&gt;\n    select(\n      !!y_name,\n      !!group_name,\n      year,\n      product_eng,\n      !!!control_names,\n      !!!weather_names,\n      !!!share_geo,\n      !!transition_name,\n      !!other_var_to_keep\n    ) |&gt;\n    mutate(\n      !!group_name := factor(!!sym(group_name))\n    )\n\n  # Year dummy fixed-effects\n  if (add_year_fe) {\n    df_focus &lt;- df_focus |&gt;\n      dummy_cols(select_columns = \"year\", remove_first_dummy = FALSE)\n  }\n\n\n  # For each region, only keep the longest sequence of non NA values found in\n  # the weather variables\n  df_focus &lt;-\n    df_focus |&gt;\n    group_by(region_id) |&gt;\n    mutate(\n      across(\n        .cols  = !!weather_names,\n        .fns   = keep_values_longest_non_na,\n        .names = \"{.col}_keep\"\n      )\n    ) |&gt;\n    rowwise() |&gt;\n    mutate(keep_cols = all(across(ends_with(\"_keep\")))) |&gt;\n    ungroup() |&gt;\n    filter(keep_cols) |&gt;\n    select(-keep_cols, -!!paste0(weather_names, \"_keep\"))\n\n  if (!is.null(share_geo)) {\n    # For each geographical type, multiply the weather variables by the share\n    # that the geo. type represents\n    for(share_geo_type in share_geo) {\n      df_focus &lt;-\n        df_focus |&gt;\n        mutate(\n          across(\n            .cols  = !!weather_names,\n            .fns   = ~ .x * !!sym(share_geo_type),\n            .names = str_c(\"{.col}_\", share_geo_type)\n          )\n        )\n    }\n  }\n\n  if (!is.null(transition_name)) {\n\n    state_1_name &lt;- state_names[1]\n    state_2_name &lt;- state_names[2]\n\n    if (transition_method == \"logistic\") {\n      df_focus &lt;-\n        df_focus |&gt;\n        mutate(\n          fz = logist(!!sym(transition_name), gamma = gamma)\n        )\n    } else if (transition_method == \"normal\") {\n      df_focus &lt;-\n        df_focus |&gt;\n        mutate(\n          fz = pnorm(-!!sym(transition_name))\n        )\n    } else {\n      stop(\"transition method must be either \\\"losistic\\\" or \\\"normal\\\"\")\n    }\n    df_focus &lt;- df_focus |&gt;\n      dummy_cols(group_name, remove_first_dummy = FALSE)\n\n    ind_dummies_group &lt;- str_detect(colnames(df_focus), str_c(group_name, \"_\"))\n    dummies_group_name &lt;- colnames(df_focus)[ind_dummies_group]\n\n    if (add_year_fe) {\n      ind_dummies_year &lt;- str_detect(colnames(df_focus), \"^year_\")\n      dummies_year_name &lt;- colnames(df_focus)[ind_dummies_year]\n      dummies_group_name &lt;- c(dummies_group_name, dummies_year_name)\n    }\n\n    df_focus &lt;-\n      df_focus |&gt;\n      mutate(\n        # First regime:\n        across(\n          .cols  = c(!!!control_names, !!!weather_names, !!!dummies_group_name),\n          .fns   = list(\n            state_1_name = ~ (1 - fz) * .x,\n            state_2_name = ~ fz * .x\n          ),\n          .names = \"{fn}_{col}\"\n        )\n      ) |&gt;\n      rename_with(\n        .fn = ~str_replace(string = .x, pattern = \"state_1_name\", replacement = state_1_name),\n        .cols = starts_with(\"state_1_name\")\n      ) |&gt;\n      rename_with(\n        .fn = ~str_replace(string = .x, pattern = \"state_2_name\", replacement = state_2_name),\n        .cols = starts_with(\"state_2_name\")\n      )\n  } else {\n    df_focus &lt;-\n      df_focus |&gt;\n      dummy_cols(group_name, remove_first_dummy = FALSE)\n  }\n\n\n  # Prepare the values for y at t+h\n  for (h in 0:horizons) {\n    df_horizons[[h+1]] &lt;-\n      df_focus |&gt;\n      group_by(!!sym(group_name)) |&gt;\n      arrange(year) |&gt;\n      mutate(time = row_number()) |&gt;\n      mutate(y_lead = dplyr::lead(!!sym(y_name), n = h)) |&gt;\n      slice(1:(n()-h))\n  }\n  names(df_horizons) &lt;- 0:horizons\n  df_horizons\n}\n\nFollowing the data preparation step, we proceed to define a function, estimate_linear_lp() that performs the estimation of models for all time horizons. This function used the datasets obtained through the get_data_lp() function.\n\n#' Estimate Local Projections\n#'\n#' @param df original dataset\n#' @param horizons number of horizons\n#' @param y_name name of the exogenous variable\n#' @param group_name name of the group variable\n#' @param crop_name name of the crop to focus on\n#' @param control_names vector of names of the control variables\n#' @param weather_names vector of names of the weather variables\n#' @param detrend if `TRUE` a group-wise quadratic temporal effect is estimated\n#'  (group:time + group:I(time^2))\n#' @param add_year_fe should columns with year dummy variables be added?\n#'   Default to `TRUE`\n#' @param add_intercept should an intercept we added to the regressions?\n#'   (default to `FALSE`)\n#' @param share_geo vector of names of the variables that contain the share of\n#'   each type of geographical pattern. By default `NULL`: no share used\n#' @param std type of standard error (`\"NW\"` for Newey-West, `\"Cluster\"`,\n#'   `\"Standard\"` otherwise)\n#' @param transition_name name of the variable used to define the transition to\n#'   the two states. By default `NULL`\n#' @param transition_method if transition function, name of the method to use:\n#'   `logistic` or `normal` (default to `NULL`, i.e., no transition)\n#' @param state_names name of the two states in a vector of characters (only if\n#'   `transition_name` is not `NULL`). First period corresponds to mapped values\n#'   of `transition_name` close to 0, which is for large positive values of\n#'   `transition_name`\n#' @param gamma logistic growth rate (default to 3, only used if\n#'   `transition_name` is not `NULL`)\n#' @param other_var_to_keep vector of names of other variables to keep in the\n#'   returned dataset (default to `NULL`: no additional vairable kept)\n#' @export\n#' @importFrom dplyr mutate sym ungroup summarise across left_join\n#' @importFrom stringr str_c str_detect\n#' @importFrom purrr map map_dbl list_rbind\n#' @importFrom tibble enframe\n#' @importFrom tidyr pivot_longer\n#' @importFrom sandwich NeweyWest\n#' @importFrom stats sd model.matrix nobs residuals lm coef\nestimate_linear_lp &lt;- function(df,\n                              horizons,\n                              y_name,\n                              group_name,\n                              crop_name,\n                              control_names,\n                              weather_names,\n                              detrend = FALSE,\n                              add_year_fe = TRUE,\n                              add_intercept = FALSE,\n                              share_geo = NULL,\n                              transition_name = NULL,\n                              transition_method = NULL,\n                              state_names = c(\"planted\", \"harvested\"),\n                              gamma = 3,\n                              std = c(\"nw\", \"cluster\", \"standard\"),\n                              other_var_to_keep = NULL) {\n\n  # Format the dataset\n  data_lp &lt;-\n    get_data_lp(\n      df = df,\n      horizons = horizons,\n      y_name = y_name,\n      group_name = group_name,\n      crop_name = crop_name,\n      control_names = control_names,\n      weather_names = weather_names,\n      share_geo = share_geo,\n      transition_name = transition_name,\n      transition_method = transition_method,\n      state_names = state_names,\n      gamma = gamma,\n      other_var_to_keep = other_var_to_keep\n    )\n\n  # Recode levels for the groups\n  for(h in 0:horizons){\n    data_lp[[h + 1]] &lt;-\n      data_lp[[h + 1]] |&gt;\n      mutate(\n        !!group_name := as.factor(as.character(!!sym(group_name)))\n      )\n  }\n\n  control_names_full &lt;- control_names\n  weather_names_full &lt;- weather_names\n  ind_names_groups &lt;- str_detect(\n    colnames(data_lp[[1]]), str_c(\"^\", group_name, \"_\")\n  )\n  group_names_full &lt;- colnames(data_lp[[1]])[ind_names_groups]\n\n  if (!is.null(share_geo)) {\n    # Name of the weather variables\n    weather_names_full &lt;- paste(\n      rep(weather_names, each = length(share_geo)),\n      share_geo,\n      sep = \"_\"\n    )\n  }\n\n  if (!is.null(transition_name)) {\n\n    state_1_name &lt;- str_c(state_names[1], \"_\")\n    state_2_name &lt;- str_c(state_names[2], \"_\")\n\n    # Name of the variables\n    weather_names_full &lt;- str_c(\n      rep(\n        c(state_1_name, state_2_name),\n        each = length(weather_names)\n      ),\n      rep(weather_names, 2)\n    )\n    control_names_full &lt;- str_c(\n      rep(\n        c(state_1_name, state_2_name),\n        each = length(control_names)\n      ),\n      rep(control_names, 2)\n    )\n    ind_names_groups &lt;- str_detect(\n      colnames(data_lp[[1]]),\n      str_c(\"(^\", state_1_name, \"|\", state_2_name, \")\", group_name, \"_\")\n    )\n    group_names_full &lt;- colnames(data_lp[[1]])[ind_names_groups]\n  }\n\n  # Observed standard deviations in the data\n  sd_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = sd\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"std_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n  # Observed median value in the data\n  median_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = ~quantile(.x, probs = .5)\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"median_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n  # Observed quantile of order 0.05 value in the data\n  q05_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = ~quantile(.x, probs = .05)\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"q05_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n  # Observed quantile of order 0.95 value in the data\n  q95_weather_shock &lt;-\n    map(\n      .x = data_lp,\n      .f = ~ungroup(.x) |&gt;\n        summarise(\n          across(\n            .cols  = c(!!control_names_full, !!weather_names_full, !!share_geo),\n            .fns   = ~quantile(.x, probs = .95)\n          )\n        )\n    ) |&gt;\n    list_rbind(names_to = \"horizon\") |&gt;\n    pivot_longer(cols = -horizon, names_to = \"name\", values_to = \"q95_shock\") |&gt;\n    mutate(horizon = as.numeric(horizon))\n\n\n  if (detrend == TRUE) {\n    # Formula for the regressions\n    formula_lp &lt;- paste0(\n      \"y_lead\",\n      \" ~ -1+\",\n      # \" ~ 1+\", # intercept\n      paste(weather_names_full, collapse = \" + \"),\n      \" + \",\n      paste(control_names_full, collapse = \" + \"),\n      \" + \",\n      paste0(\n        c(\n          paste0(group_names_full, \":time\"),\n          paste0(group_names_full, \":I(time^2)\")\n        ),\n        collapse = \" + \"\n      )\n    )\n  } else {\n    # Formula for the regressions\n    formula_lp &lt;- paste0(\n      \"y_lead\",\n      \" ~ -1+\",\n      # \" ~ 1+\", # intercept\n      paste(weather_names_full, collapse = \" + \"),\n      \" + \",\n      paste(control_names_full, collapse = \" + \"),\n      \" + \",\n      ifelse(\n        add_intercept,\n        # removing last group\n        yes = paste(group_names_full[-length(group_names_full)], collapse = \" + \"),\n        # keeping last group\n        no = paste(group_names_full, collapse = \" + \")\n      )\n    )\n  }\n\n  if (add_year_fe) {\n    names_year_fe &lt;-\n      colnames(data_lp[[1]])[str_detect(colnames(data_lp[[1]]), \"^year_[[:digit:]]{4}\")]\n    names_year_fe &lt;- str_c(names_year_fe, collapse = \" + \")\n    formula_lp &lt;- paste0(formula_lp, \" + \", names_year_fe)\n  }\n\n  empty_res &lt;- vector(mode = \"list\", length = horizons + 1)\n  reg_lp &lt;- empty_res\n  sig_ols &lt;- empty_res\n  log_likelihood &lt;- empty_res\n  mse &lt;- empty_res\n  coefs &lt;- empty_res\n  cl_std &lt;- empty_res\n\n\n  for (h in 0:horizons) {\n    # Global assignment... otherwise, errors with coeftest()\n    current_data_h &lt;&lt;- data_lp[[h+1]]\n    # Regression\n    reg_h &lt;- lm(formula = formula_lp, data = current_data_h)\n    # Standard error of the residuals\n    sig_ols_h &lt;- sd(reg_h$residuals)\n    # Log likelihood\n    u_h &lt;- reg_h$residuals\n    log_likelihood_h &lt;-\n      sum(log(1 / sqrt(2 * pi * sig_ols_h^2) * exp(-u_h^2 / (2 * sig_ols_h^2))))\n    mse_h &lt;- mean(u_h^2)\n    coefs_h &lt;- enframe(coef(reg_h)) |&gt; mutate(horizon = h)\n    if (std == \"Cluster\") {\n      cl_std_h &lt;- coeftest(\n        reg_h,\n        vcov = vcovCL,\n        cluster = formula(str_c(\"~\", group_name)))[, \"Std. Error\"] |&gt;\n        enframe() |&gt;\n        mutate(horizon = h)\n    } else {\n      cl_std_h &lt;- enframe(sqrt(diag(vcov(reg_h))), value = \"std\") |&gt;\n        mutate(horizon = h)\n    }\n\n    # Store results in lists\n    reg_lp[[h+1]] &lt;- reg_h\n    sig_ols[[h+1]] &lt;- sig_ols_h\n    log_likelihood[[h+1]] &lt;- log_likelihood_h\n    mse[[h+1]] &lt;- mse_h\n    coefs[[h+1]] &lt;- coefs_h\n    cl_std[[h+1]] &lt;- cl_std_h\n  }\n  se_df &lt;- list_rbind(cl_std) |&gt; rename(std = value)\n\n  coefs &lt;-\n    coefs |&gt; list_rbind() |&gt;\n    left_join(se_df, by = c(\"horizon\", \"name\")) |&gt;\n    mutate(\n      crop = crop_name,\n      horizon = as.numeric(horizon)\n    ) |&gt;\n    left_join(sd_weather_shock, by = c(\"horizon\", \"name\")) |&gt;\n    left_join(median_weather_shock, by = c(\"horizon\", \"name\")) |&gt;\n    left_join(q05_weather_shock, by = c(\"horizon\", \"name\")) |&gt;\n    left_join(q95_weather_shock, by = c(\"horizon\", \"name\"))\n\n  list(\n    # reg_lp = reg_lp,\n    coefs = coefs,\n    horizons = horizons,\n    log_likelihood = log_likelihood |&gt; list_c(),\n    mse = mse |&gt; list_c(),\n    crop_name = crop_name,\n    data_lp = data_lp\n  )\n}\n\n\n\n\n\n\n\nNote\n\n\n\nThe get_data_lp() function is defined in the R script saved here: /weatherperu/R/format_data-annual.R. The estimation function, estimate_linear_lp(), is defined in the R script saved here: /weatherperu/R/estimations-annual.R.\n\nsource(\"../weatherperu/R/estimations-annual.R\")\n\n\n\n\n\n14.1.2 Estimation\nTo loop over the different crops, we can used the map() function. This function enables us to apply the estimate_linear_lp() function to each crop iteratively.\n\ncrops &lt;- df$product_eng |&gt; unique()\nweather_variables &lt;- c(\"temp_max_dev\", \"precip_piscop_sum_dev\")\ncontrol_variables &lt;- c(\n  \"rer_hp\", \"r_hp\", \"pi\", \"ind_prod\", \"ONI\", \"int_price_inf\"\n)\n\nThe estimations are run as follows (in this notebook, we load previously estimated results to save compilation time)\n\nresul_lp_year &lt;- map(\n  crops, ~ estimate_linear_lp(\n    df = df,\n    horizons = 4,\n    y_name = \"y_new_normalized\",\n    group_name = \"region_id\",\n    detrend = TRUE,\n    add_year_fe = FALSE,\n    add_intercept = FALSE,\n    crop_name = .x,\n    control_names = control_variables,\n    weather_names = weather_variables,\n    std = \"Cluster\",\n    other_var_to_keep = \"y_new\"\n  )\n)\nsave(resul_lp_year, file = \"..R/output/resul_lp_year.rda\")\n\n\nload(\"../R/output/resul_lp_year.rda\")\n\n\n\n14.1.3 Results\nWe can visualize the Impulse Response Functions (IRFs) by plotting the estimated coefficients associated with the weather variables. These coefficients represent the impact of weather on agricultural production and can provide valuable insights into the dynamics of the system. By plotting the IRFs, we can gain a better understanding of the relationship between weather variables and the response of agricultural production over time.\nThe data for the graphs:\n\ndf_irfs_lp_year &lt;- map(resul_lp_year, \"coefs\") |&gt; \n  list_rbind() |&gt; \n  filter(name %in% weather_variables) |&gt; \n  mutate(\n    shock_1_sd = value * std_shock,\n    lower_95 = (value - qnorm(0.975) * std) * std_shock,\n    upper_95 = (value + qnorm(0.975) * std) * std_shock,\n    lower_68 = (value - qnorm(0.84)  * std) * std_shock,\n    upper_68 = (value + qnorm(0.84)  * std) * std_shock\n  ) |&gt; \n  mutate(\n    crop = factor(\n      crop, \n      levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"))\n  ) |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = c(\n        \"temp_max_dev\",\n        \"precip_piscop_sum_dev\"\n      ),\n      labels = c(\n        \"Temp. anomalies\", \n        \"Precip. anomalies\"\n      )\n    )\n  )\n\nFor the confidence intervals:\n\ndf_irfs_lp_ci_year &lt;- \n  df_irfs_lp_year |&gt; \n  select(horizon, crop, name, matches(\"^(lower)|^(upper)\", perl = TRUE)) |&gt; \n  pivot_longer(\n    cols = matches(\"^(lower)|^(upper)\", perl = TRUE),\n    names_pattern = \"(.*)_(95|68)$\",\n    names_to = c(\".value\", \"level\")\n  ) |&gt; \n  mutate(level = str_c(level, \"%\"))\n\n\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci_year |&gt; filter(horizon &lt;= 8),\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs_lp_year |&gt; filter(horizon &lt;= 8),\n    mapping = aes(x = horizon, y = shock_1_sd),\n    colour = \"#0072B2\") +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", axes = \"all\",\n    independent = \"y\", switch = \"y\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Horizon (years)\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"gray10\", \"95%\" = \"gray60\")\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\nFigure 14.1: Agricultural production response to a weather shock\n\n\n\n\n\n\n\n\n\nAcevedo, Sebastian, Mico Mrkaic, Natalija Novta, Evgenia Pugacheva, and Petia Topalova. 2020. “The Effects of Weather Shocks on Economic Activity: What Are the Channels of Impact?” Journal of Macroeconomics 65: 103207.\n\n\nJordà, Òscar. 2005. “Estimation and Inference of Impulse Responses by Local Projections.” American Economic Review 95 (1): 161–82. https://doi.org/10.1257/0002828053828518.",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Annual Agricultural Production (LP)</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections-comparison.html",
    "href": "robustness-local_projections-comparison.html",
    "title": "15  Monthly/Quarterly/Annual Comparison (LP)",
    "section": "",
    "text": "\\[\n\\definecolor{bayesred}{RGB}{147, 30, 24}\n\\definecolor{bayesblue}{RGB}{32, 35, 91}\n\\definecolor{bayesorange}{RGB}{218, 120, 1}\n\\definecolor{grey}{RGB}{128, 128, 128}\n\\definecolor{couleur1}{RGB}{0,163,137}\n\\definecolor{couleur2}{RGB}{255,124,0}\n\\definecolor{couleur3}{RGB}{0, 110, 158}\n\\definecolor{coul1}{RGB}{255,37,0}\n\\definecolor{coul2}{RGB}{242,173,0}\n\\definecolor{col_neg}{RGB}{155, 191, 221}\n\\definecolor{col_pos}{RGB}{255, 128, 106}\n\\definecolor{wongBlack}{RGB}{0,0,0}\n\\definecolor{wongLightBlue}{RGB}{86, 180, 233}\n\\definecolor{wongGold}{RGB}{230, 159, 0}\n\\definecolor{wongGreen}{RGB}{0, 158, 115}\n\\definecolor{wongYellow}{RGB}{240, 228, 66}\n\\definecolor{wongBlue}{RGB}{0, 114, 178}\n\\definecolor{wongOrange}{RGB}{213, 94, 0}\n\\definecolor{wongPurple}{RGB}{204, 121, 167}\n\\definecolor{IBMPurple}{RGB}{120, 94, 240}\n\\definecolor{IBMMagenta}{RGB}{220, 38, 127}\n\\]\n\n\n\n\n\n\nObjectives\n\n\n\nThis chapter compares the response of agricultural production to a standard weather shock depending on the aggregation level of the agricultural data: monthly (as in Chapter 7), quarterly (Chapter 13), or annual (Chapter 14).\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nLet us load the theme function for graphs:\n\nsource(\"../weatherperu/R/utils.R\")\n\nLet us now load the estimations made in Chapter 7 with monthly production data:\n\nload(\"../R/output/df_irfs_lp_piscop.rda\")\n\nThose made in Chapter 13 with quarterly production data:\n\nload(\"../R/output/df_irfs_lp_quarter.rda\")\n\nAnd the estimations made in Chapter 14 with annual production data:\n\nload(\"../R/output/df_irfs_lp_year.rda\")\n\nLet us merge the IRfs. We need to make sure that the values for each quarter are repeated 3 times so that the horizons can be compared. The same reasoning applies to annual data for which each year response is repeated 12 times.\n\ndf_irfs_lp_comparison &lt;- df_irfs_lp |&gt; \n  mutate(data_type = \"monthly\") |&gt; \n  bind_rows(\n    df_irfs_lp_quarter |&gt; \n      mutate(data_type = \"quarterly\") |&gt; \n      mutate(count = 3) |&gt; \n      uncount(count) |&gt; \n      group_by(crop, horizon, name) |&gt; \n      mutate(\n        horizon = ifelse(\n          horizon != 0, \n          yes = (horizon-1)*3 + row_number(), no = 0\n        )\n      )\n  ) |&gt; \n  bind_rows(\n    df_irfs_lp_year |&gt; \n      mutate(data_type = \"annual\") |&gt; \n      mutate(count = 12) |&gt; \n      uncount(count) |&gt; \n      group_by(crop, horizon, name) |&gt; \n      mutate(\n        horizon = ifelse(\n          horizon != 0, \n          yes = (horizon-1)*12 + row_number(), no = 0\n        )\n      )\n  ) |&gt; \n  mutate(\n    data_type = factor(\n      data_type,\n      levels = c(\"monthly\", \"quarterly\", \"annual\"),\n      labels = c(\"Monthly\", \"Quarterly\", \"Annual\")\n    )\n  )\n\nWe do the same with confidence intervals:\n\ndf_irfs_lp_ci_comparison &lt;- df_irfs_lp_ci |&gt; \n  mutate(data_type = \"monthly\") |&gt; \n  bind_rows(\n    df_irfs_lp_ci_quarter |&gt; \n      mutate(data_type = \"quarterly\") |&gt; \n      mutate(count = 3) |&gt; \n      uncount(count) |&gt; \n      group_by(crop, horizon, name, level) |&gt; \n      mutate(\n        horizon = ifelse(\n          horizon != 0, \n          yes = (horizon-1)*3 + row_number(), no = 0\n        )\n      )\n  ) |&gt; \n  bind_rows(\n    df_irfs_lp_ci_year |&gt; \n      mutate(data_type = \"annual\") |&gt; \n      mutate(count = 12) |&gt; \n      uncount(count) |&gt; \n      group_by(crop, horizon, name, level) |&gt; \n      mutate(\n        horizon = ifelse(\n          horizon != 0, \n          yes = (horizon-1)*12 + row_number(), no = 0\n        )\n      )\n  ) |&gt; \n  mutate(\n    data_type = factor(\n      data_type,\n      levels = c(\"monthly\", \"quarterly\", \"annual\"),\n      labels = c(\"Monthly\", \"Quarterly\", \"Annual\")\n    )\n  )\n\nThen, we can plot the IRfs:\n\nnb_h &lt;- 14\n\n# Duration of the growing season\ngs_duration_df &lt;- tribble(\n  ~crop, ~tc,\n  \"Rice\", 4,\n  \"Maize\", 5,\n  \"Potato\", 6,\n  \"Cassava\", 9\n)\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci_comparison |&gt; \n      filter(level == \"68%\", horizon &lt;= !!nb_h),\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = data_type, colour = data_type),\n    alpha = .2, linetype = \"dashed\"\n  ) +\n  geom_line(\n    data = df_irfs_lp_comparison |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(x = horizon, y = shock_1_sd, colour = data_type)\n  ) +\n  scale_colour_manual(\n    NULL, \n    values = c(\"Monthly\" = \"#56B4E9\", \"Quarterly\" = \"#E69F00\", \"Annual\" = \"#CC79A7\")\n  ) +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  geom_vline(\n    data = gs_duration_df, \n    mapping = aes(xintercept = tc),\n    colour = \"#D55E00\", linetype = \"dashed\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", axes = \"all\",\n    independent = \"y\", switch = \"y\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_x_continuous(breaks = seq(0, nb_h, by = 2)) +\n  labs(x = \"Horizon (in months)\", y = NULL) +\n  scale_fill_manual(\n    NULL,\n    values = c(\"Monthly\" = \"#56B4E9\", \"Quarterly\" = \"#E69F00\", \"Annual\" = \"#CC79A7\")\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\nFigure 15.1: Agricultural production response to a weather shock, using either monthly, quarterly, or annual data.",
    "crumbs": [
      "III. Robustness Check: Data Frequency",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Monthly/Quarterly/Annual Comparison (LP)</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections_chirps.html",
    "href": "robustness-local_projections_chirps.html",
    "title": "16  Agricultural Production (LP)",
    "section": "",
    "text": "16.1 Linear Local Projections\nIn this section, we focus on estimating the Local Projections (Jordà 2005) to quantify the impact of weather on agricultural production. We use panel data, similar to the approach used in the study by Acevedo et al. (2020), and independently estimate models for each specific crop.\nFor a particular crop denoted as \\(c\\), the model can be expressed as follows: \\[\n\\begin{aligned}\n\\underbrace{y_{c,i,{\\color{wongGold}t+h}}}_{\\text{Production}} = &  {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}T}}} {\\color{wongPurple}{T_{i,{\\color{wongGold}t}}}} + {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}P}}} {\\color{wongPurple}P_{i,{\\color{wongGold}t}}}\\\\\n        &+\\gamma_{c,i,h}\\underbrace{X_{t}}_{\\text{controls}}  + \\underbrace{\\zeta_{c,i,h} \\text{Trend}_{t} + \\eta_{c,i,h} \\text{Trend}^2_{t}}_{\\text{regional monthly trend}} + \\varepsilon_{c,i,t+h}\n\\end{aligned}\n\\tag{16.1}\\]\nHere, \\(i\\) represents the region, \\(t\\) represents the time, and \\(h\\) represents the horizon. The primary focus lies on estimating the coefficients associated with temperature and precipitation for different time horizons \\(\\color{wongGold}h=\\{0,1,...,T_{c}\\}\\)\nNote that we allow a crop regional monthly specific quadratic trend to be estimated.",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Agricultural Production (LP)</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections_chirps.html#sec-lp-linear-chirps",
    "href": "robustness-local_projections_chirps.html#sec-lp-linear-chirps",
    "title": "16  Agricultural Production (LP)",
    "section": "",
    "text": "16.1.1 Functions\nThe estimation functions presented in Chapter 7.1.1 can be sourced.\n\nsource(\"../weatherperu/R/estimations.R\")\n\n\n\n16.1.2 Estimation\nTo loop over the different crops, we can use the map() function. This function enables us to apply the estimate_linear_lp() function to each crop iteratively, facilitating the estimation process.\n\ncrops &lt;- df$product_eng |&gt; unique()\nweather_variables &lt;- c(\"temp_max_dev\", \"precip_sum_dev\")\ncontrol_variables &lt;- c(\n  \"rer_hp\", \"r_hp\", \"pi\", \"ind_prod\", \"ONI\", \"price_int_inf\"\n)\nnb_h &lt;- 14\n\nThe estimation (this code takes about a minute to run, we load results in this notebook):\n\nresul_lp &lt;- vector(mode = \"list\", length = length(crops))\nfor (i_crop in 1:length(crops)) {\n  resul_lp[[i_crop]] &lt;- estimate_linear_lp(\n    df,\n    horizons = nb_h,\n    y_name = \"y_new_normalized\",\n    group_name = \"region_id\",\n    detrend = TRUE,\n    add_month_fe = FALSE,\n    add_intercept = FALSE,\n    crop_name = crops[i_crop],\n    control_names = control_variables,\n    weather_names = weather_variables,\n    std = \"Cluster\",\n    # std = \"nw\",\n    other_var_to_keep = \"y_new\"\n  )\n}\nsave(resul_lp, file = \"..R/output/resul_lp_chirps.rda\")\n\n\nload(\"../R/output/resul_lp_chirps.rda\")\n\n\n\n16.1.3 Results\nWe can visualize the Impulse Response Functions (IRFs) by plotting the estimated coefficients associated with the weather variables. These coefficients represent the impact of weather on agricultural production and can provide valuable insights into the dynamics of the system. By plotting the IRFs, we can gain a better understanding of the relationship between weather variables and the response of agricultural production over time.\nThe data for the graphs:\n\ndf_irfs_lp &lt;- map(resul_lp, \"coefs\") |&gt; \n  list_rbind() |&gt; \n  filter(name %in% weather_variables) |&gt; \n  mutate(\n    shock_1_sd = value * std_shock,\n    lower_95 = (value - qnorm(0.975) * std) * std_shock,\n    upper_95 = (value + qnorm(0.975) * std) * std_shock,\n    lower_68 = (value - qnorm(0.84)  * std) * std_shock,\n    upper_68 = (value + qnorm(0.84)  * std) * std_shock\n  ) |&gt; \n  mutate(\n    crop = factor(\n      crop, \n      levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"))\n  ) |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = c(\n        \"temp_max_dev\",\n        \"precip_sum_dev\"\n      ),\n      labels = c(\n        \"Temp. anomalies\", \n        \"Precip. anomalies\"\n      )\n    )\n  )\n\nFor the confidence intervals:\n\ndf_irfs_lp_ci &lt;- \n  df_irfs_lp |&gt; \n  select(horizon, crop, name, matches(\"^(lower)|^(upper)\", perl = TRUE)) |&gt; \n  pivot_longer(\n    cols = matches(\"^(lower)|^(upper)\", perl = TRUE),\n    names_pattern = \"(.*)_(95|68)$\",\n    names_to = c(\".value\", \"level\")\n  ) |&gt; \n  mutate(level = str_c(level, \"%\"))\n\n\n# Duration of the growing season\ngs_duration_df &lt;- tribble(\n  ~crop, ~tc,\n  \"Rice\", 4,\n  \"Maize\", 5,\n  \"Potato\", 6,\n  \"Cassava\", 9\n)\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs_lp |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(x = horizon, y = shock_1_sd),\n    colour = \"#0072B2\") +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  geom_vline(\n    data = gs_duration_df, \n    mapping = aes(xintercept = tc),\n    colour = \"#D55E00\", linetype = \"dashed\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", axes = \"all\",\n    independent = \"y\", switch = \"y\") +\n  scale_x_continuous(breaks = seq(0, nb_h, by = 2)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"gray10\", \"95%\" = \"gray60\")\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\nFigure 16.1: Agricultural production response to a weather shock\n\n\n\n\n\n\n\n16.1.4 Exporting results\nLet us save the results for later use.\n\nsave(df_irfs_lp, df_irfs_lp_ci, file = \"../R/output/df_irfs_lp_piscop.rda\")\nsave(resul_lp, file = \"../R/output/resul_lp_piscop.rda\")",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Agricultural Production (LP)</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections_chirps.html#comparaison-between-piscop-and-chirps",
    "href": "robustness-local_projections_chirps.html#comparaison-between-piscop-and-chirps",
    "title": "16  Agricultural Production (LP)",
    "section": "16.2 Comparaison between PISCOp and CHIRPS",
    "text": "16.2 Comparaison between PISCOp and CHIRPS\nWe can plot the IRFs obtained either using PISCOp or CHIRPS rainfall data.\n\n\nCode\n# With Piscop data\nload(\"../R/output/df_irfs_lp_piscop.rda\")\ndf_irfs_lp_piscop &lt;- df_irfs_lp\ndf_irfs_lp_ci_piscop &lt;- df_irfs_lp_ci\nrm(df_irfs_lp, df_irfs_lp_ci)\n\n# With Chirps data\nload(\"../R/output/df_irfs_lp_chirps.rda\")\ndf_irfs_lp_chirps &lt;- df_irfs_lp\ndf_irfs_lp_ci_chirps &lt;- df_irfs_lp_ci\nrm(df_irfs_lp, df_irfs_lp_ci)\n\n# Merge results\ndf_irfs_lp &lt;- \n  df_irfs_lp_piscop |&gt; mutate(data = \"PISCOp\") |&gt; \n  bind_rows(df_irfs_lp_chirps |&gt; mutate(data = \"CHIRPS\")) |&gt; \n  mutate(data = factor(data, levels = c(\"PISCOp\", \"CHIRPS\")))\n\ndf_irfs_lp_ci &lt;- \n  df_irfs_lp_ci_piscop |&gt; mutate(data = \"PISCOp\") |&gt; \n  bind_rows(df_irfs_lp_ci_chirps |&gt; mutate(data = \"CHIRPS\"))\n\n# Aesthetics\ndf_irfs_lp_ci &lt;- df_irfs_lp_ci |&gt; \n  mutate(\n    fill_lab = str_c(data, \"_\", level),\n    fill_lab = factor(\n      fill_lab, \n      levels = c(\"PISCOp_68%\", \"PISCOp_95%\", \"CHIRPS_68%\", \"CHIRPS_95%\"), \n      labels = c(\"PISCOp, 68%\", \"PISCOp, 95%\", \"CHIRPS, 68%\", \"CHIRPS, 95%\")\n    )\n  )\n\n\n\n\nCode\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci |&gt; \n      filter(horizon &lt;= !!nb_h),\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = fill_lab),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs_lp |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(x = horizon, y = shock_1_sd, colour = data),\n    linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  geom_vline(\n    data = gs_duration_df, \n    mapping = aes(xintercept = tc),\n    colour = \"#D55E00\", linetype = \"dashed\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", axes = \"all\",\n    independent = \"y\", switch = \"y\") +\n  scale_x_continuous(breaks = seq(0, nb_h, by = 2)) +\n  scale_y_continuous(labels = scales::label_percent(suffix = \"\\\\%\")) +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"Data, C.I. level\", \n    values = c(\n      \"PISCOp, 68%\" = \"#117733\", \n      \"PISCOp, 95%\" = \"#44AA99\", \n      \"CHIRPS, 68%\" = \"#882255\", \n      \"CHIRPS, 95%\" = \"#CC6677\"\n    )\n  ) +\n  scale_colour_manual(\n    NULL, values = c(\"PISCOp\" = \"#117733\", \"CHIRPS\" = \"#882255\"), guide = \"none\"\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nFigure 16.2: Agricultural production response to a weather shock, using PISCOp vs. CHIRPS data\n\n\n\n\n\n\n\n\n\nAcevedo, Sebastian, Mico Mrkaic, Natalija Novta, Evgenia Pugacheva, and Petia Topalova. 2020. “The Effects of Weather Shocks on Economic Activity: What Are the Channels of Impact?” Journal of Macroeconomics 65: 103207.\n\n\nJordà, Òscar. 2005. “Estimation and Inference of Impulse Responses by Local Projections.” American Economic Review 95 (1): 161–82. https://doi.org/10.1257/0002828053828518.",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Agricultural Production (LP)</span>"
    ]
  },
  {
    "objectID": "robustness-ag-fluctuations_chirps.html",
    "href": "robustness-ag-fluctuations_chirps.html",
    "title": "17  Aggregate Fluctuations",
    "section": "",
    "text": "17.1 Synthetic measure of the weather\nThis section provides a detailed description of the methodology employed to create the synthetic measure of the weather.\nWe need to load the results from the estimations made in Chapter 16.\nload(\"../R/output/resul_lp_chirps.rda\")\n# Data used in the local projections\nload(\"../data/output/df_lp.rda\")\nThe average price of each crop in the data, denoted hereafter \\(p_c\\):\naverage_price_crops &lt;- \n  df |&gt; group_by(product_eng) |&gt; \n  summarise(price_crop = mean(Value_prices, na.rm = TRUE))\naverage_price_crops\n\n# A tibble: 4 × 2\n  product_eng price_crop\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Cassava          0.550\n2 Dent corn        0.680\n3 Potato           0.578\n4 Rice             0.671\nThen, we can put the coefficients in a table, for each crop and each time horizon:\nweather_variables &lt;- c(\n  \"temp_max_dev\", \"precip_sum_dev\"\n  )\ncoefs_weather &lt;- map(\n  .x = resul_lp,\n  .f = ~ .x$coefs |&gt; \n    filter(name %in% weather_variables) |&gt; \n    select(horizon, name, value)\n)\nnames(coefs_weather) &lt;- map_chr(resul_lp, \"crop_name\")\ncoefs_weather &lt;- \n  list_rbind(coefs_weather, names_to = \"crop\")\nThen, we follow a methodology consisting of several key steps.",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "robustness-ag-fluctuations_chirps.html#sec-synthetic-measure-weather-chirps",
    "href": "robustness-ag-fluctuations_chirps.html#sec-synthetic-measure-weather-chirps",
    "title": "17  Aggregate Fluctuations",
    "section": "",
    "text": "17.1.1 Step 1: Weather Shock Contribution\nThe first step involves estimating the contribution of weather shocks at each time period (\\(t\\)) for the chosen crop (\\(c\\)) and time horizon (\\(h\\)). This contribution is determined by considering the weather variables, temperature (\\(T_{i,t}\\)) and precipitation (\\(P_{i,t}\\)), and their respective coefficients (\\(\\beta_{c,h}^{T}\\) as well as \\(\\beta_{c,h}^{P}\\)). The weather shock contribution (\\(\\Gamma_{c,i,t,h}\\)) is obtained by multiplying these coefficients with the corresponding weather variables and summing them up:\n\\[\\Gamma_{c,i,t,h} = \\beta_{c,h}^{T} T_{i,t-h} + \\beta_{c,h}^{P} P_{i,t-h} \\tag{17.1}\\]\nThis first step is performed, using two user-defined functions: weather_contrib_crop_h() which computes the contribution of the weather for a single crop and time horizon, and with contrib_weather_crop() which uses the former to compute the contribution of the weather for a single crop, for all horizons. We will consider 9 horizons:\n\nnb_periods_wcal &lt;- 8\ncoefs_weather &lt;- coefs_weather |&gt;\n  filter(horizon &lt;= nb_periods_wcal)\n\n\n#' Computes the contribution of the weather for a single crop and time horizon\n#'\n#' @param lp_crop LP for a specific crop\n#' @param h horizon (single value)\n#' @param weather_variables vector of names of the weather variables\n#' @param ic_conf confidence interval used to determine whether a shock has a\n#'   significant effect (default to .95)\nweather_contrib_crop_h &lt;- function(lp_crop,\n                                   h,\n                                   weather_variables,\n                                   ic_conf = .95) {\n  # The data\n  data_lp &lt;- lp_crop$data_lp\n  data_lp &lt;- data_lp[[which(names(data_lp) == h)]] |&gt; \n    select(region_id, date, product_eng, !!weather_variables) |&gt; \n    ungroup()\n  # The coefficients\n  lp_coefs &lt;- \n    lp_crop$coefs |&gt; \n    filter(horizon == !!h, name %in% !!weather_variables) |&gt; \n    mutate(\n      value_lb = value - qnorm(1 - ((1 - ic_conf) / 2)) * std,\n      value_ub = value + qnorm(1 - ((1 - ic_conf) / 2)) * std\n    )\n  \n  # Keeping the values\n  lp_coefs_value &lt;- lp_coefs$value\n  # The lower and upper bounds\n  lp_coefs_value_lb &lt;- lp_coefs$value_lb\n  lp_coefs_value_ub &lt;- lp_coefs$value_ub\n  \n  data_lp |&gt; \n    nest(.by = c(date, region_id)) |&gt; \n    mutate(\n      contribution = map(\n        .x = data,\n        .f = ~ as.matrix(.x[, weather_variables]) %*% lp_coefs_value |&gt; \n          sum()\n      ),\n      contribution_lb = map(\n        .x = data,\n        .f = ~ as.matrix(.x[, weather_variables]) %*% lp_coefs_value_lb |&gt; \n          sum()\n      ),\n      contribution_ub = map(\n        .x = data,\n        .f = ~ as.matrix(.x[, weather_variables]) %*% lp_coefs_value_ub |&gt; \n          sum()\n      )\n    ) |&gt; \n    unnest(cols = c(contribution, contribution_lb, contribution_ub)) |&gt; \n    select(-data) |&gt; \n    mutate(\n      significant = (contribution_lb &gt; 0 & contribution_ub &gt; 0) | (contribution_lb &lt; 0 & contribution_ub &lt; 0),\n      significant = as.numeric(significant)\n    ) |&gt; \n    mutate(date = date + lubridate::period(str_c(!!h, \" month\")))\n}\n\n\n#' Computes the contribution of the weather for a single crop, for all horizons\n#'\n#' @param lp_crop LP for a specific crop\n#' @param weather_variables vector of names of the weather variables\n#' @param horizons vector of horizons. If `NULL`, uses all horizons in lp_crop\ncontrib_weather_crop &lt;- function(lp_crop,\n                                 weather_variables,\n                                 horizons = NULL) {\n  if (is.null(horizons)) horizons &lt;- unique(lp_crop$coefs$horizon)\n  \n  map(\n    .x = horizons,\n    .f = ~weather_contrib_crop_h(\n      lp_crop = lp_crop, \n      h = .x, \n      weather_variables = weather_variables\n    ) |&gt; \n      mutate(horizon = .x)\n  ) |&gt; \n    list_rbind() |&gt; \n    # group_by(date) |&gt; \n    # summarise(value = sum(contribution)) |&gt; \n    mutate(crop = lp_crop$crop_name) |&gt; \n    mutate(contribution_signif = contribution * significant)\n}\n\nLet us compute the \\(\\Gamma_{c,i,t,h}\\) for each crop, region, date and horizon:\n\nweather_measure_crop &lt;- \n  map(\n    .x = resul_lp,\n    .f = ~contrib_weather_crop(\n      lp_crop = .x, \n      weather_variables = weather_variables,\n      horizons = 0:nb_periods_wcal\n    )\n  ) |&gt; \n  list_rbind()\n\nLet us add the monthly regional selling prices of the crops (df$Value_prices), as well as the average crop-specific prices computed earlier on the whole sample (average_price_crops$price_crop) which will be used as a weight when we will aggregate all crops.\n\nweather_measure_crop &lt;- \n  weather_measure_crop |&gt; \n  left_join(\n    df |&gt; select(product_eng, region_id, date, y_new, Value_prices),\n    by = c(\"date\", \"crop\" = \"product_eng\", \"region_id\")\n  ) |&gt; \n  left_join(\n    average_price_crops,\n    by = c(\"crop\" = \"product_eng\")\n  )\n\nweather_measure_crop\n\n# A tibble: 123,552 × 12\n   date       region_id contribution contribution_lb contribution_ub significant\n   &lt;date&gt;     &lt;fct&gt;            &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1 2001-01-01 1               0.340           0.444         0.236              1\n 2 2001-02-01 1               0.239           0.376         0.101              1\n 3 2001-03-01 1               0.162           0.206         0.118              1\n 4 2001-04-01 1               0.0901          0.180         0.000664           1\n 5 2001-05-01 1               0.112           0.142         0.0829             1\n 6 2001-06-01 1               0.190           0.283         0.0980             1\n 7 2001-07-01 1               0.111           0.149         0.0731             1\n 8 2001-08-01 1               0.0361          0.0565        0.0157             1\n 9 2001-09-01 1               0.136           0.140         0.132              1\n10 2001-10-01 1               0.0317          0.0580        0.00533            1\n# ℹ 123,542 more rows\n# ℹ 6 more variables: horizon &lt;int&gt;, crop &lt;chr&gt;, contribution_signif &lt;dbl&gt;,\n#   y_new &lt;dbl&gt;, Value_prices &lt;dbl&gt;, price_crop &lt;dbl&gt;\n\n\n\n\n17.1.2 Step 2: Quantity Weights\nFor each crop and date \\(t\\), we define some weights for the regional observations using the data observed at horizon \\(h=0\\), as the sum of the monthly agricultural production over the regions considered in the analysis. The monthly production is expressed in monetary terms, by multiplying the quantities (y_new, i.e., \\(y^{raw}\\)) by unit price (price_crop, i.e., \\(p_c\\)).\n\\[\\omega_{c,t} = \\sum_{i} y^{\\text{raw}}_{c,t,i} \\times p_{c}\\]\n\nquantity_weight &lt;- \n  weather_measure_crop |&gt; \n  filter(horizon == 0) |&gt; \n  group_by(crop, date, horizon, price_crop) |&gt; \n  summarise(quantity_weight = sum(price_crop * y_new), .groups = \"drop\") |&gt; \n  select(crop, date, quantity_weight)\nquantity_weight\n\n# A tibble: 720 × 3\n   crop    date       quantity_weight\n   &lt;chr&gt;   &lt;date&gt;               &lt;dbl&gt;\n 1 Cassava 2001-01-01          38797.\n 2 Cassava 2001-02-01          39625.\n 3 Cassava 2001-03-01          44893.\n 4 Cassava 2001-04-01          44893.\n 5 Cassava 2001-05-01          41857.\n 6 Cassava 2001-06-01          36333.\n 7 Cassava 2001-07-01          36333.\n 8 Cassava 2001-08-01          39766.\n 9 Cassava 2001-09-01          36525.\n10 Cassava 2001-10-01          36525.\n# ℹ 710 more rows\n\n\n\n\n17.1.3 Step 3: Crop-Specific Weather-Adjusted Agricultural Production\nFor each crop \\(c\\), at each date \\(t\\), we compute the weather-adjusted agricultural production, \\(y_{c,t}^{\\omega}\\), as the sum of the significant crop-specific contributions of the weather to the agricultural production. The crop-specific contribution across regions is first aggregated at the national level, using an average of the crop and region-specific contribution of the weather to the monetary equivalence of the agricultural production.\n\\[y_{c,t}^{\\omega} = \\sum_{h}\\sum_{i} \\frac{\\mathbb{1}_{\\text{signif}_{c,i,t,h}} \\times \\gamma_{c,i,t,h} \\times p_{c} \\times  y^{\\text{raw}}_{c,t,i}}{\\text{card}(I_{c,t})},\\]\nwhere \\(\\text{card(I)}\\) is the number of regions that produce crop \\(c\\) at time \\(t\\). The characteristic function \\(\\mathbb{1}_{\\text{signif}_{c,it,h}}\\) is equal to 1 when the contribution is significantly different from 0 (using the 95% confidence intervals of the coefficients \\(\\beta_{c,h}^{T}\\) and \\(\\beta_{c,h}^{P}\\)), and is equal to 0 otherwise.\n\nweather_adjusted_ag &lt;- \n  weather_measure_crop |&gt; \n  # each group: observations across regions, for a crop x date x horizon\n  group_by(crop, date, horizon, price_crop) |&gt; \n  # weather-adjusted agricultural production at each horizon\n  # y_{c,t,h}^{w}\n  summarise(\n    y_w = sum(price_crop * contribution_signif *  y_new / n(), na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt; \n  group_by(crop, date) |&gt; \n  # weather-adjusted agricultural production summed over horizons\n  # y_{c,t}^{w}\n  summarise(\n    y_w = sum(y_w),\n    .groups = \"drop\"\n  )\n\n\n\n17.1.4 Step 4: Aggregation at the National Level\nThen, we aggregate the crop-specific weather-adjusted agricultural production at the national level using the following formula: \\[y_{t}^{\\omega} = \\frac{\\sum_{c} y_{c,t}^{\\omega}}{\\sum_{c}\\omega_{c,t}},\\]\nwhere \\(\\omega_{c,t}\\) are the quantity weights computed in the second step.\n\nw_df &lt;- \n  weather_adjusted_ag |&gt; \n  left_join(quantity_weight, by = c(\"crop\", \"date\")) |&gt; \n  group_by(date) |&gt; \n  summarise(\n    w = sum(y_w) / sum(quantity_weight),\n    .groups = \"drop\"\n  )\n\n\n\n17.1.5 Step 5: Detrending\nLastly, we express the national weather-adjusted production as a loss, and take it as a deviation from its trend:\n\\[\nW_{t} = -100 \\times (y_{t}^{\\omega} - \\overline{y_{t}^{\\omega}}),\n\\]\nwhere \\(y_{t}^{\\omega}\\) is the trend, obtained with the HP filter.\n\n# Load detrending functions\nsource(\"../weatherperu/R/detrending.R\")\n\nw_df &lt;- w_df |&gt; \n  mutate(\n    w_trend = hp_filter_trend(w, freq = 14400),\n    w_dt = - 100 * (w - w_trend),\n  )",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "robustness-ag-fluctuations_chirps.html#agricultural-production",
    "href": "robustness-ag-fluctuations_chirps.html#agricultural-production",
    "title": "17  Aggregate Fluctuations",
    "section": "17.2 Agricultural Production",
    "text": "17.2 Agricultural Production\nIn the second step in the creation of the synthetic measure of the weater, we computed quantity weights \\(\\omega_{c,t}\\), which correspond to the agricultural production in month \\(t\\) for crop \\(t\\), expressed in monetary terms. Let us aggregate these values across crops to obtain a monthly agricultural production: \\[y_t^{A} = \\sum_{c}\\omega_{c,t}\\]\nWe then express these as percentage deviation from their trend computed using the HP filter.\n\nagricultural_output &lt;- \n  quantity_weight |&gt; \n  group_by(date) |&gt; \n  summarise(quantity_weight = sum(quantity_weight), .groups = \"drop\") |&gt; \n  mutate(\n    # Remove seasonality\n    q_sa = adj_season_X13(quantity_weight, ymd(\"2001-01-01\")),\n    # Extract trend\n    q_sa_trend = hp_filter_trend(q_sa, freq = 14400),\n    # Percentage dev. from trend\n    q = 100 * log(q_sa / q_sa_trend)\n  ) |&gt; \n  select(date, q)\n\nNote: this data is actually not used.",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "robustness-ag-fluctuations_chirps.html#sec-var-macro-indicators-chirps",
    "href": "robustness-ag-fluctuations_chirps.html#sec-var-macro-indicators-chirps",
    "title": "17  Aggregate Fluctuations",
    "section": "17.3 Macroeconomic indicators",
    "text": "17.3 Macroeconomic indicators\nThe vector of endogenous variables in the estimation, denoted as \\(Y_t\\), consists of eight variables: \\(W_t\\), \\(RER_t\\), \\(\\pi_t^{a}\\), \\(\\pi_{t}\\) , \\(X_t\\), \\(y^A_t\\), \\(y_t\\), and \\(r_t\\):\n\n\\(W_t\\) represents the aggregate measure of weather-driven agricultural losses defined in Section 17.1, which quantifies the loss in agricultural value added due to weather shocks, expressed as a deviation from its trend. It is the focal variable of interest in this analysis.\n\\(RER_t\\) denotes the Real Exchange Rate (RER), which reflects the relative value of the domestic currency against a basket of foreign currencies.\n\\(\\pi^a_t\\) corresponds to the percentage change of the Food Consumer Price Index (CPIA), which serves as a measure of food inflation.\n\\(\\pi_t\\) corresponds to the percentage change of the Consumer Price Index (CPI), which serves as a measure of inflation.\n\\(X_t\\) denotes Exports.\n\\(y^A_t\\) is the Agricultural production.\n\\(y_t\\) represents the Gross Domestic Product (GDP), which serves as a measure of the overall economic activity and output in the country.\n\\(r_t\\) is the nominal rate.\n\nTo construct the vectors \\(Y_t\\), we use data from the Central Reserve Bank of Perú.\nThe Real Exchange Rate (RER) data are obtained using the token PN01259PM, the Food Consumer Price Index (CPIA) data with token PN01336PM, the Consumer Price Index (CPI) data with token PN01270PM, the Exports data with token PN01461BM, the GDP data with token PN01773AM, the agricultural GDP with token PN01755AM, and the nominal rate with token PN07819NM.\nLet us load the dataset of macroeconomic variables (see Chapter 3 for details on the construction of these variables).\n\nload(\"../data/output/macro/df_macro.rda\")",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "robustness-ag-fluctuations_chirps.html#merging-the-data",
    "href": "robustness-ag-fluctuations_chirps.html#merging-the-data",
    "title": "17  Aggregate Fluctuations",
    "section": "17.4 Merging the Data",
    "text": "17.4 Merging the Data\nThe sample period for our analysis covers the time span from January 2003 (2003M1) to December 2015 (2015M12). This period provides a comprehensive view of the relationship between weather-driven agricultural losses and the selected economic indicators in Peru.\n\ndf_var &lt;- \n  df_macro |&gt; \n  left_join(\n    w_df,\n    by = \"date\"\n  ) |&gt; \n  mutate(\n    w = w_dt,\n    q = ya\n  )\n  \nvariable_names &lt;- c(\n  \"Agricultural losses\" = \"w\",\n  \"Real exchange rate\" = \"rer_dt_sa\",\n  \"Food inflation rate (pp)\" = \"pia\",\n  \"Inflation rate (pp)\" = \"pi\",\n  \"Exports (pp)\" = \"x\",\n  \"Agricultural output (pp)\" = \"q\",\n  \"GDP (pp)\" = \"y\",\n  \"Interest rate (pp)\" = \"r\"\n)\n\nLet us add labels to the new columns:\n\ndf_var &lt;- \n  df_var |&gt;  \n  labelled::set_variable_labels(\n    w = \"Agricultural losses\"\n  )\n\nLet us divide the all the values (except exports) by 100:\n\ndf_var &lt;- \n  df_var |&gt;  \n  labelled::set_variable_labels(\n    w = \"Agricultural losses\",\n    q = \"Agricultural output (pp)\"\n  ) |&gt; \n  mutate(\n    w = w / 100, \n    rer_dt_sa = rer_dt_sa / 100, \n    pi = pi / 100, \n    pia = pia / 100, \n    # x\n    q = q / 100, \n    y = y / 100, \n    r = r / 100\n  )\n\nFigure Figure 17.1 displays the time series data for the variables included in the vector \\(Y\\).\n\nggplot(\n    data = df_var |&gt; \n      filter(date &gt;= \"2003-01-01\") |&gt; \n      select(date, w, rer_dt_sa, pia, pi, x, q, y, r) |&gt; \n      pivot_longer(cols = -date) |&gt; \n      mutate(\n        name = factor(\n          name,\n          levels = variable_names,\n          labels = names(variable_names)\n        )\n      ),\n    mapping = aes(x = date, y = value)\n  ) +\n  geom_line() +\n  facet_wrap(~name, scales = \"free_y\") +\n  theme_paper() +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\nFigure 17.1: Time Series of Endogenous Variables for Vector Autoregression (VAR) Analysis in Peru (2003-2015)\n\n\n\n\n\nBefore proceeding to the estimation, there is one final step, which involves converting the data into the ts format.\n\nstart_date &lt;- \"2003-01-01\"\ndf_var_ts &lt;-\n  df_var |&gt; \n  filter(date &gt;= start_date) |&gt; \n  select(\n    w, rer_dt_sa, pia, pi, x, q, y, r\n  ) |&gt;\n  ts(start = c(year(start_date), month(start_date)), freq = 12)",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "robustness-ag-fluctuations_chirps.html#var-estimation",
    "href": "robustness-ag-fluctuations_chirps.html#var-estimation",
    "title": "17  Aggregate Fluctuations",
    "section": "17.5 VAR Estimation",
    "text": "17.5 VAR Estimation\nWe estimate a VAR(p) model with a constant term but no trend. Let us look how many lags we should use, using the automatic selection method provided by the VARselect() function from {vars}.\n\ninfo_var_estim &lt;- vars::VARselect(y = df_var_ts, type = \"const\", lag.max = 6)\ninfo_var_estim\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     2      1      1      2 \n\n$criteria\n                   1             2             3             4             5\nAIC(n) -7.747263e+01 -7.770196e+01 -7.739961e+01 -7.700066e+01 -7.688240e+01\nHQ(n)  -7.688553e+01 -7.659299e+01 -7.576878e+01 -7.484796e+01 -7.420784e+01\nSC(n)  -7.602753e+01 -7.497231e+01 -7.338543e+01 -7.170194e+01 -7.029915e+01\nFPE(n)  2.262378e-34  1.810786e-34  2.492771e-34  3.840991e-34  4.570219e-34\n                   6\nAIC(n) -7.688737e+01\nHQ(n)  -7.369094e+01\nSC(n)  -6.901958e+01\nFPE(n)  4.949715e-34\n\n\n\n17.5.1 Results\nLet us estimate a VAR(2) model with a constant term but no trend.\n\nvar_l1 &lt;- vars::VAR(y = df_var_ts, p = 2, type = \"const\")\nsummary(var_l1)\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: w, rer_dt_sa, pia, pi, x, q, y, r \nDeterministic variables: const \nSample size: 154 \nLog Likelihood: 4365.114 \nRoots of the characteristic polynomial:\n0.9046 0.9046 0.8016 0.8016 0.5208 0.5208 0.4869 0.3924 0.3924 0.3918 0.3918 0.3161 0.3001 0.3001 0.2654 0.2654\nCall:\nvars::VAR(y = df_var_ts, p = 2, type = \"const\")\n\n\nEstimation results for equation w: \n================================== \nw = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nw.l1          1.306040   0.080118  16.301  &lt; 2e-16 ***\nrer_dt_sa.l1  0.046111   0.054023   0.854   0.3948    \npia.l1       -0.136006   0.221138  -0.615   0.5396    \npi.l1         0.690791   0.459060   1.505   0.1347    \nx.l1         -0.006499   0.006041  -1.076   0.2839    \nq.l1         -0.004749   0.022919  -0.207   0.8362    \ny.l1         -0.058412   0.054691  -1.068   0.2874    \nr.l1          0.345238   0.270627   1.276   0.2042    \nw.l2         -0.465249   0.080684  -5.766 5.14e-08 ***\nrer_dt_sa.l2 -0.096123   0.055889  -1.720   0.0877 .  \npia.l2        0.023634   0.219890   0.107   0.9146    \npi.l2        -0.463362   0.452074  -1.025   0.3072    \nx.l2          0.002580   0.005809   0.444   0.6576    \nq.l2          0.014389   0.022552   0.638   0.5245    \ny.l2         -0.043382   0.055677  -0.779   0.4372    \nr.l2         -0.422871   0.263612  -1.604   0.1110    \nconst         0.007019   0.008466   0.829   0.4085    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.006198 on 137 degrees of freedom\nMultiple R-Squared: 0.8979, Adjusted R-squared: 0.886 \nF-statistic:  75.3 on 16 and 137 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation rer_dt_sa: \n========================================== \nrer_dt_sa = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nw.l1         -0.048520   0.125584  -0.386   0.6998    \nrer_dt_sa.l1  1.031043   0.084680  12.176   &lt;2e-16 ***\npia.l1        0.416236   0.346630   1.201   0.2319    \npi.l1        -0.974084   0.719569  -1.354   0.1781    \nx.l1          0.011715   0.009470   1.237   0.2182    \nq.l1         -0.025749   0.035925  -0.717   0.4747    \ny.l1          0.101000   0.085727   1.178   0.2408    \nr.l1         -0.358932   0.424204  -0.846   0.3990    \nw.l2          0.122906   0.126471   0.972   0.3329    \nrer_dt_sa.l2 -0.200048   0.087605  -2.284   0.0239 *  \npia.l2        0.033474   0.344674   0.097   0.9228    \npi.l2         0.166914   0.708618   0.236   0.8141    \nx.l2         -0.016094   0.009106  -1.767   0.0794 .  \nq.l2         -0.014405   0.035350  -0.407   0.6843    \ny.l2          0.011787   0.087273   0.135   0.8928    \nr.l2          0.385006   0.413207   0.932   0.3531    \nconst         0.004251   0.013271   0.320   0.7492    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.009716 on 137 degrees of freedom\nMultiple R-Squared: 0.7759, Adjusted R-squared: 0.7498 \nF-statistic: 29.65 on 16 and 137 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation pia: \n==================================== \npia = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n               Estimate Std. Error t value Pr(&gt;|t|)  \nw.l1          0.0228987  0.0583039   0.393   0.6951  \nrer_dt_sa.l1  0.0593240  0.0393139   1.509   0.1336  \npia.l1        0.1766751  0.1609276   1.098   0.2742  \npi.l1         0.1011576  0.3340698   0.303   0.7625  \nx.l1         -0.0049384  0.0043965  -1.123   0.2633  \nq.l1         -0.0182040  0.0166785  -1.091   0.2770  \ny.l1          0.0081665  0.0397998   0.205   0.8377  \nr.l1         -0.0758842  0.1969423  -0.385   0.7006  \nw.l2         -0.0329210  0.0587156  -0.561   0.5759  \nrer_dt_sa.l2 -0.0072449  0.0406719  -0.178   0.8589  \npia.l2       -0.0134118  0.1600194  -0.084   0.9333  \npi.l2         0.0458875  0.3289856   0.139   0.8893  \nx.l2          0.0027569  0.0042274   0.652   0.5154  \nq.l2          0.0008317  0.0164116   0.051   0.9597  \ny.l2          0.0822987  0.0405176   2.031   0.0442 *\nr.l2          0.0449044  0.1918371   0.234   0.8153  \nconst         0.0058256  0.0061610   0.946   0.3460  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.004511 on 137 degrees of freedom\nMultiple R-Squared: 0.195,  Adjusted R-squared: 0.101 \nF-statistic: 2.074 on 16 and 137 DF,  p-value: 0.01272 \n\n\nEstimation results for equation pi: \n=================================== \npi = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n               Estimate Std. Error t value Pr(&gt;|t|)  \nw.l1          0.0080372  0.0283975   0.283   0.7776  \nrer_dt_sa.l1  0.0443933  0.0191482   2.318   0.0219 *\npia.l1       -0.0271240  0.0783814  -0.346   0.7298  \npi.l1         0.2721367  0.1627121   1.673   0.0967 .\nx.l1         -0.0016133  0.0021414  -0.753   0.4525  \nq.l1         -0.0112904  0.0081234  -1.390   0.1668  \ny.l1          0.0260703  0.0193849   1.345   0.1809  \nr.l1         -0.0067475  0.0959227  -0.070   0.9440  \nw.l2         -0.0171631  0.0285981  -0.600   0.5494  \nrer_dt_sa.l2 -0.0149574  0.0198097  -0.755   0.4515  \npia.l2        0.0139319  0.0779391   0.179   0.8584  \npi.l2        -0.0333737  0.1602358  -0.208   0.8353  \nx.l2          0.0012462  0.0020590   0.605   0.5460  \nq.l2          0.0002241  0.0079934   0.028   0.9777  \ny.l2          0.0350258  0.0197345   1.775   0.0781 .\nr.l2         -0.0117855  0.0934362  -0.126   0.8998  \nconst         0.0030110  0.0030008   1.003   0.3174  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.002197 on 137 degrees of freedom\nMultiple R-Squared: 0.2757, Adjusted R-squared: 0.1911 \nF-statistic:  3.26 on 16 and 137 DF,  p-value: 8.545e-05 \n\n\nEstimation results for equation x: \n================================== \nx = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n             Estimate Std. Error t value Pr(&gt;|t|)    \nw.l1          0.19287    1.13440   0.170   0.8652    \nrer_dt_sa.l1 -0.18287    0.76491  -0.239   0.8114    \npia.l1       -1.44110    3.13111  -0.460   0.6461    \npi.l1         5.66205    6.49987   0.871   0.3852    \nx.l1          0.14015    0.08554   1.638   0.1036    \nq.l1         -0.49746    0.32451  -1.533   0.1276    \ny.l1          1.14128    0.77437   1.474   0.1428    \nr.l1         -1.09894    3.83183  -0.287   0.7747    \nw.l2         -1.37597    1.14241  -1.204   0.2305    \nrer_dt_sa.l2  0.07380    0.79134   0.093   0.9258    \npia.l2       -1.47822    3.11344  -0.475   0.6357    \npi.l2         3.89828    6.40095   0.609   0.5435    \nx.l2          0.14727    0.08225   1.790   0.0756 .  \nq.l2         -0.15771    0.31931  -0.494   0.6222    \ny.l2          0.33067    0.78834   0.419   0.6755    \nr.l2         -0.75509    3.73250  -0.202   0.8400    \nconst         0.79838    0.11987   6.660 6.09e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.08776 on 137 degrees of freedom\nMultiple R-Squared: 0.2285, Adjusted R-squared: 0.1384 \nF-statistic: 2.536 on 16 and 137 DF,  p-value: 0.001903 \n\n\nEstimation results for equation q: \n================================== \nq = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n              Estimate Std. Error t value Pr(&gt;|t|)  \nw.l1         -0.221275   0.307297  -0.720   0.4727  \nrer_dt_sa.l1 -0.165336   0.207208  -0.798   0.4263  \npia.l1       -0.404183   0.848187  -0.477   0.6345  \npi.l1        -0.947496   1.760751  -0.538   0.5914  \nx.l1         -0.004343   0.023172  -0.187   0.8516  \nq.l1          0.195050   0.087906   2.219   0.0281 *\ny.l1          0.099675   0.209769   0.475   0.6354  \nr.l1          0.664620   1.038006   0.640   0.5231  \nw.l2         -0.117578   0.309467  -0.380   0.7046  \nrer_dt_sa.l2  0.012057   0.214366   0.056   0.9552  \npia.l2       -1.124135   0.843400  -1.333   0.1848  \npi.l2         1.260305   1.733955   0.727   0.4686  \nx.l2         -0.023058   0.022281  -1.035   0.3026  \nq.l2         -0.143117   0.086499  -1.655   0.1003  \ny.l2         -0.013976   0.213553  -0.065   0.9479  \nr.l2         -0.292466   1.011098  -0.289   0.7728  \nconst         0.018331   0.032472   0.565   0.5733  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.02377 on 137 degrees of freedom\nMultiple R-Squared: 0.215,  Adjusted R-squared: 0.1233 \nF-statistic: 2.345 on 16 and 137 DF,  p-value: 0.004218 \n\n\nEstimation results for equation y: \n================================== \ny = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nw.l1         -0.135287   0.127746  -1.059  0.29145    \nrer_dt_sa.l1 -0.057330   0.086138  -0.666  0.50681    \npia.l1        0.235367   0.352599   0.668  0.50556    \npi.l1         0.543544   0.731961   0.743  0.45900    \nx.l1         -0.003029   0.009633  -0.314  0.75370    \nq.l1          0.041480   0.036543   1.135  0.25832    \ny.l1          0.401592   0.087203   4.605 9.31e-06 ***\nr.l1          0.364126   0.431509   0.844  0.40023    \nw.l2          0.202041   0.128648   1.570  0.11861    \nrer_dt_sa.l2  0.015182   0.089114   0.170  0.86497    \npia.l2       -0.226865   0.350609  -0.647  0.51868    \npi.l2         0.273563   0.720821   0.380  0.70489    \nx.l2          0.012710   0.009262   1.372  0.17225    \nq.l2          0.015528   0.035958   0.432  0.66654    \ny.l2          0.244103   0.088776   2.750  0.00677 ** \nr.l2         -0.338894   0.420323  -0.806  0.42149    \nconst        -0.013156   0.013499  -0.975  0.33148    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.009883 on 137 degrees of freedom\nMultiple R-Squared: 0.5336, Adjusted R-squared: 0.4792 \nF-statistic: 9.798 on 16 and 137 DF,  p-value: 5.699e-16 \n\n\nEstimation results for equation r: \n================================== \nr = w.l1 + rer_dt_sa.l1 + pia.l1 + pi.l1 + x.l1 + q.l1 + y.l1 + r.l1 + w.l2 + rer_dt_sa.l2 + pia.l2 + pi.l2 + x.l2 + q.l2 + y.l2 + r.l2 + const \n\n               Estimate Std. Error t value Pr(&gt;|t|)    \nw.l1         -7.247e-03  2.164e-02  -0.335   0.7382    \nrer_dt_sa.l1  7.094e-03  1.459e-02   0.486   0.6277    \npia.l1        3.625e-03  5.973e-02   0.061   0.9517    \npi.l1         2.501e-02  1.240e-01   0.202   0.8405    \nx.l1         -7.099e-06  1.632e-03  -0.004   0.9965    \nq.l1          2.544e-03  6.191e-03   0.411   0.6817    \ny.l1          1.608e-02  1.477e-02   1.088   0.2783    \nr.l1          1.459e+00  7.310e-02  19.960  &lt; 2e-16 ***\nw.l2          1.241e-02  2.179e-02   0.570   0.5699    \nrer_dt_sa.l2 -8.054e-03  1.510e-02  -0.534   0.5945    \npia.l2       -8.419e-03  5.939e-02  -0.142   0.8875    \npi.l2         2.040e-01  1.221e-01   1.670   0.0971 .  \nx.l2          5.125e-04  1.569e-03   0.327   0.7444    \nq.l2         -4.291e-03  6.091e-03  -0.704   0.4824    \ny.l2          2.622e-02  1.504e-02   1.743   0.0835 .  \nr.l2         -5.185e-01  7.120e-02  -7.282 2.34e-11 ***\nconst         1.187e-03  2.287e-03   0.519   0.6046    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.001674 on 137 degrees of freedom\nMultiple R-Squared: 0.9819, Adjusted R-squared: 0.9797 \nF-statistic: 463.6 on 16 and 137 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                   w  rer_dt_sa        pia         pi          x          q\nw          3.842e-05 -8.365e-06  8.254e-07 -2.939e-07  1.410e-05  1.128e-05\nrer_dt_sa -8.365e-06  9.440e-05 -4.134e-06 -2.720e-06 -3.258e-05  1.323e-05\npia        8.254e-07 -4.134e-06  2.035e-05  8.395e-06 -2.955e-05  3.655e-06\npi        -2.939e-07 -2.720e-06  8.395e-06  4.827e-06 -2.616e-05  3.657e-06\nx          1.410e-05 -3.258e-05 -2.955e-05 -2.616e-05  7.703e-03 -3.240e-04\nq          1.128e-05  1.323e-05  3.655e-06  3.657e-06 -3.240e-04  5.652e-04\ny         -1.687e-06  5.904e-06  1.355e-06  2.014e-06 -4.146e-05  6.308e-05\nr          1.040e-06  5.704e-07  1.956e-06  8.289e-07 -1.005e-05  1.266e-06\n                   y          r\nw         -1.687e-06  1.040e-06\nrer_dt_sa  5.904e-06  5.704e-07\npia        1.355e-06  1.956e-06\npi         2.014e-06  8.289e-07\nx         -4.146e-05 -1.005e-05\nq          6.308e-05  1.266e-06\ny          9.768e-05  3.728e-06\nr          3.728e-06  2.803e-06\n\nCorrelation matrix of residuals:\n                 w rer_dt_sa      pia       pi        x        q        y\nw          1.00000  -0.13889  0.02952 -0.02158  0.02591  0.07654 -0.02754\nrer_dt_sa -0.13889   1.00000 -0.09432 -0.12740 -0.03821  0.05728  0.06149\npia        0.02952  -0.09432  1.00000  0.84714 -0.07465  0.03408  0.03039\npi        -0.02158  -0.12740  0.84714  1.00000 -0.13567  0.07002  0.09276\nx          0.02591  -0.03821 -0.07465 -0.13567  1.00000 -0.15528 -0.04780\nq          0.07654   0.05728  0.03408  0.07002 -0.15528  1.00000  0.26844\ny         -0.02754   0.06149  0.03039  0.09276 -0.04780  0.26844  1.00000\nr          0.10026   0.03506  0.25906  0.22534 -0.06839  0.03181  0.22527\n                 r\nw          0.10026\nrer_dt_sa  0.03506\npia        0.25906\npi         0.22534\nx         -0.06839\nq          0.03181\ny          0.22527\nr          1.00000\n\n\n\n\n17.5.2 Restricted VAR\nWe impose some restrictions on the VAR:\n\na &lt;- diag(1, 8)\na[lower.tri(a)] &lt;- NA\na\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    1    0    0    0    0    0    0    0\n[2,]   NA    1    0    0    0    0    0    0\n[3,]   NA   NA    1    0    0    0    0    0\n[4,]   NA   NA   NA    1    0    0    0    0\n[5,]   NA   NA   NA   NA    1    0    0    0\n[6,]   NA   NA   NA   NA   NA    1    0    0\n[7,]   NA   NA   NA   NA   NA   NA    1    0\n[8,]   NA   NA   NA   NA   NA   NA   NA    1\n\n\n\nsvar_a &lt;- vars::SVAR(var_l1, Amat = a, max.iter = 1000)\n\nWarning in vars::SVAR(var_l1, Amat = a, max.iter = 1000): Convergence not\nachieved after 1000 iterations. Convergence value: 6.81032508658627e-05 .\n\n\nThe matrix of the estimated coefficients:\n\nsvar_a$A\n\n                   w  rer_dt_sa        pia         pi           x           q\nw         1.00000000 0.00000000 0.00000000 0.00000000 0.000000000 0.000000000\nrer_dt_sa 0.10443238 1.00000000 0.00000000 0.00000000 0.000000000 0.000000000\npia       0.09576007 0.09535393 1.00000000 0.00000000 0.000000000 0.000000000\npi        0.09565631 0.09347590 0.08923415 1.00000000 0.000000000 0.000000000\nx         0.09077617 0.12705693 0.12991314 0.12808971 1.000000000 0.000000000\nq         0.07624405 0.07291027 0.08563015 0.08710426 0.042308911 1.000000000\ny         0.07910663 0.06949304 0.07726102 0.07790653 0.006098028 0.005853906\nr         0.07799215 0.07468340 0.07802918 0.08026269 0.004230866 0.047414317\n                   y r\nw         0.00000000 0\nrer_dt_sa 0.00000000 0\npia       0.00000000 0\npi        0.00000000 0\nx         0.00000000 0\nq         0.00000000 0\ny         1.00000000 0\nr         0.07505698 1\n\n\n\nsave(svar_a, df_var, file = \"../R/output/var_objects_chirps.rda\")\n\n\n\n17.5.3 Impulse Response Functions\nLet us focus on the impulse to the weather aggregate cost equation, with 95% confidence bootstrapped error bands (500 runs).\n\nimpulse_name &lt;- \"w\"\n\nThe following estimations are not run in this notebook. The estimation takes about 10 minutes each.\n\nnb_runs &lt;- 500\n\nirfs_95 &lt;- vars::irf(\n  svar_a, impulse = impulse_name, boot = TRUE, ci = .95, \n  n.ahead = 20, runs = nb_runs\n)\n\nAnd with 68% confidence bootstrapped error bands (500 runs, again).\n\nirfs_68 &lt;- vars::irf(\n  svar_a, impulse = impulse_name, boot = TRUE, ci = .68, \n  n.ahead = 20, runs = nb_runs\n)\n\nThe results can be saved:\n\nsave(irfs_95, file = \"../R/output/irfs_95_chirps.rda\")\nsave(irfs_68, file = \"../R/output/irfs_68_chirps.rda\")\n\nLet us load previously estimated IRFs.\n\nload(\"../R/output/irfs_95_chirps.rda\")\nload(\"../R/output/irfs_68_chirps.rda\")\n\n\nlevels &lt;- variable_names\nlabels &lt;- names(variable_names)\n\nNow, we can plot the resulting IRFs. Let us prepare the data for the response:\n\ndf_irfs &lt;- \n  irfs_95$irf[[impulse_name]] |&gt; \n  as_tibble() |&gt; \n  mutate(horizon = row_number() - 1) |&gt; \n  pivot_longer(cols = -horizon) |&gt; \n  mutate(impulse = !!impulse_name)\n\ndf_irfs &lt;- \n  df_irfs |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = !!levels,\n      labels = !!labels\n    )\n  )\n\nThe error bands:\n\ndf_irfs_ci &lt;- \n  map(\n    .x = list(`95` = irfs_95, `68` = irfs_68),\n    .f = function(irf_lvl) {\n      map(\n        .x = list(\n          lower = irf_lvl$Lower[[impulse_name]], \n          mean = irf_lvl$irf[[impulse_name]],\n          upper = irf_lvl$Upper[[impulse_name]]),\n        .f = ~ .x |&gt; \n          as_tibble() |&gt; \n          mutate(horizon = row_number() - 1) |&gt; \n          pivot_longer(cols = -horizon, values_to = \"bound\") |&gt; \n          mutate(impulse = !!impulse_name)\n      ) |&gt; \n        list_rbind(names_to = \"bound_type\")\n    }\n  ) |&gt; \n  list_rbind(names_to = \"level\") |&gt; \n  pivot_wider(names_from = bound_type, values_from = bound)\n\ndf_irfs_ci &lt;- \n  df_irfs_ci |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = !!levels,\n      labels = !!labels),\n    level = factor(level, levels = c(68, 95), labels = c(\"68%\", \"95%\"))\n  )\n\nAnd lastly, the graph.\n\nggplot() +\n  geom_ribbon(\n    data = df_irfs_ci,\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper,\n      fill = level),\n    alpha = .3\n  ) +\n  geom_line(\n    data = df_irfs_ci,\n    mapping = aes(x = horizon, y = mean),\n    colour = \"#0072B2\"\n  ) +\n  geom_hline(yintercept = 0, colour = \"#444444\") +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"#0072B2\", \"95%\" = \"#56B4E9\")) +\n  facet_wrap(~name, scales = \"free_y\", ncol = 4) +\n  theme_paper() +\n  coord_cartesian(xlim = c(0, 20))\n\n\n\n\n\n\n\nFigure 17.2: VAR(2) system response to one standard deviation orthogonal shock to the weather aggregate cost equation\n\n\n\n\n\nThe exact figure replication:\n\n\nCode\nlibrary(tikzDevice)\nlibrary(lemon)\nggplot() +\n  geom_ribbon(\n    data = df_irfs_ci |&gt; \n      filter(level == \"68%\") |&gt; \n      mutate(level = str_replace(level, \"\\\\%\", \"\\\\\\\\%\")),\n    mapping = aes(x = horizon,\n                  ymin = lower, ymax = upper,\n                  fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs,\n    mapping = aes(x = horizon, y = value),\n    colour = \"#009E73\"\n  ) +\n  geom_hline(yintercept = 0, colour = \"#444444\") +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68\\\\%\" = \"gray10\", \"95\\\\%\" = \"gray60\"),\n    guide = \"none\") +\n  facet_rep_wrap(~name, scales = \"free_y\", repeat.tick.labels = TRUE, ncol = 4) +\n  theme_paper() +\n  coord_cartesian(xlim = c(0, 20))\n\n\n\n\n\n\n\n\nFigure 17.3: VAR(2) system response to one standard deviation orthogonal shock to the weather aggregate cost equation. The gray bands depict the 68% confidence intervals.",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "robustness-ag-fluctuations_chirps.html#comparison-between-piscop-and-chirps-data",
    "href": "robustness-ag-fluctuations_chirps.html#comparison-between-piscop-and-chirps-data",
    "title": "17  Aggregate Fluctuations",
    "section": "17.6 Comparison between PISCOp and CHIRPS data",
    "text": "17.6 Comparison between PISCOp and CHIRPS data\nWe can plot the IRFs obtained using either the PISCOp rainfall data or the CHIRPS rainfall data.\n\n\nCode\n# PISCOp data\nload(\"../R/output/irfs_68_piscop.rda\")\nload(\"../R/output/irfs_95_piscop.rda\")\n\ndf_irfs_piscop &lt;- \n  irfs_68$irf[[impulse_name]] |&gt; \n  as_tibble() |&gt; \n  mutate(horizon = row_number() - 1) |&gt; \n  pivot_longer(cols = -horizon) |&gt; \n  mutate(impulse = !!impulse_name)\n\ndf_irfs_piscop &lt;- \n  df_irfs_piscop |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = !!levels,\n      labels = !!labels\n    )\n  )\n\ndf_irfs_ci_piscop &lt;- \n  map(\n    .x = list(`95` = irfs_95, `68` = irfs_68),\n    .f = function(irf_lvl) {\n      map(\n        .x = list(\n          lower = irf_lvl$Lower[[impulse_name]], \n          mean = irf_lvl$irf[[impulse_name]],\n          upper = irf_lvl$Upper[[impulse_name]]),\n        .f = ~ .x |&gt; \n          as_tibble() |&gt; \n          mutate(horizon = row_number() - 1) |&gt; \n          pivot_longer(cols = -horizon, values_to = \"bound\") |&gt; \n          mutate(impulse = !!impulse_name)\n      ) |&gt; \n        list_rbind(names_to = \"bound_type\")\n    }\n  ) |&gt; \n  list_rbind(names_to = \"level\") |&gt; \n  pivot_wider(names_from = bound_type, values_from = bound)\n\ndf_irfs_ci_piscop &lt;- \n  df_irfs_ci_piscop |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = !!levels,\n      labels = !!labels),\n    level = factor(level, levels = c(68, 95), labels = c(\"68%\", \"95%\"))\n  )\n\n# CHIRPS data\nload(\"../R/output/irfs_68_chirps.rda\")\nload(\"../R/output/irfs_95_chirps.rda\")\ndf_irfs_chirps &lt;- \n  irfs_68$irf[[impulse_name]] |&gt; \n  as_tibble() |&gt; \n  mutate(horizon = row_number() - 1) |&gt; \n  pivot_longer(cols = -horizon) |&gt; \n  mutate(impulse = !!impulse_name)\n\ndf_irfs_chirps &lt;- \n  df_irfs_chirps |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = !!levels,\n      labels = !!labels\n    )\n  )\n\ndf_irfs_ci_chirps &lt;- \n  map(\n    .x = list(`95` = irfs_95, `68` = irfs_68),\n    .f = function(irf_lvl) {\n      map(\n        .x = list(\n          lower = irf_lvl$Lower[[impulse_name]], \n          mean = irf_lvl$irf[[impulse_name]],\n          upper = irf_lvl$Upper[[impulse_name]]),\n        .f = ~ .x |&gt; \n          as_tibble() |&gt; \n          mutate(horizon = row_number() - 1) |&gt; \n          pivot_longer(cols = -horizon, values_to = \"bound\") |&gt; \n          mutate(impulse = !!impulse_name)\n      ) |&gt; \n        list_rbind(names_to = \"bound_type\")\n    }\n  ) |&gt; \n  list_rbind(names_to = \"level\") |&gt; \n  pivot_wider(names_from = bound_type, values_from = bound)\n\ndf_irfs_ci_chirps &lt;- \n  df_irfs_ci_chirps |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = !!levels,\n      labels = !!labels),\n    level = factor(level, levels = c(68, 95), labels = c(\"68%\", \"95%\"))\n  )\n\n# Merge results\ndf_irfs &lt;- \n  df_irfs_piscop |&gt; mutate(data = \"PISCOp\") |&gt; \n  bind_rows(df_irfs_chirps |&gt; mutate(data = \"CHIRPS\")) |&gt; \n  mutate(data = factor(data, levels = c(\"PISCOp\", \"CHIRPS\")))\n\ndf_irfs_ci &lt;- \n  df_irfs_ci_piscop |&gt; mutate(data = \"PISCOp\") |&gt; \n  bind_rows(df_irfs_ci_chirps |&gt; mutate(data = \"CHIRPS\"))\n\n# Aesthetics\ndf_irfs_ci &lt;- df_irfs_ci |&gt; \n  mutate(\n    fill_lab = str_c(data, \"_\", level),\n    fill_lab = factor(\n      fill_lab, \n      levels = c(\"PISCOp_68%\", \"PISCOp_95%\", \"CHIRPS_68%\", \"CHIRPS_95%\"), \n      labels = c(\"PISCOp, 68%\", \"PISCOp, 95%\", \"CHIRPS, 68%\", \"CHIRPS, 95%\")\n    )\n  )\n\n\n\n\nCode\nlibrary(tikzDevice)\nlibrary(lemon)\nggplot() +\n  geom_ribbon(\n    data = df_irfs_ci |&gt; \n      filter(level == \"68%\"),\n    mapping = aes(x = horizon,\n                  ymin = lower, ymax = upper,\n                  fill = fill_lab),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs,\n    mapping = aes(x = horizon, y = value, colour = data),\n    linewidth = 1.1\n  ) +\n  geom_hline(yintercept = 0, colour = \"#444444\") +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"Data, C.I. level\", \n    values = c(\n      \"PISCOp, 68%\" = \"#117733\", \n      \"PISCOp, 95%\" = \"#44AA99\", \n      \"CHIRPS, 68%\" = \"#882255\", \n      \"CHIRPS, 95%\" = \"#CC6677\"\n    )\n  ) +\n  scale_colour_manual(\n    NULL, values = c(\"PISCOp\" = \"#117733\", \"CHIRPS\" = \"#882255\"), guide = \"none\"\n  ) +\n  facet_rep_wrap(~name, scales = \"free_y\", repeat.tick.labels = TRUE, ncol = 4) +\n  theme_paper() +\n  coord_cartesian(xlim = c(0, 20))\n\n\n\n\n\n\n\n\nFigure 17.4: VAR(2) system response to an unit orthogonal shock to the weather aggregate cost equation, using either PISCOp data or CHIRPS data",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "robustness-ag-fluctuations_chirps.html#estimation-with-local-projections",
    "href": "robustness-ag-fluctuations_chirps.html#estimation-with-local-projections",
    "title": "17  Aggregate Fluctuations",
    "section": "17.7 Estimation with Local Projections",
    "text": "17.7 Estimation with Local Projections\nWe can also estimate the propagation of the shock using local projections rather than VAR.\nLet us build the table with the data:\n\ndf_lp &lt;- \n  df_var |&gt; \n  filter(date &gt;= start_date) |&gt; \n  select(\n    w, rer_dt_sa, pia, pi, x, q, y, r\n  )\n\nThe linear impulse response functions can be estimated using the lp_lin() function from {lpirfs}.\n\nlibrary(lpirfs)\n# With 95% conf. int.\nresults_lin_95 &lt;- lp_lin(\n  endog_data     = df_lp,\n  lags_endog_lin = 2,# 2 lags\n  trend          = 0,# no trend\n  shock_type     = 0,# std dev. shock\n  confint        = 1.96,\n  hor            = 20\n)\n# With 68% conf. int.\nresults_lin_68 &lt;- lp_lin(\n  endog_data     = df_lp,\n  lags_endog_lin = 2,# 2 lags\n  trend          = 0,# no trend\n  shock_type     = 0,# std dev. shock\n  confint        = 1,\n  hor            = 20\n)\n\nWe define a small function, get_irfs(), to format the IRFs from the object returned by lp_lin().\n\nget_irfs &lt;- function(resul_lin) {\n  irf_lin_mean &lt;- resul_lin[[\"irf_lin_mean\"]]\n  irf_lin_low &lt;- resul_lin[[\"irf_lin_low\"]]\n  irf_lin_up &lt;- resul_lin[[\"irf_lin_up\"]]\n  specs &lt;- resul_lin$specs\n  \n  irfs_df &lt;- NULL\n  for (rr in 1:(specs$endog)) {\n    for (ss in 1:(specs$endog)) {\n      tbl_lin_mean &lt;- as.matrix(t(irf_lin_mean[, 1:specs$hor, ss]))[, rr]\n      tbl_lin_low &lt;- as.matrix(t(irf_lin_low[, 1:specs$hor, ss]))[, rr]\n      tbl_lin_up &lt;- as.matrix(t(irf_lin_up[, 1:specs$hor, ss]))[, rr]\n      tbl_lin &lt;- tibble(\n        horizon = seq_along(tbl_lin_mean), \n        mean = tbl_lin_mean, \n        low = tbl_lin_low, \n        up = tbl_lin_up,\n        shocked = specs$column_names[ss],\n        on = specs$column_names[rr]\n      )\n      irfs_df &lt;- bind_rows(irfs_df, tbl_lin)\n    }\n  }\n  \n  irfs_df &lt;- \n    irfs_df |&gt;\n    mutate(\n      shocked = factor(shocked, levels = variable_names, labels = names(variable_names)),\n      on = factor(on, levels = variable_names, labels = names(variable_names))\n    )\n  \n  irfs_df\n}\n\nThe formatted IRFs:\n\nirfs_df &lt;- \n  get_irfs(results_lin_95) |&gt; mutate(level = \"95%\") |&gt; \n  bind_rows(\n    get_irfs(results_lin_68) |&gt; mutate(level = \"68%\")\n  )\n\nAnd the figure:\n\n\nCode\nggplot() +\n  geom_ribbon(\n    data = irfs_df |&gt; filter(shocked == \"Agricultural losses\"), \n    mapping = aes(\n      x = horizon,\n      ymin = low, ymax = up,\n      fill = level),\n    alpha = .3\n  ) +\n  geom_line(\n    data = irfs_df |&gt; filter(shocked == \"Agricultural losses\"), ,\n    mapping = aes(x = horizon, y = mean),\n    colour = \"#0072B2\"\n  ) +\n  geom_hline(yintercept = 0, colour = \"#444444\") +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"#0072B2\", \"95%\" = \"#56B4E9\")) +\n  facet_wrap(~on, scales = \"free_y\", ncol = 4) +\n  theme_paper() +\n  coord_cartesian(xlim = c(0, 20)) +\n  scale_y_continuous(labels = scales::label_percent())\n\n\n\n\n\n\n\n\nFigure 17.5: System response to one standard deviation orthogonal shock to the weather aggregate cost equation. Estimations made with local projections.\n\n\n\n\n\nAnd with the 68% confidence interval only:\n\n\nCode\nlibrary(tikzDevice)\nlibrary(lemon)\nggplot(\n    data = irfs_df |&gt; filter(shocked == \"Agricultural losses\") |&gt; \n      filter(level == \"68%\") |&gt; \n      mutate(level = str_replace(level, \"\\\\%\", \"\\\\\\\\%\")) |&gt; \n      mutate(\n        mean = mean * 100,\n        low = low * 100,\n        up = up * 100\n      )\n  ) +\n  geom_ribbon(\n    mapping = aes(x = horizon,ymin = low, ymax = up, fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    mapping = aes(x = horizon, y = mean),\n    colour = \"#009E73\"\n  ) +\n  geom_hline(yintercept = 0, colour = \"#444444\") +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68\\\\%\" = \"gray10\", \"95\\\\%\" = \"gray60\"),\n    guide = \"none\") +\n  facet_rep_wrap(~on, scales = \"free_y\", repeat.tick.labels = TRUE, ncol = 4) +\n  theme_paper() +\n  coord_cartesian(xlim = c(0, 20))\n\n\n\n\n\n\n\n\nFigure 17.6: System response to one standard deviation orthogonal shock to the weather aggregate cost equation. Estimations made with local projections. The gray bands depict the 68% confidence intervals.",
    "crumbs": [
      "IV. Robustness Check: CHIRPS Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregate Fluctuations</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections_posvsneg.html",
    "href": "robustness-local_projections_posvsneg.html",
    "title": "18  Agricultural Production: Positive vs. Negative Surprise Shocks (LP)",
    "section": "",
    "text": "18.1 Linear Local Projections\nIn this section, we focus on estimating the Local Projections (Jordà 2005) to quantify the impact of weather on agricultural production. We use panel data, similar to the approach used in the study by Acevedo et al. (2020), and independently estimate models for each specific crop.\nFor a particular crop denoted as \\(c\\), the model can be expressed as follows: \\[\n\\begin{aligned}\n\\underbrace{y_{c,i,{\\color{wongGold}t+h}}}_{\\text{Production}} = &  {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}HotSurprise}}} {\\color{wongPurple}{\\text{HotSurprise}^{+}_{i,{\\color{wongGold}t}}}} +\n{\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}ColdSurprise}}} {\\color{wongPurple}{\\text{ColdSurprise}^{+}_{i,{\\color{wongGold}t}}}}\\\\\n& + {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}DrySurprise}}} {\\color{wongPurple}{\\text{DrySurprise}^{+}_{i,{\\color{wongGold}t}}}} +\n+ {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}WetSurprise}}} {\\color{wongPurple}{\\text{WetSurprise}^{+}_{i,{\\color{wongGold}t}}}}\\\\\n        &+\\gamma_{c,i,h}\\underbrace{X_{t}}_{\\text{controls}}  + \\underbrace{\\zeta_{c,i,h} \\text{Trend}_{t} + \\eta_{c,i,h} \\text{Trend}^2_{t}}_{\\text{regional monthly trend}} + \\varepsilon_{c,i,t+h}\n\\end{aligned}\n\\tag{18.1}\\]\nHere, \\(i\\) represents the region, \\(t\\) represents the time, and \\(h\\) represents the horizon. The primary focus lies on estimating the coefficients associated with temperature surprise shocks and precipitation surprise shocks for different time horizons \\(\\color{wongGold}h=\\{0,1,...,T_{c}\\}\\)\nNote that we allow a crop regional monthly specific quadratic trend to be estimated.",
    "crumbs": [
      "V. Robustness Check: Other",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Agricultural Production: Positive vs. Negative Surprise Shocks (LP)</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections_posvsneg.html#sec-lp-linear-chirps",
    "href": "robustness-local_projections_posvsneg.html#sec-lp-linear-chirps",
    "title": "18  Agricultural Production: Positive vs. Negative Surprise Shocks (LP)",
    "section": "",
    "text": "18.1.1 Functions\nThe estimation functions presented in Chapter 7.1.1 can be sourced.\n\nsource(\"../weatherperu/R/estimations.R\")\n\n\n\n18.1.2 Estimation\nTo loop over the different crops, we can use the map() function. This function enables us to apply the estimate_linear_lp() function to each crop iteratively, facilitating the estimation process.\n\ncrops &lt;- df$product_eng |&gt; unique()\nweather_variables &lt;- c(\"cold_surprise\", \"hot_surprise\", \"dry_surprise\", \"wet_surprise\")\ncontrol_variables &lt;- c(\"rer_hp\", \"r_hp\", \"pi\", \"ind_prod\", \"ONI\", \"price_int_inf\")\nnb_h &lt;- 14\n\nThe estimation (this code takes about a minute to run, we load results in this notebook):\n\nresul_lp &lt;- vector(mode = \"list\", length = length(crops))\nfor (i_crop in 1:length(crops)) {\n  print(crops[i_crop])\n  resul_lp[[i_crop]] &lt;- estimate_linear_lp(\n    df,\n    horizons = nb_h,\n    y_name = \"y_new_normalized\",\n    group_name = \"region_id\",\n    detrend = TRUE,\n    add_month_fe = FALSE,\n    add_intercept = FALSE,\n    crop_name = crops[i_crop],\n    control_names = control_variables,\n    weather_names = weather_variables,\n    std = \"Cluster\",\n    # std = \"nw\",\n    other_var_to_keep = \"y_new\"\n  )\n}\nsave(resul_lp, file = \"output/resul_lp_surprise.rda\")\n\n\nload(\"../R/output/resul_lp_surprise.rda\")\n\n\n\n18.1.3 Results\nWe can visualize the Impulse Response Functions (IRFs) by plotting the estimated coefficients associated with the weather variables. These coefficients represent the impact of weather on agricultural production and can provide valuable insights into the dynamics of the system. By plotting the IRFs, we can gain a better understanding of the relationship between weather variables and the response of agricultural production over time.\nThe data for the graphs:\n\ndf_irfs_lp &lt;- map(resul_lp, \"coefs\") |&gt; \n  list_rbind() |&gt; \n  filter(name %in% weather_variables) |&gt; \n  mutate(\n    shock_1_sd = value * std_shock,\n    lower_95 = (value - qnorm(0.975) * std) * std_shock,\n    upper_95 = (value + qnorm(0.975) * std) * std_shock,\n    lower_68 = (value - qnorm(0.84)  * std) * std_shock,\n    upper_68 = (value + qnorm(0.84)  * std) * std_shock\n  ) |&gt; \n  mutate(\n    crop = factor(\n      crop, \n      levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"))\n  ) |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = c(\n        \"cold_surprise\",\n        \"hot_surprise\",\n        \"dry_surprise\",\n        \"wet_surprise\"\n      ),\n      labels = c(\n        \"Cold surprise\", \n        \"Hot surprise\",\n        \"Dry surprise\", \n        \"Wet surprise\"\n      )\n    )\n  )\n\nFor the confidence intervals:\n\ndf_irfs_lp_ci &lt;- \n  df_irfs_lp |&gt; \n  select(horizon, crop, name, matches(\"^(lower)|^(upper)\", perl = TRUE)) |&gt; \n  pivot_longer(\n    cols = matches(\"^(lower)|^(upper)\", perl = TRUE),\n    names_pattern = \"(.*)_(95|68)$\",\n    names_to = c(\".value\", \"level\")\n  ) |&gt; \n  mutate(level = str_c(level, \"%\"))\n\n\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs_lp |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(x = horizon, y = shock_1_sd),\n    colour = \"#0072B2\") +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", \n    independent = \"y\", switch = \"y\") +\n  scale_x_continuous(breaks = seq(0, nb_h, by = 2)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"gray10\", \"95%\" = \"gray60\")\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\nFigure 18.1: Agricultural production response to a surprise weather shock\n\n\n\n\n\n\n\n18.1.4 Exporting results\nLet us save the results.\n\nsave(df_irfs_lp, df_irfs_lp_ci, file = \"../R/output/df_irfs_lp_surprise.rda\")\nsave(resul_lp, file = \"../R/output/resul_lp_surprise.rda\")\n\n\n\n\n\nAcevedo, Sebastian, Mico Mrkaic, Natalija Novta, Evgenia Pugacheva, and Petia Topalova. 2020. “The Effects of Weather Shocks on Economic Activity: What Are the Channels of Impact?” Journal of Macroeconomics 65: 103207.\n\n\nJordà, Òscar. 2005. “Estimation and Inference of Impulse Responses by Local Projections.” American Economic Review 95 (1): 161–82. https://doi.org/10.1257/0002828053828518.\n\n\nNatoli, Filippo. 2024. “The Macroeconomic Effects of Unexpected Temperature Shocks.” https://doi.org/10.2139/ssrn.4160944.",
    "crumbs": [
      "V. Robustness Check: Other",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Agricultural Production: Positive vs. Negative Surprise Shocks (LP)</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections_until_2014.html",
    "href": "robustness-local_projections_until_2014.html",
    "title": "19  Agricultural Production (LP): without 2015 data",
    "section": "",
    "text": "19.1 Linear Local Projections\nIn this section, we focus on estimating the Local Projections (Jordà 2005) to quantify the impact of weather on agricultural production. We use panel data, similar to the approach used in the study by Acevedo et al. (2020), and independently estimate models for each specific crop.\nFor a particular crop denoted as \\(c\\), the model can be expressed as follows: \\[\n\\begin{aligned}\n\\underbrace{y_{c,i,{\\color{wongGold}t+h}}}_{\\text{Production}} = &  {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}T}}} {\\color{wongPurple}{T_{i,{\\color{wongGold}t}}}} + {\\color{wongOrange}\\beta_{c,{\\color{wongGold}h}}^{{\\color{wongPurple}P}}} {\\color{wongPurple}P_{i,{\\color{wongGold}t}}}\\\\\n        &+\\gamma_{c,i,h}\\underbrace{X_{t}}_{\\text{controls}}  + \\underbrace{\\zeta_{c,i,h} \\text{Trend}_{t} + \\eta_{c,i,h} \\text{Trend}^2_{t}}_{\\text{regional monthly trend}} + \\varepsilon_{c,i,t+h}\n\\end{aligned}\n\\tag{19.1}\\]\nHere, \\(i\\) represents the region, \\(t\\) represents the time, and \\(h\\) represents the horizon. The primary focus lies on estimating the coefficients associated with temperature and precipitation for different time horizons \\(\\color{wongGold}h=\\{0,1,...,T_{c}\\}\\)\nNote that we allow a crop regional monthly specific quadratic trend to be estimated.",
    "crumbs": [
      "V. Robustness Check: Other",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Agricultural Production (LP): without 2015 data</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections_until_2014.html#sec-lp-linear-until2014",
    "href": "robustness-local_projections_until_2014.html#sec-lp-linear-until2014",
    "title": "19  Agricultural Production (LP): without 2015 data",
    "section": "",
    "text": "19.1.1 Functions\nThe estimation functions presented in Chapter 7.1.1 can be sourced.\n\nsource(\"../weatherperu/R/estimations.R\")\n\n\n\n19.1.2 Estimation\nTo loop over the different crops, we can use the map() function. This function enables us to apply the estimate_linear_lp() function to each crop iteratively, facilitating the estimation process.\n\ncrops &lt;- df$product_eng |&gt; unique()\nweather_variables &lt;- c(\"temp_max_dev\", \"precip_piscop_sum_dev\")\ncontrol_variables &lt;- c(\n  \"rer_hp\", \"r_hp\", \"pi\", \"ind_prod\", \"ONI\", \"price_int_inf\"\n)\nnb_h &lt;- 14\n\nThe estimation (this code takes about a minute to run, we load results in this notebook):\n\nresul_lp &lt;- vector(mode = \"list\", length = length(crops))\nfor (i_crop in 1:length(crops)) {\n  resul_lp[[i_crop]] &lt;- estimate_linear_lp(\n    df,\n    horizons = nb_h,\n    y_name = \"y_new_normalized\",\n    group_name = \"region_id\",\n    detrend = TRUE,\n    add_month_fe = FALSE,\n    add_intercept = FALSE,\n    crop_name = crops[i_crop],\n    control_names = control_variables,\n    weather_names = weather_variables,\n    std = \"Cluster\",\n    # std = \"nw\",\n    other_var_to_keep = \"y_new\"\n  )\n}\nsave(resul_lp, file = \"..R/output/resul_lp_piscop_until2014.rda\")\n\n\nload(\"../R/output/resul_lp_piscop_until2014.rda\")\n\n\n\n19.1.3 Results\nWe can visualize the Impulse Response Functions (IRFs) by plotting the estimated coefficients associated with the weather variables. These coefficients represent the impact of weather on agricultural production and can provide valuable insights into the dynamics of the system. By plotting the IRFs, we can gain a better understanding of the relationship between weather variables and the response of agricultural production over time.\nThe data for the graphs:\n\ndf_irfs_lp &lt;- map(resul_lp, \"coefs\") |&gt; \n  list_rbind() |&gt; \n  filter(name %in% weather_variables) |&gt; \n  mutate(\n    shock_1_sd = value * std_shock,\n    lower_95 = (value - qnorm(0.975) * std) * std_shock,\n    upper_95 = (value + qnorm(0.975) * std) * std_shock,\n    lower_68 = (value - qnorm(0.84)  * std) * std_shock,\n    upper_68 = (value + qnorm(0.84)  * std) * std_shock\n  ) |&gt; \n  mutate(\n    crop = factor(\n      crop, \n      levels = c(\"Rice\", \"Dent corn\", \"Potato\", \"Cassava\"),\n      labels = c(\"Rice\", \"Maize\", \"Potato\", \"Cassava\"))\n  ) |&gt; \n  mutate(\n    name = factor(\n      name,\n      levels = c(\n        \"temp_max_dev\",\n        \"precip_piscop_sum_dev\"\n      ),\n      labels = c(\n        \"Temp. anomalies\", \n        \"Precip. anomalies\"\n      )\n    )\n  )\n\nFor the confidence intervals:\n\ndf_irfs_lp_ci &lt;- \n  df_irfs_lp |&gt; \n  select(horizon, crop, name, matches(\"^(lower)|^(upper)\", perl = TRUE)) |&gt; \n  pivot_longer(\n    cols = matches(\"^(lower)|^(upper)\", perl = TRUE),\n    names_pattern = \"(.*)_(95|68)$\",\n    names_to = c(\".value\", \"level\")\n  ) |&gt; \n  mutate(level = str_c(level, \"%\"))\n\n\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = level),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs_lp |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(x = horizon, y = shock_1_sd),\n    colour = \"#0072B2\") +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", \n    independent = \"y\", switch = \"y\") +\n  scale_x_continuous(breaks = seq(0, nb_h, by = 2)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Horizon\", y = NULL) +\n  scale_fill_manual(\n    \"C.I. level\", \n    values = c(\"68%\" = \"gray10\", \"95%\" = \"gray60\")\n  ) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\nFigure 19.1: Agricultural production response to a weather shock\n\n\n\n\n\n\n\n19.1.4 Exporting results\nLet us save the results for later use.\n\nsave(df_irfs_lp, df_irfs_lp_ci, file = \"../R/output/resul_lp_piscop_until2014.rda\")\nsave(resul_lp, file = \"../R/output/df_irfs_lp_piscop_until2014.rda\")",
    "crumbs": [
      "V. Robustness Check: Other",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Agricultural Production (LP): without 2015 data</span>"
    ]
  },
  {
    "objectID": "robustness-local_projections_until_2014.html#comparaison-between-piscop-and-chirps",
    "href": "robustness-local_projections_until_2014.html#comparaison-between-piscop-and-chirps",
    "title": "19  Agricultural Production (LP): without 2015 data",
    "section": "19.2 Comparaison between PISCOp and CHIRPS",
    "text": "19.2 Comparaison between PISCOp and CHIRPS\nWe can plot the IRFs obtained either using PISCOp or CHIRPS rainfall data.\n\n\nCode\ndf_irfs_lp_2014 &lt;- df_irfs_lp\ndf_irfs_lp_ci_2014 &lt;- df_irfs_lp_ci\n# With Piscop data\nload(\"../R/output/df_irfs_lp_piscop.rda\")\ndf_irfs_lp_comparison &lt;- df_irfs_lp |&gt; \n  mutate(data_type = \"Full sample\") |&gt; \n  bind_rows(\n    df_irfs_lp_2014 |&gt; \n      mutate(data_type = \"Without 2015 data\")\n  ) |&gt; \n  mutate(\n    data_type = factor(\n      data_type,\n      levels = c(\"Full sample\", \"Without 2015 data\")\n    )\n  )\n\n\ndf_irfs_lp_ci_comparison &lt;- df_irfs_lp_ci |&gt; \n  mutate(data_type = \"Full sample\") |&gt; \n  bind_rows(\n    df_irfs_lp_ci_2014 |&gt; \n      mutate(data_type = \"Without 2015 data\")\n  ) |&gt; \n  mutate(\n    data_type = factor(\n      data_type,\n      levels = c(\"Full sample\", \"Without 2015 data\")\n    )\n  )\n\n\n\n\nCode\nggplot() +\n  geom_ribbon(\n    data = df_irfs_lp_ci_comparison |&gt; \n      filter(level == \"68%\", horizon &lt;= !!nb_h),\n    mapping = aes(\n      x = horizon,\n      ymin = lower, ymax = upper, fill = data_type, colour = data_type, \n      linetype = data_type),\n    alpha = .2\n  ) +\n  geom_line(\n    data = df_irfs_lp_comparison |&gt; filter(horizon &lt;= !!nb_h),\n    mapping = aes(x = horizon, y = shock_1_sd, colour = data_type, \n                  linetype = data_type),\n    linewidth = 1.1\n  ) +\n  scale_colour_manual(\n    NULL, \n    values = c(\"Full sample\" = \"#56B4E9\", \"Without 2015 data\" = \"#D55E00\")\n  ) +\n  geom_hline(yintercept = 0, colour = \"gray40\") +\n  ggh4x::facet_grid2(\n    name~crop, scales = \"free_y\", \n    independent = \"y\", switch = \"y\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_x_continuous(breaks = seq(0, nb_h, by = 2)) +\n  labs(x = \"Horizon (in months)\", y = NULL) +\n  scale_fill_manual(\n    NULL,\n    values = c(\"Full sample\" = \"#56B4E9\", \"Without 2015 data\" = \"#D55E00\")\n  ) +\n  scale_linetype_discrete(NULL) +\n  theme_paper() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nFigure 19.2: Agricultural production response to a weather shock, with or without including data from 2015.\n\n\n\n\n\n\n\n\n\nAcevedo, Sebastian, Mico Mrkaic, Natalija Novta, Evgenia Pugacheva, and Petia Topalova. 2020. “The Effects of Weather Shocks on Economic Activity: What Are the Channels of Impact?” Journal of Macroeconomics 65: 103207.\n\n\nJordà, Òscar. 2005. “Estimation and Inference of Impulse Responses by Local Projections.” American Economic Review 95 (1): 161–82. https://doi.org/10.1257/0002828053828518.",
    "crumbs": [
      "V. Robustness Check: Other",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Agricultural Production (LP): without 2015 data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acevedo, Sebastian, Mico Mrkaic, Natalija Novta, Evgenia Pugacheva, and\nPetia Topalova. 2020. “The Effects of Weather Shocks on Economic\nActivity: What Are the Channels of Impact?” Journal of\nMacroeconomics 65: 103207.\n\n\nAragón, Fernando M, Francisco Oteiza, and Juan Pablo Rud. 2021.\n“Climate Change and Agriculture: Subsistence Farmers’ Response to\nExtreme Heat.” American Economic Journal: Economic\nPolicy 13 (1): 1–35. https://doi.org/10.1257/pol.20190316.\n\n\nAuerbach, Alan, and Yuriy Gorodnichenko. 2011. “Fiscal Multipliers\nin Recession and Expansion.” National Bureau of Economic\nResearch. https://doi.org/10.3386/w17447.\n\n\nAybar, Cesar, Carlos Fernández, Adrian Huerta, Waldo Lavado, Fiorella\nVega, and Oscar Felipe-Obando. 2020. “Construction of a\nHigh-Resolution Gridded Rainfall Dataset for Peru from 1981 to the\nPresent Day.” Hydrological Sciences Journal 65 (5):\n770–85.\n\n\nBurke, Marshall, Erick Gong, and Kelly Jones. 2014. “Income Shocks\nand HIV in Africa.” The Economic Journal\n125 (585): 1157–89. https://doi.org/10.1111/ecoj.12149.\n\n\nFunk, Chris, Pete Peterson, Martin Landsfeld, Diego Pedreros, James\nVerdin, Shraddhanand Shukla, Gregory Husak, et al. 2015.\n“CHIRPS: Rainfall Estimates from Rain Gauge and\nSatellite Observations.” https://doi.org/10.15780/G2RP4Q.\n\n\nJordà, Òscar. 2005. “Estimation and Inference of Impulse Responses\nby Local Projections.” American Economic Review 95 (1):\n161–82. https://doi.org/10.1257/0002828053828518.\n\n\nNatoli, Filippo. 2024. “The Macroeconomic Effects of Unexpected\nTemperature Shocks.” https://doi.org/10.2139/ssrn.4160944.",
    "crumbs": [
      "References"
    ]
  }
]